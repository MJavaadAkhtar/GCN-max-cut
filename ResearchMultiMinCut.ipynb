{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from commons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separate code base from heurestics\n",
    "# Utils code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.float32\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCNSoftmax(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer\n",
    "\n",
    "def partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the partition of each edge (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.not_equal.outer(s, s).astype(int)\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "    return weight\n",
    "\n",
    "import torch\n",
    "\n",
    "def partition_weight2(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph as a PyTorch tensor.\n",
    "    :param s: Tensor indicating the partition of each node (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    # Ensure s is a tensor\n",
    "    # s = torch.tensor(s, dtype=torch.float32)\n",
    "\n",
    "    # Compute outer difference to create partition matrix\n",
    "    s = s.unsqueeze(0)  # Convert s to a row vector\n",
    "    t = s.t()           # Transpose s to a column vector\n",
    "    partition_matrix = (s != t).float()  # Compute outer product and convert boolean to float\n",
    "\n",
    "    # Calculate the weight of edges between different partitions\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut(q_torch, s):\n",
    "    '''\n",
    "\n",
    "    :param q_torch: The adjacent matrix of the graph\n",
    "    :param s: The binary output from the neural network. s will be in form of [[prob1, prob2, ..., prob n], ...]\n",
    "    :return: The calculated cut loss value\n",
    "    '''\n",
    "    if len(s) > 0:\n",
    "        totalCuts = len(s[0])\n",
    "        CutValue = 0\n",
    "        for i in range(totalCuts):\n",
    "            CutValue += partition_weight2(q_torch, s[:,i])\n",
    "        return CutValue/2\n",
    "    return 0\n",
    "\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 80 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def FIndAC(graph):\n",
    "    max_degree = max(dict(graph.degree()).values())\n",
    "    A_initial = max_degree + 1  # A is set to be one more than the maximum degree\n",
    "    C_initial = max_degree / 2  # C is set to half the maximum degree\n",
    "\n",
    "    return A_initial, C_initial\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "#     \"\"\"\n",
    "#     Train a GCN model with early stopping.\n",
    "#     \"\"\"\n",
    "#     # loss for a whole epoch\n",
    "#     prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "#     prev_cummulative_loss = float('inf')\n",
    "#     cummulativeCount = 0\n",
    "#     count = 0  # Patience counter\n",
    "#     best_loss = float('inf')  # Initialize best loss to infinity\n",
    "#     best_model_state = None  # Placeholder for the best model state\n",
    "#     loss_list = []\n",
    "#     epochList = []\n",
    "#     cumulative_loss = 0\n",
    "#\n",
    "#     t_gnn_start = time()\n",
    "#\n",
    "#     # contains information regarding all terminal nodes for the dataset\n",
    "#     terminal_configs = {}\n",
    "#     epochCount = 0\n",
    "#     criterion = nn.BCELoss()\n",
    "#     A = nn.Parameter(torch.tensor([65.0]))\n",
    "#     C = nn.Parameter(torch.tensor([32.5]))\n",
    "#\n",
    "#     embed = nn.Embedding(80, dim_embedding)\n",
    "#     embed = embed.type(torch_dtype).to(torch_device)\n",
    "#     inputs = embed.weight\n",
    "#\n",
    "#     for epoch in range(number_epochs):\n",
    "#\n",
    "#         cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "#\n",
    "#         for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "#             epochCount +=1\n",
    "#\n",
    "#\n",
    "#             # Ensure model is in training mode\n",
    "#             net.train()\n",
    "#\n",
    "#             # Pass the graph and the input features to the model\n",
    "#             logits = net(dgl_graph, adjacency_matrix)\n",
    "#             logits = override_fixed_nodes(logits)\n",
    "#\n",
    "#             # Compute the loss\n",
    "#             # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "#\n",
    "#             loss = loss_func( logits, adjacency_matrix)\n",
    "#\n",
    "#\n",
    "#             # Backpropagation\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#\n",
    "#             # Update cumulative loss\n",
    "#             cumulative_loss += loss.item()\n",
    "#\n",
    "#\n",
    "#\n",
    "#             # # Check for early stopping\n",
    "#             if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "#                 count += 1\n",
    "#                 if count >= patience: # play around with patience value, try lower one\n",
    "#                     print(f'Stopping early at epoch {epoch}')\n",
    "#                     break\n",
    "#             else:\n",
    "#                 count = 0  # Reset patience counter if loss decreases\n",
    "#\n",
    "#             # Update best model\n",
    "#             if cumulative_loss < best_loss:\n",
    "#                 best_loss = cumulative_loss\n",
    "#                 best_model_state = net.state_dict()  # Save the best model state\n",
    "#\n",
    "#         loss_list.append(loss)\n",
    "#\n",
    "#         # # Early stopping break from the outer loop\n",
    "#         # if count >= patience:\n",
    "#         #     count=0\n",
    "#\n",
    "#         prev_loss = cumulative_loss  # Update previous loss\n",
    "#\n",
    "#         if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "#             print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "#\n",
    "#             if save_directory != None:\n",
    "#                 checkpoint = {\n",
    "#                     'epoch': epoch,\n",
    "#                     'model': net.state_dict(),\n",
    "#                     'optimizer': optimizer.state_dict(),\n",
    "#                     'lossList':loss_list,\n",
    "#                     'inputs':inputs}\n",
    "#                 torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "#\n",
    "#             if (prev_cummulative_loss == cummulativeCount):\n",
    "#                 cummulativeCount+=1\n",
    "#\n",
    "#                 if cummulativeCount > 4:\n",
    "#                     break\n",
    "#             else:\n",
    "#                 prev_cummulative_loss = cumulative_loss\n",
    "#\n",
    "#\n",
    "#     t_gnn = time() - t_gnn_start\n",
    "#\n",
    "#     # Load the best model state\n",
    "#     if best_model_state is not None:\n",
    "#         net.load_state_dict(best_model_state)\n",
    "#\n",
    "#     print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "#     print(f'Best cumulative loss: {best_loss}')\n",
    "#     loss = loss_func(logits, adjacency_matrix)\n",
    "#     if save_directory != None:\n",
    "#         checkpoint = {\n",
    "#             'epoch': epoch,\n",
    "#             'model': net.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#             'lossList':loss_list,\n",
    "#             'inputs':inputs}\n",
    "#         torch.save(checkpoint, './final_'+save_directory)\n",
    "#\n",
    "#     return net, best_loss, epoch, inputs, loss_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HyperParameters initialization and related functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def printCombo(orig):\n",
    "    # Original dictionary\n",
    "    input_dict = orig\n",
    "\n",
    "    # Generate all permutations of the dictionary values\n",
    "    value_permutations = list(permutations(input_dict.values()))\n",
    "\n",
    "    # Create a list of dictionaries from the permutations\n",
    "    permuted_dicts = [{key: value for key, value in zip(input_dict.keys(), perm)} for perm in value_permutations]\n",
    "\n",
    "    return permuted_dicts\n",
    "\n",
    "def GetOptimalNetValue(net, dgl_graph, inp, q_torch, terminal_dict):\n",
    "    net.eval()\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if (dgl_graph.number_of_nodes() < 30):\n",
    "        inp = torch.ones((dgl_graph.number_of_nodes(), 30))\n",
    "\n",
    "    # find all potential combination of terminal nodes with respective indices\n",
    "\n",
    "    perm_items = printCombo(terminal_dict)\n",
    "    for i in perm_items:\n",
    "        probs = net(dgl_graph, inp, i)\n",
    "        binary_partitions = (probs >= 0.5).float()\n",
    "        cut_value_item = calculateAllCut(q_torch, binary_partitions)\n",
    "        if cut_value_item < best_loss:\n",
    "            best_loss = cut_value_item\n",
    "    return best_loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hamiltonian loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def terminal_independence_penalty(s, terminal_nodes):\n",
    "    \"\"\"\n",
    "    Calculate a penalty that enforces each terminal node to be in a distinct partition.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param terminal_nodes: A list of indices for terminal nodes.\n",
    "    :return: The penalty term.\n",
    "    \"\"\"\n",
    "    penalty = 0\n",
    "    num_terminals = len(terminal_nodes)\n",
    "    # Compare each pair of terminal nodes\n",
    "    for i in range(num_terminals):\n",
    "        for j in range(i + 1, num_terminals):\n",
    "            # Calculate the dot product of the probability vectors for the two terminals\n",
    "            dot_product = torch.dot(s[terminal_nodes[i]], s[terminal_nodes[j]])\n",
    "            # Penalize the similarity in their partition assignments (dot product should be close to 0)\n",
    "            penalty += dot_product\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter2(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_new(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Differentiable calculation of HC for minimizing edge weights between different partitions.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value, focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    K = s.shape[1]\n",
    "    V = s.shape[0]\n",
    "\n",
    "    # Create a full partition matrix indicating the likelihood of each node pair being in the same partition\n",
    "    partition_matrix = torch.matmul(s, s.T)\n",
    "\n",
    "    # Calculate the complement matrix, which indicates the likelihood of node pairs being in different partitions\n",
    "    complement_matrix = 1 - partition_matrix\n",
    "\n",
    "    # Apply adjacency matrix to only consider actual edges and their weights\n",
    "    inter_partition_edges = adjacency_matrix * complement_matrix\n",
    "\n",
    "    # Summing up all contributions for edges between different partitions\n",
    "    HC = torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_vectorized_old(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # HC = ∑(u,v)∈E(1−∑k∈K(su,k*sv,k))*adjacency_matrix[u,v]\n",
    "    K = s.shape[1]\n",
    "    # Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\n",
    "    prod = adjacency_matrix * (1 - s @ s.T)\n",
    "    HC = torch.sum(prod)\n",
    "    return HC\n",
    "import torch\n",
    "\n",
    "def min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The expected min-cut value, computed as a differentiable loss.\n",
    "    \"\"\"\n",
    "    V = s.size(0)  # Number of nodes\n",
    "    K = s.size(1)  # Number of partitions\n",
    "\n",
    "    # Ensure the partition matrix s sums to 1 over partitions\n",
    "    s = torch.softmax(s, dim=1)\n",
    "\n",
    "    # Compute the expected weight of edges within each partition\n",
    "    intra_partition_cut = torch.zeros((K, K), dtype=torch.float32)\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Probability that a node pair (i, j) is split between partitions k and l\n",
    "            partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "            partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "\n",
    "            # Compute the expected weight of the cut edges between partitions k and l\n",
    "            cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "            intra_partition_cut[k, l] = torch.sum(cut_weight)\n",
    "\n",
    "    # Sum up all contributions to get the total expected min-cut value\n",
    "    total_cut_weight = torch.sum(intra_partition_cut)\n",
    "\n",
    "    return total_cut_weight\n",
    "\n",
    "import torch\n",
    "\n",
    "# def min_cut_loss(s, adjacency_matrix):\n",
    "#     \"\"\"\n",
    "#     Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "#\n",
    "#     :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "#     :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "#     :return: The expected min-cut value, computed as a differentiable loss.\n",
    "#     \"\"\"\n",
    "#     V = s.size(0)  # Number of nodes\n",
    "#     K = s.size(1)  # Number of partitions\n",
    "#\n",
    "#     # Ensure the partition matrix s sums to 1 over partitions\n",
    "#     # s = torch.softmax(s, dim=1)\n",
    "#\n",
    "#     # Compute the expected weight of cut edges between each pair of partitions\n",
    "#     total_cut_weight = 0\n",
    "#     for k in range(K):\n",
    "#         for l in range(k + 1, K):\n",
    "#             # Probability that a node pair (i, j) is split between partitions k and l\n",
    "#             partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "#             partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "#\n",
    "#             # Compute the expected weight of the cut edges between partitions k and l\n",
    "#             cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "#             total_cut_weight += torch.sum(cut_weight)\n",
    "#\n",
    "#     return total_cut_weight\n",
    "\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC for soft partitioning.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # Initialize HC to 0\n",
    "    HC = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to HC\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to HC for partition k\n",
    "        HC_k =adjacency_matrix * (1 - partition_prob_matrix)\n",
    "        # Sum up the contributions for partition k\n",
    "        HC += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    HC = HC / 2\n",
    "\n",
    "    return HC\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor(30.)\n",
      "tensor(10.)\n",
      "tensor(10.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "s = torch.Tensor([[0,1,0],[0,1,0],[0,0,1]])\n",
    "# print(calculate_HA_vectorized(s))\n",
    "# print(calculate_HA_vectorized(torch.Tensor([[0,0.9,0.9],[0.9,0.9,0],[0,0,0.9]])))\n",
    "terminal_loss = torch.abs(s[0] - s[1]-s[2])\n",
    "# print(terminal_loss)\n",
    "# print(10 * (1 - terminal_loss))\n",
    "# print(torch.sum(10 * (1 - terminal_loss)))\n",
    "print(torch.abs(s[0] - s[1]))\n",
    "print(torch.abs(s[0] - s[2]))\n",
    "print(torch.abs(s[2] - s[1]))\n",
    "\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[1]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[2]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[2] - s[1]))))\n",
    "print(terminal_independence_penalty(s, [0,1,2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train1(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(learning_rate=0.001, n=4096,patience=20)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    datasetItem = open_file('./testData/prepareDS.pkl')\n",
    "    # print(datasetItem)\n",
    "    # datasetItem_all = {}\n",
    "    # for key, (dgl_graph, adjacency_matrix,graph) in datasetItem.items():\n",
    "    #     A, C = FIndAC(graph)\n",
    "    #     datasetItem_all[key] = [dgl_graph, adjacency_matrix, graph, A, C]\n",
    "\n",
    "    # print(len(datasetItem), datasetItem[0][3])\n",
    "    # datasetItem_2 = {}\n",
    "    # datasetItem_2[0]=datasetItem[1]\n",
    "    # print(datasetItem_2)\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "\n",
    "    # print(datasetItem[1][2].nodes)\n",
    "    # # Visualize graph\n",
    "    # pos = nx.kamada_kawai_layout(datasetItem[1][2])\n",
    "    # nx.draw(datasetItem[1][2], pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
    "    # cut_value, (part_1, part_2) = nx.minimum_cut(datasetItem_2[0][2], datasetItem_2[0][3][1], datasetItem_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "    # print(cut_value, len(part_1), len(part_2))\n",
    "\n",
    "    # resultList = []\n",
    "    # all_indexes = sorted(part_1.union(part_2))\n",
    "    # # Check membership for each index and append the appropriate pair to the result list\n",
    "    # for index in all_indexes:\n",
    "    #     if index in part_1:\n",
    "    #         resultList.append([1, 0])\n",
    "    #     elif index in part_2:\n",
    "    #         resultList.append([0, 1])\n",
    "\n",
    "    #\n",
    "    trained_net, bestLost, epoch, inp, lossList= run_gnn_training2(\n",
    "        datasetItem, net, optimizer, int(500),\n",
    "        gnn_hypers['tolerance'], gnn_hypers['patience'], loss_terminal,gnn_hypers['dim_embedding'], gnn_hypers['number_classes'], modelName,  TORCH_DTYPE,  TORCH_DEVICE)\n",
    "\n",
    "    return trained_net, bestLost, epoch, inp, lossList\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Training, Setting A to 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Establishin baseline\n",
    "\n",
    "Neural network model to establish baseline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossOrig_2.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 2629117.7470703125\n",
      "Epoch: 100, Cumulative Loss: 2603246.4545898438\n",
      "Epoch: 200, Cumulative Loss: 2603246.4545898438\n",
      "Epoch: 300, Cumulative Loss: 2603246.4545898438\n",
      "Epoch: 400, Cumulative Loss: 2603246.4545898438\n",
      "GNN training took 1388.975 seconds.\n",
      "Best cumulative loss: 10772.7900390625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k @ partition_l)\n",
    "            min_cut_loss += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return min_cut_loss\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A= 0, C=1, T=1000):\n",
    "    \"\"\"\n",
    "    Compute the overall loss including cut loss and terminal independence.\n",
    "\n",
    "    :param s: Node partition probabilities.\n",
    "    :param adjacency_matrix: Graph adjacency matrix.\n",
    "    :param terminals: List of terminal node indices.\n",
    "    :param C: Weight for the cut loss.\n",
    "    :param T: Weight for the terminal independence penalty.\n",
    "    :return: Total loss.\n",
    "    \"\"\"\n",
    "    cut_loss = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    terminal_loss = terminal_independence_penalty(s, [0,1,2])\n",
    "    total_loss = C * cut_loss + T * terminal_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossBaseline.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXP 1\n",
    "\n",
    "Removing terminal penalties, settings contant for terminal 0, 1, 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 167\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# loss += penalty* terminal_independence_penalty(s, [0,1,2])\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n\u001B[0;32m--> 167\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossExp1.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[63], line 57\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     56\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 57\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# Update cumulative loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp1.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp2\n",
    "Removing terminal penalty, now using cloning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1089309.7293701172\n",
      "Epoch: 100, Cumulative Loss: 1025623.7824707031\n",
      "Epoch: 200, Cumulative Loss: 1025238.1572265625\n",
      "Epoch: 300, Cumulative Loss: 1024769.947265625\n",
      "Epoch: 400, Cumulative Loss: 1024636.330078125\n",
      "GNN training took 191.703 seconds.\n",
      "Best cumulative loss: 4242.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXP 3\n",
    "\n",
    "Removing terminal penalties, settings contant for terminal 0, 1, 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # Fix the outputs for nodes 0, 1, and 2\n",
    "        h[0] = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)\n",
    "        h[1] = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32)\n",
    "        h[2] = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float32)\n",
    "\n",
    "        return h\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossExp2.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[23], line 51\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     50\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 51\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Update cumulative loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modifying the loss function\n",
    "## exp 1 - loss\n",
    "expriment 1 of modifying the loss function + keeping terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 4785276.850097656\n",
      "Stopping early at epoch 40\n",
      "Epoch: 100, Cumulative Loss: 1663353.253540039\n",
      "Epoch: 200, Cumulative Loss: 1438121.930053711\n",
      "Stopping early at epoch 209\n",
      "Stopping early at epoch 300\n",
      "Epoch: 300, Cumulative Loss: 1558179.4991455078\n",
      "Stopping early at epoch 328\n",
      "Stopping early at epoch 365\n",
      "Epoch: 400, Cumulative Loss: 1358117.3784179688\n",
      "GNN training took 186.82 seconds.\n",
      "Best cumulative loss: 4739.478515625\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp1_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modifying the loss function\n",
    "## exp 2 - loss\n",
    "expriment 2 of modifying the loss function + removing terminal indepdendence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1110715.6811523438\n",
      "Epoch: 100, Cumulative Loss: 1104329.0\n",
      "Epoch: 200, Cumulative Loss: 1104329.0\n",
      "Epoch: 300, Cumulative Loss: 1104329.0\n",
      "Epoch: 400, Cumulative Loss: 1104329.0\n",
      "GNN training took 190.264 seconds.\n",
      "Best cumulative loss: 4671.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp2_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modifying the loss function\n",
    "## exp 3 - loss\n",
    "expriment 3 of modifying the loss function (explicitly making the max proab to 1) + keeping terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1628496.0\n",
      "Epoch: 100, Cumulative Loss: 1176600.0\n",
      "Epoch: 200, Cumulative Loss: 1148917.0\n",
      "Epoch: 300, Cumulative Loss: 1143571.0\n",
      "Epoch: 400, Cumulative Loss: 1138089.0\n",
      "GNN training took 339.987 seconds.\n",
      "Best cumulative loss: 4553.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp3_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## exp 4\n",
    "binary loss function with fixed penality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1089103.0\n",
      "Epoch: 100, Cumulative Loss: 1064404.0\n",
      "Epoch: 200, Cumulative Loss: 1064118.0\n",
      "Epoch: 300, Cumulative Loss: 1064103.0\n",
      "Epoch: 400, Cumulative Loss: 1064054.0\n",
      "GNN training took 501.735 seconds.\n",
      "Best cumulative loss: 4541.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp4_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 5 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 122807.00138473511\n",
      "Epoch: 100, Cumulative Loss: 30866.000024795532\n",
      "Epoch: 200, Cumulative Loss: 30751.000003814697\n",
      "Epoch: 300, Cumulative Loss: 30877.0\n",
      "Epoch: 400, Cumulative Loss: 30831.000007629395\n",
      "GNN training took 420.717 seconds.\n",
      "Best cumulative loss: 127.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp5_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 6 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function + terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 3042784.951171875\n",
      "Stopping early at epoch 76\n",
      "Stopping early at epoch 80\n",
      "Epoch: 100, Cumulative Loss: 619306.998260498\n",
      "Stopping early at epoch 168\n",
      "Stopping early at epoch 194\n",
      "Epoch: 200, Cumulative Loss: 470624.99826049805\n",
      "Stopping early at epoch 221\n",
      "Stopping early at epoch 222\n",
      "Stopping early at epoch 282\n",
      "Stopping early at epoch 283\n",
      "Epoch: 300, Cumulative Loss: 421319.9992828369\n",
      "Stopping early at epoch 306\n",
      "Stopping early at epoch 327\n",
      "Stopping early at epoch 352\n",
      "Stopping early at epoch 353\n",
      "Stopping early at epoch 377\n",
      "Stopping early at epoch 378\n",
      "Stopping early at epoch 379\n",
      "Epoch: 400, Cumulative Loss: 408950.99939346313\n",
      "Stopping early at epoch 411\n",
      "Stopping early at epoch 412\n",
      "Stopping early at epoch 440\n",
      "Stopping early at epoch 441\n",
      "Stopping early at epoch 474\n",
      "Stopping early at epoch 475\n",
      "Stopping early at epoch 476\n",
      "GNN training took 334.293 seconds.\n",
      "Best cumulative loss: 472.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp6_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 7 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each pair of nodes to compute the contribution to the cut value\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):  # Only consider upper triangle to avoid double counting\n",
    "            loss += (1 - (s[i] * s[j]).sum())*adjacency_matrix[i, j]\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 96833.00001907349\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp7_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences:\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000, -0.2000, -0.1000],\n",
      "         [ 0.1000, -0.1000,  0.0000]],\n",
      "\n",
      "        [[-0.3000,  0.2000,  0.1000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.2000,  0.1000,  0.1000]],\n",
      "\n",
      "        [[-0.1000,  0.1000,  0.0000],\n",
      "         [ 0.2000, -0.1000, -0.1000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]])\n",
      "Absolute differences:\n",
      " tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.3000, 0.2000, 0.1000],\n",
      "         [0.1000, 0.1000, 0.0000]],\n",
      "\n",
      "        [[0.3000, 0.2000, 0.1000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.2000, 0.1000, 0.1000]],\n",
      "\n",
      "        [[0.1000, 0.1000, 0.0000],\n",
      "         [0.2000, 0.1000, 0.1000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "Summed differences:\n",
      " tensor([[0.0000, 0.6000, 0.2000],\n",
      "        [0.6000, 0.0000, 0.4000],\n",
      "        [0.2000, 0.4000, 0.0000]])\n",
      "Weighted summed differences:\n",
      " tensor([[0.0000, 0.6000, 0.1000],\n",
      "        [0.6000, 0.0000, 0.0800],\n",
      "        [0.1000, 0.0800, 0.0000]])\n",
      "Min Cut Loss:\n",
      " tensor(1.5600)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Adjusted example for 3 partitions\n",
    "s = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.5, 0.3], [0.4, 0.4, 0.2]])\n",
    "adjacency_matrix = torch.tensor([[0.0, 1.0, 0.5], [1.0, 0.0, 0.2], [0.5, 0.2, 0.0]])\n",
    "\n",
    "# Compute differences\n",
    "diff = s.unsqueeze(1) - s.unsqueeze(0)\n",
    "abs_diff = torch.abs(diff)\n",
    "sum_diff = torch.sum(abs_diff, dim=2)\n",
    "min_cut_loss = torch.sum(sum_diff * adjacency_matrix)\n",
    "\n",
    "# Print results\n",
    "print(\"Differences:\\n\", diff)\n",
    "print(\"Absolute differences:\\n\", abs_diff)\n",
    "print(\"Summed differences:\\n\", sum_diff)\n",
    "print(\"Weighted summed differences:\\n\", sum_diff * adjacency_matrix)\n",
    "print(\"Min Cut Loss:\\n\", min_cut_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.], grad_fn=<SubBackward0>)\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "# Example usage\n",
    "tensor = torch.tensor([0.3, 0.1, 0.6], requires_grad=True)\n",
    "one_hot_tensor = max_to_one_hot(tensor)\n",
    "print(one_hot_tensor)\n",
    "\n",
    "# To check if it backpropagates correctly\n",
    "one_hot_tensor.sum().backward()\n",
    "print(tensor.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each pair of nodes to compute the contribution to the cut value\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):  # Only consider upper triangle to avoid double counting\n",
    "            # if adjacency_matrix[i, j] > 0:\n",
    "                # Check if nodes i and j are in different partitions\n",
    "                # in_different_partitions = (s[i] * s[j]).sum() == 0\n",
    "                # if in_different_partitions:\n",
    "                    loss += (1 - (s[i] * s[j]).sum())*adjacency_matrix[i, j]\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "s = torch.tensor([[1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [0, 1]], dtype=torch.float32)\n",
    "\n",
    "adjacency_matrix = torch.tensor([[0, 2, 1, 0],\n",
    "                                 [2, 0, 5, 1],\n",
    "                                 [1, 5, 0, 1],\n",
    "                                 [0, 1, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Compute the loss\n",
    "loss_value = min_cut_loss(s, adjacency_matrix)\n",
    "print(f\"Loss: {loss_value.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vectorized_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "s = torch.tensor([[1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [0, 1]], dtype=torch.float32)\n",
    "\n",
    "adjacency_matrix = torch.tensor([[0, 2, 1, 0],\n",
    "                                 [2, 0, 5, 1],\n",
    "                                 [1, 5, 0, 1],\n",
    "                                 [0, 1, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Compute the loss\n",
    "loss_value = vectorized_min_cut_loss(s, adjacency_matrix)\n",
    "print(f\"Loss: {loss_value.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}