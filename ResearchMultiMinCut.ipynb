{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from commons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separate code base from heurestics\n",
    "# Utils code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.float32\n",
    "\n",
    "\n",
    "def partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the partition of each edge (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.not_equal.outer(s, s).astype(int)\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "    return weight\n",
    "\n",
    "import torch\n",
    "\n",
    "def partition_weight2(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph as a PyTorch tensor.\n",
    "    :param s: Tensor indicating the partition of each node (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    # Ensure s is a tensor\n",
    "    # s = torch.tensor(s, dtype=torch.float32)\n",
    "\n",
    "    # Compute outer difference to create partition matrix\n",
    "    s = s.unsqueeze(0)  # Convert s to a row vector\n",
    "    t = s.t()           # Transpose s to a column vector\n",
    "    partition_matrix = (s != t).float()  # Compute outer product and convert boolean to float\n",
    "\n",
    "    # Calculate the weight of edges between different partitions\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut(q_torch, s):\n",
    "    '''\n",
    "\n",
    "    :param q_torch: The adjacent matrix of the graph\n",
    "    :param s: The binary output from the neural network. s will be in form of [[prob1, prob2, ..., prob n], ...]\n",
    "    :return: The calculated cut loss value\n",
    "    '''\n",
    "    if len(s) > 0:\n",
    "        totalCuts = len(s[0])\n",
    "        CutValue = 0\n",
    "        for i in range(totalCuts):\n",
    "            CutValue += partition_weight2(q_torch, s[:,i])\n",
    "        return CutValue/2\n",
    "    return 0\n",
    "\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 80 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def FIndAC(graph):\n",
    "    max_degree = max(dict(graph.degree()).values())\n",
    "    A_initial = max_degree + 1  # A is set to be one more than the maximum degree\n",
    "    C_initial = max_degree / 2  # C is set to half the maximum degree\n",
    "\n",
    "    return A_initial, C_initial\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HyperParameters initialization and related functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def printCombo(orig):\n",
    "    # Original dictionary\n",
    "    input_dict = orig\n",
    "\n",
    "    # Generate all permutations of the dictionary values\n",
    "    value_permutations = list(permutations(input_dict.values()))\n",
    "\n",
    "    # Create a list of dictionaries from the permutations\n",
    "    permuted_dicts = [{key: value for key, value in zip(input_dict.keys(), perm)} for perm in value_permutations]\n",
    "\n",
    "    return permuted_dicts\n",
    "\n",
    "def GetOptimalNetValue(net, dgl_graph, inp, q_torch, terminal_dict):\n",
    "    net.eval()\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if (dgl_graph.number_of_nodes() < 30):\n",
    "        inp = torch.ones((dgl_graph.number_of_nodes(), 30))\n",
    "\n",
    "    # find all potential combination of terminal nodes with respective indices\n",
    "\n",
    "    perm_items = printCombo(terminal_dict)\n",
    "    for i in perm_items:\n",
    "        probs = net(dgl_graph, inp, i)\n",
    "        binary_partitions = (probs >= 0.5).float()\n",
    "        cut_value_item = calculateAllCut(q_torch, binary_partitions)\n",
    "        if cut_value_item < best_loss:\n",
    "            best_loss = cut_value_item\n",
    "    return best_loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hamiltonian loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def terminal_independence_penalty(s, terminal_nodes):\n",
    "    \"\"\"\n",
    "    Calculate a penalty that enforces each terminal node to be in a distinct partition.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param terminal_nodes: A list of indices for terminal nodes.\n",
    "    :return: The penalty term.\n",
    "    \"\"\"\n",
    "    penalty = 0\n",
    "    num_terminals = len(terminal_nodes)\n",
    "    # Compare each pair of terminal nodes\n",
    "    for i in range(num_terminals):\n",
    "        for j in range(i + 1, num_terminals):\n",
    "            # Calculate the dot product of the probability vectors for the two terminals\n",
    "            dot_product = torch.dot(s[terminal_nodes[i]], s[terminal_nodes[j]])\n",
    "            # Penalize the similarity in their partition assignments (dot product should be close to 0)\n",
    "            penalty += dot_product\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter2(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_new(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Differentiable calculation of HC for minimizing edge weights between different partitions.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value, focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    K = s.shape[1]\n",
    "    V = s.shape[0]\n",
    "\n",
    "    # Create a full partition matrix indicating the likelihood of each node pair being in the same partition\n",
    "    partition_matrix = torch.matmul(s, s.T)\n",
    "\n",
    "    # Calculate the complement matrix, which indicates the likelihood of node pairs being in different partitions\n",
    "    complement_matrix = 1 - partition_matrix\n",
    "\n",
    "    # Apply adjacency matrix to only consider actual edges and their weights\n",
    "    inter_partition_edges = adjacency_matrix * complement_matrix\n",
    "\n",
    "    # Summing up all contributions for edges between different partitions\n",
    "    HC = torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_vectorized_old(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # HC = ∑(u,v)∈E(1−∑k∈K(su,k*sv,k))*adjacency_matrix[u,v]\n",
    "    K = s.shape[1]\n",
    "    # Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\n",
    "    prod = adjacency_matrix * (1 - s @ s.T)\n",
    "    HC = torch.sum(prod)\n",
    "    return HC\n",
    "import torch\n",
    "\n",
    "def min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The expected min-cut value, computed as a differentiable loss.\n",
    "    \"\"\"\n",
    "    V = s.size(0)  # Number of nodes\n",
    "    K = s.size(1)  # Number of partitions\n",
    "\n",
    "    # Ensure the partition matrix s sums to 1 over partitions\n",
    "    s = torch.softmax(s, dim=1)\n",
    "\n",
    "    # Compute the expected weight of edges within each partition\n",
    "    intra_partition_cut = torch.zeros((K, K), dtype=torch.float32)\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Probability that a node pair (i, j) is split between partitions k and l\n",
    "            partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "            partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "\n",
    "            # Compute the expected weight of the cut edges between partitions k and l\n",
    "            cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "            intra_partition_cut[k, l] = torch.sum(cut_weight)\n",
    "\n",
    "    # Sum up all contributions to get the total expected min-cut value\n",
    "    total_cut_weight = torch.sum(intra_partition_cut)\n",
    "\n",
    "    return total_cut_weight\n",
    "\n",
    "import torch\n",
    "\n",
    "# def min_cut_loss(s, adjacency_matrix):\n",
    "#     \"\"\"\n",
    "#     Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "#\n",
    "#     :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "#     :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "#     :return: The expected min-cut value, computed as a differentiable loss.\n",
    "#     \"\"\"\n",
    "#     V = s.size(0)  # Number of nodes\n",
    "#     K = s.size(1)  # Number of partitions\n",
    "#\n",
    "#     # Ensure the partition matrix s sums to 1 over partitions\n",
    "#     # s = torch.softmax(s, dim=1)\n",
    "#\n",
    "#     # Compute the expected weight of cut edges between each pair of partitions\n",
    "#     total_cut_weight = 0\n",
    "#     for k in range(K):\n",
    "#         for l in range(k + 1, K):\n",
    "#             # Probability that a node pair (i, j) is split between partitions k and l\n",
    "#             partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "#             partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "#\n",
    "#             # Compute the expected weight of the cut edges between partitions k and l\n",
    "#             cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "#             total_cut_weight += torch.sum(cut_weight)\n",
    "#\n",
    "#     return total_cut_weight\n",
    "\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC for soft partitioning.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # Initialize HC to 0\n",
    "    HC = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to HC\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to HC for partition k\n",
    "        HC_k =adjacency_matrix * (1 - partition_prob_matrix)\n",
    "        # Sum up the contributions for partition k\n",
    "        HC += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    HC = HC / 2\n",
    "\n",
    "    return HC\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor(30.)\n",
      "tensor(10.)\n",
      "tensor(10.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "s = torch.Tensor([[0,1,0],[0,1,0],[0,0,1]])\n",
    "# print(calculate_HA_vectorized(s))\n",
    "# print(calculate_HA_vectorized(torch.Tensor([[0,0.9,0.9],[0.9,0.9,0],[0,0,0.9]])))\n",
    "terminal_loss = torch.abs(s[0] - s[1]-s[2])\n",
    "# print(terminal_loss)\n",
    "# print(10 * (1 - terminal_loss))\n",
    "# print(torch.sum(10 * (1 - terminal_loss)))\n",
    "print(torch.abs(s[0] - s[1]))\n",
    "print(torch.abs(s[0] - s[2]))\n",
    "print(torch.abs(s[2] - s[1]))\n",
    "\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[1]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[2]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[2] - s[1]))))\n",
    "print(terminal_independence_penalty(s, [0,1,2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train1(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(learning_rate=0.001, n=4096,patience=20)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    datasetItem = open_file('./testData/prepareDS.pkl')\n",
    "    # print(datasetItem)\n",
    "    # datasetItem_all = {}\n",
    "    # for key, (dgl_graph, adjacency_matrix,graph) in datasetItem.items():\n",
    "    #     A, C = FIndAC(graph)\n",
    "    #     datasetItem_all[key] = [dgl_graph, adjacency_matrix, graph, A, C]\n",
    "\n",
    "    # print(len(datasetItem), datasetItem[0][3])\n",
    "    # datasetItem_2 = {}\n",
    "    # datasetItem_2[0]=datasetItem[1]\n",
    "    # print(datasetItem_2)\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "\n",
    "    # print(datasetItem[1][2].nodes)\n",
    "    # # Visualize graph\n",
    "    # pos = nx.kamada_kawai_layout(datasetItem[1][2])\n",
    "    # nx.draw(datasetItem[1][2], pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
    "    # cut_value, (part_1, part_2) = nx.minimum_cut(datasetItem_2[0][2], datasetItem_2[0][3][1], datasetItem_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "    # print(cut_value, len(part_1), len(part_2))\n",
    "\n",
    "    # resultList = []\n",
    "    # all_indexes = sorted(part_1.union(part_2))\n",
    "    # # Check membership for each index and append the appropriate pair to the result list\n",
    "    # for index in all_indexes:\n",
    "    #     if index in part_1:\n",
    "    #         resultList.append([1, 0])\n",
    "    #     elif index in part_2:\n",
    "    #         resultList.append([0, 1])\n",
    "\n",
    "    #\n",
    "    trained_net, bestLost, epoch, inp, lossList= run_gnn_training2(\n",
    "        datasetItem, net, optimizer, int(500),\n",
    "        gnn_hypers['tolerance'], gnn_hypers['patience'], loss_terminal,gnn_hypers['dim_embedding'], gnn_hypers['number_classes'], modelName,  TORCH_DTYPE,  TORCH_DEVICE)\n",
    "\n",
    "    return trained_net, bestLost, epoch, inp, lossList\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Training, Setting A to 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 4467581.311767578\n",
      "Epoch: 100, Cumulative Loss: 1668383.3168945312\n",
      "Stopping early at epoch 109\n",
      "Epoch: 200, Cumulative Loss: 1490733.4799804688\n",
      "Stopping early at epoch 206\n",
      "Stopping early at epoch 234\n",
      "Epoch: 300, Cumulative Loss: 1417838.9104003906\n",
      "Epoch: 400, Cumulative Loss: 1397359.0704345703\n",
      "Stopping early at epoch 465\n",
      "Stopping early at epoch 466\n",
      "Stopping early at epoch 498\n",
      "GNN training took 221.145 seconds.\n",
      "Best cumulative loss: 4581.91259765625\n"
     ]
    }
   ],
   "source": [
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    # HA = calculate_HA_vectorized(s)\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_min_cut_new(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_min_cut_intra_inter(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossOrig_2.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 2671551.08203125\n",
      "Epoch: 100, Cumulative Loss: 2461037.6333007812\n",
      "Epoch: 200, Cumulative Loss: 2450037.1362304688\n",
      "Epoch: 300, Cumulative Loss: 2447161.0883789062\n",
      "Epoch: 400, Cumulative Loss: 2449458.3696289062\n",
      "GNN training took 1144.661 seconds.\n",
      "Best cumulative loss: 10010.3642578125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Use probabilities directly for nodes being in partitions k and l\n",
    "            # partition_k = s[:, k].unsqueeze(1)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "            # Edge weights between partitions\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k @ partition_l)\n",
    "            min_cut_loss += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return min_cut_loss\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A= 0, C=1, T=100):\n",
    "    \"\"\"\n",
    "    Compute the overall loss including cut loss and terminal independence.\n",
    "\n",
    "    :param s: Node partition probabilities.\n",
    "    :param adjacency_matrix: Graph adjacency matrix.\n",
    "    :param terminals: List of terminal node indices.\n",
    "    :param C: Weight for the cut loss.\n",
    "    :param T: Weight for the terminal independence penalty.\n",
    "    :return: Total loss.\n",
    "    \"\"\"\n",
    "    cut_loss = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    terminal_loss = terminal_independence_penalty(s, [0,1,2])\n",
    "    total_loss = C * cut_loss + T * terminal_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter_min_cut_loss_9_new.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 80112.09645080566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 43\u001B[0m\n\u001B[1;32m     39\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m C \u001B[38;5;241m*\u001B[39m cut_loss \u001B[38;5;241m+\u001B[39m T \u001B[38;5;241m*\u001B[39m terminal_loss\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m total_loss\n\u001B[0;32m---> 43\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_Lossinter_min_cut_loss_10_new.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[20], line 53\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     52\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 53\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Update cumulative loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/function.py:277\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBackwardCFunction\u001B[39;00m(_C\u001B[38;5;241m.\u001B[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;66;03m# _forward_cls is defined by derived class\u001B[39;00m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;66;03m# The user should define either backward or vjp but never both.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m         backward_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mbackward  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    281\u001B[0m         vjp_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mvjp  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(V):\n",
    "        for l in range(k + 1, V):\n",
    "            # Use probabilities directly for nodes being in partitions k and l\n",
    "            partition_k = s[k]\n",
    "            partition_l = s[l]\n",
    "            # Edge weights between partitions\n",
    "            summation = torch.sum(torch.abs_( partition_k-partition_l))\n",
    "\n",
    "            min_cut_loss += torch.sum(summation * adjacency_matrix[k][l])\n",
    "\n",
    "    return min_cut_loss\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A= 0, C=1, T=100):\n",
    "    \"\"\"\n",
    "    Compute the overall loss including cut loss and terminal independence.\n",
    "\n",
    "    :param s: Node partition probabilities.\n",
    "    :param adjacency_matrix: Graph adjacency matrix.\n",
    "    :param terminals: List of terminal node indices.\n",
    "    :param C: Weight for the cut loss.\n",
    "    :param T: Weight for the terminal independence penalty.\n",
    "    :return: Total loss.\n",
    "    \"\"\"\n",
    "    cut_loss = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    terminal_loss = terminal_independence_penalty(s, [0,1,2])\n",
    "    total_loss = C * cut_loss + T * terminal_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter_min_cut_loss_10_new.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 681368.5506591797\n",
      "Epoch: 100, Cumulative Loss: 216319.7434539795\n",
      "Epoch: 200, Cumulative Loss: 195802.2372970581\n",
      "Stopping early at epoch 248\n",
      "Stopping early at epoch 268\n",
      "Stopping early at epoch 286\n",
      "Epoch: 300, Cumulative Loss: 187731.35697174072\n",
      "Stopping early at epoch 325\n",
      "Stopping early at epoch 349\n",
      "Epoch: 400, Cumulative Loss: 176453.22048187256\n",
      "Stopping early at epoch 484\n",
      "GNN training took 192.318 seconds.\n",
      "Best cumulative loss: 705.4923706054688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "\n",
    "    # Compute differences\n",
    "    diff = s.unsqueeze(1) - s.unsqueeze(0)\n",
    "    abs_diff = torch.abs(diff)\n",
    "    sum_diff = torch.sum(abs_diff, dim=2)\n",
    "    min_cut_loss = torch.sum(sum_diff * adjacency_matrix)\n",
    "\n",
    "    return min_cut_loss\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A= 0, C=1, T=1000):\n",
    "    \"\"\"\n",
    "    Compute the overall loss including cut loss and terminal independence.\n",
    "\n",
    "    :param s: Node partition probabilities.\n",
    "    :param adjacency_matrix: Graph adjacency matrix.\n",
    "    :param terminals: List of terminal node indices.\n",
    "    :param C: Weight for the cut loss.\n",
    "    :param T: Weight for the terminal independence penalty.\n",
    "    :return: Total loss.\n",
    "    \"\"\"\n",
    "    cut_loss = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    terminal_loss = terminal_independence_penalty(s, [0,1,2])\n",
    "    total_loss = C * cut_loss + T * terminal_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter_min_cut_loss_10_new.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences:\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000, -0.2000, -0.1000],\n",
      "         [ 0.1000, -0.1000,  0.0000]],\n",
      "\n",
      "        [[-0.3000,  0.2000,  0.1000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.2000,  0.1000,  0.1000]],\n",
      "\n",
      "        [[-0.1000,  0.1000,  0.0000],\n",
      "         [ 0.2000, -0.1000, -0.1000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]])\n",
      "Absolute differences:\n",
      " tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.3000, 0.2000, 0.1000],\n",
      "         [0.1000, 0.1000, 0.0000]],\n",
      "\n",
      "        [[0.3000, 0.2000, 0.1000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.2000, 0.1000, 0.1000]],\n",
      "\n",
      "        [[0.1000, 0.1000, 0.0000],\n",
      "         [0.2000, 0.1000, 0.1000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "Summed differences:\n",
      " tensor([[0.0000, 0.6000, 0.2000],\n",
      "        [0.6000, 0.0000, 0.4000],\n",
      "        [0.2000, 0.4000, 0.0000]])\n",
      "Weighted summed differences:\n",
      " tensor([[0.0000, 0.6000, 0.1000],\n",
      "        [0.6000, 0.0000, 0.0800],\n",
      "        [0.1000, 0.0800, 0.0000]])\n",
      "Min Cut Loss:\n",
      " tensor(1.5600)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Adjusted example for 3 partitions\n",
    "s = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.5, 0.3], [0.4, 0.4, 0.2]])\n",
    "adjacency_matrix = torch.tensor([[0.0, 1.0, 0.5], [1.0, 0.0, 0.2], [0.5, 0.2, 0.0]])\n",
    "\n",
    "# Compute differences\n",
    "diff = s.unsqueeze(1) - s.unsqueeze(0)\n",
    "abs_diff = torch.abs(diff)\n",
    "sum_diff = torch.sum(abs_diff, dim=2)\n",
    "min_cut_loss = torch.sum(sum_diff * adjacency_matrix)\n",
    "\n",
    "# Print results\n",
    "print(\"Differences:\\n\", diff)\n",
    "print(\"Absolute differences:\\n\", abs_diff)\n",
    "print(\"Summed differences:\\n\", sum_diff)\n",
    "print(\"Weighted summed differences:\\n\", sum_diff * adjacency_matrix)\n",
    "print(\"Min Cut Loss:\\n\", min_cut_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}