{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from commons import *\n",
    "from dgl.nn.pytorch import GATConv, EdgeConv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separate code base from heurestics\n",
    "# Utils code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.float32\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCNSoftmax(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer\n",
    "\n",
    "def partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the partition of each edge (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.not_equal.outer(s, s).astype(int)\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "    return weight\n",
    "\n",
    "import torch\n",
    "\n",
    "def partition_weight2(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph as a PyTorch tensor.\n",
    "    :param s: Tensor indicating the partition of each node (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    # Ensure s is a tensor\n",
    "    # s = torch.tensor(s, dtype=torch.float32)\n",
    "\n",
    "    # Compute outer difference to create partition matrix\n",
    "    s = s.unsqueeze(0)  # Convert s to a row vector\n",
    "    t = s.t()           # Transpose s to a column vector\n",
    "    partition_matrix = (s != t).float()  # Compute outer product and convert boolean to float\n",
    "\n",
    "    # Calculate the weight of edges between different partitions\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut(q_torch, s):\n",
    "    '''\n",
    "\n",
    "    :param q_torch: The adjacent matrix of the graph\n",
    "    :param s: The binary output from the neural network. s will be in form of [[prob1, prob2, ..., prob n], ...]\n",
    "    :return: The calculated cut loss value\n",
    "    '''\n",
    "    if len(s) > 0:\n",
    "        totalCuts = len(s[0])\n",
    "        CutValue = 0\n",
    "        for i in range(totalCuts):\n",
    "            CutValue += partition_weight2(q_torch, s[:,i])\n",
    "        return CutValue/2\n",
    "    return 0\n",
    "\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 80 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def FIndAC(graph):\n",
    "    max_degree = max(dict(graph.degree()).values())\n",
    "    A_initial = max_degree + 1  # A is set to be one more than the maximum degree\n",
    "    C_initial = max_degree / 2  # C is set to half the maximum degree\n",
    "\n",
    "    return A_initial, C_initial\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HyperParameters initialization and related functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def printCombo(orig):\n",
    "    # Original dictionary\n",
    "    input_dict = orig\n",
    "\n",
    "    # Generate all permutations of the dictionary values\n",
    "    value_permutations = list(permutations(input_dict.values()))\n",
    "\n",
    "    # Create a list of dictionaries from the permutations\n",
    "    permuted_dicts = [{key: value for key, value in zip(input_dict.keys(), perm)} for perm in value_permutations]\n",
    "\n",
    "    return permuted_dicts\n",
    "\n",
    "def GetOptimalNetValue(net, dgl_graph, inp, q_torch, terminal_dict):\n",
    "    net.eval()\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if (dgl_graph.number_of_nodes() < 30):\n",
    "        inp = torch.ones((dgl_graph.number_of_nodes(), 30))\n",
    "\n",
    "    # find all potential combination of terminal nodes with respective indices\n",
    "\n",
    "    perm_items = printCombo(terminal_dict)\n",
    "    for i in perm_items:\n",
    "        probs = net(dgl_graph, inp, i)\n",
    "        binary_partitions = (probs >= 0.5).float()\n",
    "        cut_value_item = calculateAllCut(q_torch, binary_partitions)\n",
    "        if cut_value_item < best_loss:\n",
    "            best_loss = cut_value_item\n",
    "    return best_loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hamiltonian loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def terminal_independence_penalty(s, terminal_nodes):\n",
    "    \"\"\"\n",
    "    Calculate a penalty that enforces each terminal node to be in a distinct partition.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param terminal_nodes: A list of indices for terminal nodes.\n",
    "    :return: The penalty term.\n",
    "    \"\"\"\n",
    "    penalty = 0\n",
    "    num_terminals = len(terminal_nodes)\n",
    "    # Compare each pair of terminal nodes\n",
    "    for i in range(num_terminals):\n",
    "        for j in range(i + 1, num_terminals):\n",
    "            # Calculate the dot product of the probability vectors for the two terminals\n",
    "            dot_product = torch.dot(s[terminal_nodes[i]], s[terminal_nodes[j]])\n",
    "            # Penalize the similarity in their partition assignments (dot product should be close to 0)\n",
    "            penalty += dot_product\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter2(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_new(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Differentiable calculation of HC for minimizing edge weights between different partitions.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value, focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    K = s.shape[1]\n",
    "    V = s.shape[0]\n",
    "\n",
    "    # Create a full partition matrix indicating the likelihood of each node pair being in the same partition\n",
    "    partition_matrix = torch.matmul(s, s.T)\n",
    "\n",
    "    # Calculate the complement matrix, which indicates the likelihood of node pairs being in different partitions\n",
    "    complement_matrix = 1 - partition_matrix\n",
    "\n",
    "    # Apply adjacency matrix to only consider actual edges and their weights\n",
    "    inter_partition_edges = adjacency_matrix * complement_matrix\n",
    "\n",
    "    # Summing up all contributions for edges between different partitions\n",
    "    HC = torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_vectorized_old(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # HC = ∑(u,v)∈E(1−∑k∈K(su,k*sv,k))*adjacency_matrix[u,v]\n",
    "    K = s.shape[1]\n",
    "    # Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\n",
    "    prod = adjacency_matrix * (1 - s @ s.T)\n",
    "    HC = torch.sum(prod)\n",
    "    return HC\n",
    "import torch\n",
    "\n",
    "def min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The expected min-cut value, computed as a differentiable loss.\n",
    "    \"\"\"\n",
    "    V = s.size(0)  # Number of nodes\n",
    "    K = s.size(1)  # Number of partitions\n",
    "\n",
    "    # Ensure the partition matrix s sums to 1 over partitions\n",
    "    s = torch.softmax(s, dim=1)\n",
    "\n",
    "    # Compute the expected weight of edges within each partition\n",
    "    intra_partition_cut = torch.zeros((K, K), dtype=torch.float32)\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Probability that a node pair (i, j) is split between partitions k and l\n",
    "            partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "            partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "\n",
    "            # Compute the expected weight of the cut edges between partitions k and l\n",
    "            cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "            intra_partition_cut[k, l] = torch.sum(cut_weight)\n",
    "\n",
    "    # Sum up all contributions to get the total expected min-cut value\n",
    "    total_cut_weight = torch.sum(intra_partition_cut)\n",
    "\n",
    "    return total_cut_weight\n",
    "\n",
    "import torch\n",
    "\n",
    "# def min_cut_loss(s, adjacency_matrix):\n",
    "#     \"\"\"\n",
    "#     Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "#\n",
    "#     :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "#     :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "#     :return: The expected min-cut value, computed as a differentiable loss.\n",
    "#     \"\"\"\n",
    "#     V = s.size(0)  # Number of nodes\n",
    "#     K = s.size(1)  # Number of partitions\n",
    "#\n",
    "#     # Ensure the partition matrix s sums to 1 over partitions\n",
    "#     # s = torch.softmax(s, dim=1)\n",
    "#\n",
    "#     # Compute the expected weight of cut edges between each pair of partitions\n",
    "#     total_cut_weight = 0\n",
    "#     for k in range(K):\n",
    "#         for l in range(k + 1, K):\n",
    "#             # Probability that a node pair (i, j) is split between partitions k and l\n",
    "#             partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "#             partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "#\n",
    "#             # Compute the expected weight of the cut edges between partitions k and l\n",
    "#             cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "#             total_cut_weight += torch.sum(cut_weight)\n",
    "#\n",
    "#     return total_cut_weight\n",
    "\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC for soft partitioning.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # Initialize HC to 0\n",
    "    HC = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to HC\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to HC for partition k\n",
    "        HC_k =adjacency_matrix * (1 - partition_prob_matrix)\n",
    "        # Sum up the contributions for partition k\n",
    "        HC += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    HC = HC / 2\n",
    "\n",
    "    return HC\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor(30.)\n",
      "tensor(10.)\n",
      "tensor(10.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "s = torch.Tensor([[0,1,0],[0,1,0],[0,0,1]])\n",
    "# print(calculate_HA_vectorized(s))\n",
    "# print(calculate_HA_vectorized(torch.Tensor([[0,0.9,0.9],[0.9,0.9,0],[0,0,0.9]])))\n",
    "terminal_loss = torch.abs(s[0] - s[1]-s[2])\n",
    "# print(terminal_loss)\n",
    "# print(10 * (1 - terminal_loss))\n",
    "# print(torch.sum(10 * (1 - terminal_loss)))\n",
    "print(torch.abs(s[0] - s[1]))\n",
    "print(torch.abs(s[0] - s[2]))\n",
    "print(torch.abs(s[2] - s[1]))\n",
    "\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[1]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[2]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[2] - s[1]))))\n",
    "print(terminal_independence_penalty(s, [0,1,2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def train1(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(learning_rate=0.001, n=4096,patience=20)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    datasetItem = open_file('./testData/prepareDS.pkl')\n",
    "    # print(datasetItem)\n",
    "    # datasetItem_all = {}\n",
    "    # for key, (dgl_graph, adjacency_matrix,graph) in datasetItem.items():\n",
    "    #     A, C = FIndAC(graph)\n",
    "    #     datasetItem_all[key] = [dgl_graph, adjacency_matrix, graph, A, C]\n",
    "\n",
    "    # print(len(datasetItem), datasetItem[0][3])\n",
    "    # datasetItem_2 = {}\n",
    "    # datasetItem_2[0]=datasetItem[1]\n",
    "    # print(datasetItem_2)\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "\n",
    "    # print(datasetItem[1][2].nodes)\n",
    "    # # Visualize graph\n",
    "    # pos = nx.kamada_kawai_layout(datasetItem[1][2])\n",
    "    # nx.draw(datasetItem[1][2], pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
    "    # cut_value, (part_1, part_2) = nx.minimum_cut(datasetItem_2[0][2], datasetItem_2[0][3][1], datasetItem_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "    # print(cut_value, len(part_1), len(part_2))\n",
    "\n",
    "    # resultList = []\n",
    "    # all_indexes = sorted(part_1.union(part_2))\n",
    "    # # Check membership for each index and append the appropriate pair to the result list\n",
    "    # for index in all_indexes:\n",
    "    #     if index in part_1:\n",
    "    #         resultList.append([1, 0])\n",
    "    #     elif index in part_2:\n",
    "    #         resultList.append([0, 1])\n",
    "\n",
    "    #\n",
    "    trained_net, bestLost, epoch, inp, lossList= run_gnn_training2(\n",
    "        datasetItem, net, optimizer, int(500),\n",
    "        gnn_hypers['tolerance'], gnn_hypers['patience'], loss_terminal,gnn_hypers['dim_embedding'], gnn_hypers['number_classes'], modelName,  TORCH_DTYPE,  TORCH_DEVICE)\n",
    "\n",
    "    return trained_net, bestLost, epoch, inp, lossList\n",
    "\n",
    "def train_2wayNeural(modelName, filename='./testData/prepareDS.pkl'):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(learning_rate=0.001, n=4096,patience=20)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 2,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    datasetItem = open_file(filename)\n",
    "    # print(datasetItem)\n",
    "    # datasetItem_all = {}\n",
    "    # for key, (dgl_graph, adjacency_matrix,graph) in datasetItem.items():\n",
    "    #     A, C = FIndAC(graph)\n",
    "    #     datasetItem_all[key] = [dgl_graph, adjacency_matrix, graph, A, C]\n",
    "\n",
    "    # print(len(datasetItem), datasetItem[0][3])\n",
    "    # datasetItem_2 = {}\n",
    "    # datasetItem_2[0]=datasetItem[1]\n",
    "    # print(datasetItem_2)\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "\n",
    "    # print(datasetItem[1][2].nodes)\n",
    "    # # Visualize graph\n",
    "    # pos = nx.kamada_kawai_layout(datasetItem[1][2])\n",
    "    # nx.draw(datasetItem[1][2], pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
    "    # cut_value, (part_1, part_2) = nx.minimum_cut(datasetItem_2[0][2], datasetItem_2[0][3][1], datasetItem_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "    # print(cut_value, len(part_1), len(part_2))\n",
    "\n",
    "    # resultList = []\n",
    "    # all_indexes = sorted(part_1.union(part_2))\n",
    "    # # Check membership for each index and append the appropriate pair to the result list\n",
    "    # for index in all_indexes:\n",
    "    #     if index in part_1:\n",
    "    #         resultList.append([1, 0])\n",
    "    #     elif index in part_2:\n",
    "    #         resultList.append([0, 1])\n",
    "\n",
    "    #\n",
    "    trained_net, bestLost, epoch, inp, lossList= run_gnn_training2(\n",
    "        datasetItem, net, optimizer, int(500),\n",
    "        gnn_hypers['tolerance'], gnn_hypers['patience'], loss_terminal,gnn_hypers['dim_embedding'], gnn_hypers['number_classes'], modelName,  TORCH_DTYPE,  TORCH_DEVICE)\n",
    "\n",
    "    return trained_net, bestLost, epoch, inp, lossList\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Establishin baseline\n",
    "We already have a loss function that partitions (not effectively). We\n",
    "Neural network model to establish baseline for partition, so if we do hard-set terminal, we can compare it with something"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 26\u001B[0m\n\u001B[1;32m     24\u001B[0m         h \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(h, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Apply softmax over the classes dimension\u001B[39;00m\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m h\n\u001B[0;32m---> 26\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossOrig_2.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[33], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[25], line 70\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     65\u001B[0m     graph_dict[epochCount] \u001B[38;5;241m=\u001B[39m dgl_graph\n\u001B[1;32m     67\u001B[0m dgl_graph\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeat\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m adjacency_matrix\n\u001B[0;32m---> 70\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdgl_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdgl_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfeat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m logits \u001B[38;5;241m=\u001B[39m override_fixed_nodes(logits)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Apply max to one-hot encoding\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[34], line 20\u001B[0m, in \u001B[0;36mGCNSoftmax.forward\u001B[0;34m(self, g, inputs)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, g, inputs):\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# Basic forward pass\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     h \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h)\n\u001B[1;32m     22\u001B[0m     h \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(h, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout_frac, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/dgl/nn/pytorch/conv/graphconv.py:428\u001B[0m, in \u001B[0;36mGraphConv.forward\u001B[0;34m(self, graph, feat, weight, edge_weight)\u001B[0m\n\u001B[1;32m    426\u001B[0m feat_src, feat_dst \u001B[38;5;241m=\u001B[39m expand_as_pair(feat, graph)\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_norm \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 428\u001B[0m     degs \u001B[38;5;241m=\u001B[39m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_degrees\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(feat_src)\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_norm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    430\u001B[0m         norm \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mpow(degs, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/dgl/heterograph.py:3750\u001B[0m, in \u001B[0;36mDGLGraph.out_degrees\u001B[0;34m(self, u, etype)\u001B[0m\n\u001B[1;32m   3747\u001B[0m     u \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrcnodes(srctype)\n\u001B[1;32m   3748\u001B[0m u_tensor \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mprepare_tensor(\u001B[38;5;28mself\u001B[39m, u, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3749\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m F\u001B[38;5;241m.\u001B[39mas_scalar(\n\u001B[0;32m-> 3750\u001B[0m     F\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mntype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrctype\u001B[49m\u001B[43m)\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   3751\u001B[0m ) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(u_tensor):\n\u001B[1;32m   3752\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mu contains invalid node IDs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3753\u001B[0m deg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mout_degrees(etid, utils\u001B[38;5;241m.\u001B[39mprepare_tensor(\u001B[38;5;28mself\u001B[39m, u, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/dgl/heterograph.py:2930\u001B[0m, in \u001B[0;36mDGLGraph.has_nodes\u001B[0;34m(self, vid, ntype)\u001B[0m\n\u001B[1;32m   2926\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(vid_tensor) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m F\u001B[38;5;241m.\u001B[39mas_scalar(F\u001B[38;5;241m.\u001B[39mmin(vid_tensor, \u001B[38;5;241m0\u001B[39m)) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(\n\u001B[1;32m   2927\u001B[0m     vid_tensor\n\u001B[1;32m   2928\u001B[0m ):\n\u001B[1;32m   2929\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll IDs must be non-negative integers.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2930\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_ntype_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43mntype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvid_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2931\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(vid, numbers\u001B[38;5;241m.\u001B[39mIntegral):\n\u001B[1;32m   2932\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(F\u001B[38;5;241m.\u001B[39mas_scalar(ret))\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/dgl/heterograph_index.py:460\u001B[0m, in \u001B[0;36mHeteroGraphIndex.has_nodes\u001B[0;34m(self, ntype, vids)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhas_nodes\u001B[39m(\u001B[38;5;28mself\u001B[39m, ntype, vids):\n\u001B[1;32m    445\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return true if the nodes exist.\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \n\u001B[1;32m    447\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;124;03m        0-1 array indicating existence\u001B[39;00m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mfrom_dgl_nd(\n\u001B[0;32m--> 460\u001B[0m         \u001B[43m_CAPI_DGLHeteroHasVertices\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mntype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvids\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    461\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/dgl/_ffi/_ctypes/function.py:209\u001B[0m, in \u001B[0;36mFunctionBase.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Call the function with positional arguments\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \n\u001B[1;32m    205\u001B[0m \u001B[38;5;124;03margs : list\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m   The positional arguments to the function call.\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    208\u001B[0m temp_args \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 209\u001B[0m values, tcodes, num_args \u001B[38;5;241m=\u001B[39m \u001B[43m_make_dgl_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m ret_val \u001B[38;5;241m=\u001B[39m DGLValue()\n\u001B[1;32m    211\u001B[0m ret_tcode \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mc_int()\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/dgl/_ffi/_ctypes/function.py:125\u001B[0m, in \u001B[0;36m_make_dgl_args\u001B[0;34m(args, temp_args)\u001B[0m\n\u001B[1;32m    123\u001B[0m     temp_args\u001B[38;5;241m.\u001B[39mappend(arg)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, NDArrayBase):\n\u001B[0;32m--> 125\u001B[0m     values[i]\u001B[38;5;241m.\u001B[39mv_handle \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mcast(arg\u001B[38;5;241m.\u001B[39mhandle, ctypes\u001B[38;5;241m.\u001B[39mc_void_p)\n\u001B[1;32m    126\u001B[0m     type_codes[i] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    127\u001B[0m         TypeCode\u001B[38;5;241m.\u001B[39mNDARRAY_CONTAINER\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m arg\u001B[38;5;241m.\u001B[39mis_view\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m TypeCode\u001B[38;5;241m.\u001B[39mARRAY_HANDLE\n\u001B[1;32m    130\u001B[0m     )\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, _nd\u001B[38;5;241m.\u001B[39m_DGL_COMPATS):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        return h\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossOrig_2.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 2629117.7470703125\n",
      "Epoch: 100, Cumulative Loss: 2603246.4545898438\n",
      "Epoch: 200, Cumulative Loss: 2603246.4545898438\n",
      "Epoch: 300, Cumulative Loss: 2603246.4545898438\n",
      "Epoch: 400, Cumulative Loss: 2603246.4545898438\n",
      "GNN training took 1388.975 seconds.\n",
      "Best cumulative loss: 10772.7900390625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k @ partition_l)\n",
    "            min_cut_loss += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return min_cut_loss\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A= 0, C=1, T=1000):\n",
    "    \"\"\"\n",
    "    Compute the overall loss including cut loss and terminal independence.\n",
    "\n",
    "    :param s: Node partition probabilities.\n",
    "    :param adjacency_matrix: Graph adjacency matrix.\n",
    "    :param terminals: List of terminal node indices.\n",
    "    :param C: Weight for the cut loss.\n",
    "    :param T: Weight for the terminal independence penalty.\n",
    "    :return: Total loss.\n",
    "    \"\"\"\n",
    "    cut_loss = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    terminal_loss = terminal_independence_penalty(s, [0,1,2])\n",
    "    total_loss = C * cut_loss + T * terminal_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossBaseline.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXP 1\n",
    "\n",
    "- using old loss function\n",
    "\n",
    "Removing terminal penalties, settings contant for terminal 0, 1, 2 with in the neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 167\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# loss += penalty* terminal_independence_penalty(s, [0,1,2])\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n\u001B[0;32m--> 167\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossExp1.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[63], line 57\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     56\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 57\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# Update cumulative loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp1.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp2\n",
    "\n",
    "- using old loss function\n",
    "\n",
    "Removing terminal penalty, but instead of changing neural network output, we are updating a clone of it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1089309.7293701172\n",
      "Epoch: 100, Cumulative Loss: 1025623.7824707031\n",
      "Epoch: 200, Cumulative Loss: 1025238.1572265625\n",
      "Epoch: 300, Cumulative Loss: 1024769.947265625\n",
      "Epoch: 400, Cumulative Loss: 1024636.330078125\n",
      "GNN training took 191.703 seconds.\n",
      "Best cumulative loss: 4242.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXP 3\n",
    "\n",
    "Removing terminal penalties, settings contant for terminal 0, 1, 2 with in the neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # Fix the outputs for nodes 0, 1, and 2\n",
    "        h[0] = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)\n",
    "        h[1] = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32)\n",
    "        h[2] = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float32)\n",
    "\n",
    "        return h\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossExp2.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[23], line 51\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     50\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 51\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Update cumulative loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [80, 3]], which is output 0 of SoftmaxBackward0, is at version 3; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modifying the loss function\n",
    "## exp 1 - loss\n",
    "expriment 1 of modifying the loss function (still using probabilities) + keeping terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 4785276.850097656\n",
      "Stopping early at epoch 40\n",
      "Epoch: 100, Cumulative Loss: 1663353.253540039\n",
      "Epoch: 200, Cumulative Loss: 1438121.930053711\n",
      "Stopping early at epoch 209\n",
      "Stopping early at epoch 300\n",
      "Epoch: 300, Cumulative Loss: 1558179.4991455078\n",
      "Stopping early at epoch 328\n",
      "Stopping early at epoch 365\n",
      "Epoch: 400, Cumulative Loss: 1358117.3784179688\n",
      "GNN training took 186.82 seconds.\n",
      "Best cumulative loss: 4739.478515625\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp1_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modifying the loss function\n",
    "## exp 2 - loss\n",
    "- expriment 2 of modifying the loss function (keeping probabilities)\n",
    "- removing terminal indepdendence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1110715.6811523438\n",
      "Epoch: 100, Cumulative Loss: 1104329.0\n",
      "Epoch: 200, Cumulative Loss: 1104329.0\n",
      "Epoch: 300, Cumulative Loss: 1104329.0\n",
      "Epoch: 400, Cumulative Loss: 1104329.0\n",
      "GNN training took 190.264 seconds.\n",
      "Best cumulative loss: 4671.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp2_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modifying the loss function\n",
    "## exp 3 - loss\n",
    "- expriment 3 of modifying the loss function (explicitly making the max proab to 1)\n",
    "- keeping terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1628496.0\n",
      "Epoch: 100, Cumulative Loss: 1176600.0\n",
      "Epoch: 200, Cumulative Loss: 1148917.0\n",
      "Epoch: 300, Cumulative Loss: 1143571.0\n",
      "Epoch: 400, Cumulative Loss: 1138089.0\n",
      "GNN training took 339.987 seconds.\n",
      "Best cumulative loss: 4553.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp3_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## exp 4\n",
    "- expriment 4 of modifying the loss function (purely binary input)\n",
    "- keeping terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to loss\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to the loss for partition k\n",
    "        # This considers the weight of the edges\n",
    "        HC_k = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "        # Sum up the contributions for partition k\n",
    "        loss += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    loss = loss / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1089103.0\n",
      "Epoch: 100, Cumulative Loss: 1064404.0\n",
      "Epoch: 200, Cumulative Loss: 1064118.0\n",
      "Epoch: 300, Cumulative Loss: 1064103.0\n",
      "Epoch: 400, Cumulative Loss: 1064054.0\n",
      "GNN training took 501.735 seconds.\n",
      "Best cumulative loss: 4541.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp4_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 5 - loss\n",
    "- expriment 5 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 122807.00138473511\n",
      "Epoch: 100, Cumulative Loss: 30866.000024795532\n",
      "Epoch: 200, Cumulative Loss: 30751.000003814697\n",
      "Epoch: 300, Cumulative Loss: 30877.0\n",
      "Epoch: 400, Cumulative Loss: 30831.000007629395\n",
      "GNN training took 420.717 seconds.\n",
      "Best cumulative loss: 127.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp5_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 6 - loss\n",
    "\n",
    "- expriment 6 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- keeping terminal loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 3042784.951171875\n",
      "Stopping early at epoch 76\n",
      "Stopping early at epoch 80\n",
      "Epoch: 100, Cumulative Loss: 619306.998260498\n",
      "Stopping early at epoch 168\n",
      "Stopping early at epoch 194\n",
      "Epoch: 200, Cumulative Loss: 470624.99826049805\n",
      "Stopping early at epoch 221\n",
      "Stopping early at epoch 222\n",
      "Stopping early at epoch 282\n",
      "Stopping early at epoch 283\n",
      "Epoch: 300, Cumulative Loss: 421319.9992828369\n",
      "Stopping early at epoch 306\n",
      "Stopping early at epoch 327\n",
      "Stopping early at epoch 352\n",
      "Stopping early at epoch 353\n",
      "Stopping early at epoch 377\n",
      "Stopping early at epoch 378\n",
      "Stopping early at epoch 379\n",
      "Epoch: 400, Cumulative Loss: 408950.99939346313\n",
      "Stopping early at epoch 411\n",
      "Stopping early at epoch 412\n",
      "Stopping early at epoch 440\n",
      "Stopping early at epoch 441\n",
      "Stopping early at epoch 474\n",
      "Stopping early at epoch 475\n",
      "Stopping early at epoch 476\n",
      "GNN training took 334.293 seconds.\n",
      "Best cumulative loss: 472.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp6_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 7 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function\n",
    "\n",
    "- expriment 7 of modifying the loss function (purely binary input) and find exact loss value (looped)\n",
    "- removing terminal loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each pair of nodes to compute the contribution to the cut value\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):  # Only consider upper triangle to avoid double counting\n",
    "            # [0,1,0] [0,0,1] = [0] = 1\n",
    "            # [0,1,0][0,1,0] = [1] = 0\n",
    "            loss += (1 - (s[i] * s[j]).sum())*adjacency_matrix[i, j]\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 96833.00001907349\n",
      "Epoch: 100, Cumulative Loss: 32246.00002861023\n",
      "Epoch: 200, Cumulative Loss: 32122.0\n",
      "Epoch: 300, Cumulative Loss: 31906.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossExp7_loss.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[35], line 72\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     71\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 72\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Update cumulative loss\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/function.py:277\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBackwardCFunction\u001B[39;00m(_C\u001B[38;5;241m.\u001B[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;66;03m# _forward_cls is defined by derived class\u001B[39;00m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;66;03m# The user should define either backward or vjp but never both.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m         backward_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mbackward  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    281\u001B[0m         vjp_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mvjp  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp7_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 8 - loss (2 way mincut)\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function\n",
    "\n",
    "- expriment 8 of modifying the loss function (purely binary input) and find exact loss value (looped)\n",
    "- removing terminal loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 47485.00275063515\n",
      "Epoch: 100, Cumulative Loss: 16952.000015497208\n",
      "Epoch: 200, Cumulative Loss: 16921.000000476837\n",
      "Epoch: 300, Cumulative Loss: 16911.0\n",
      "Epoch: 400, Cumulative Loss: 16907.0\n",
      "GNN training took 372.75 seconds.\n",
      "Best cumulative loss: 130.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp8_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 9 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function\n",
    "\n",
    "- expriment 9 of modifying the loss function (purely binary input) and find exact loss value (looped)\n",
    "- removing terminal loss\n",
    "- Adding HA back\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HA = calculate_HA_vectorized(s)\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC + A* HA\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=1, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 112699.00022888184\n",
      "Epoch: 100, Cumulative Loss: 30808.000049591064\n",
      "Epoch: 200, Cumulative Loss: 30575.0\n",
      "Epoch: 300, Cumulative Loss: 30067.0\n",
      "Epoch: 400, Cumulative Loss: 29973.0\n",
      "GNN training took 368.172 seconds.\n",
      "Best cumulative loss: 166.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp9_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 10 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function\n",
    "\n",
    "- expriment 10 of modifying the loss function (purely binary input) and find exact loss value (looped)\n",
    "- Keeping terminal loss\n",
    "- Adding HA back\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            # logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HA = calculate_HA_vectorized(s)\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC + A*HA\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=1, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 3529389.95703125\n",
      "Stopping early at epoch 59\n",
      "Stopping early at epoch 84\n",
      "Stopping early at epoch 85\n",
      "Stopping early at epoch 86\n",
      "Epoch: 100, Cumulative Loss: 657550.9971199036\n",
      "Stopping early at epoch 107\n",
      "Stopping early at epoch 127\n",
      "Stopping early at epoch 128\n",
      "Stopping early at epoch 149\n",
      "Stopping early at epoch 150\n",
      "Stopping early at epoch 168\n",
      "Stopping early at epoch 189\n",
      "Stopping early at epoch 190\n",
      "Stopping early at epoch 191\n",
      "Epoch: 200, Cumulative Loss: 550693.000087738\n",
      "Stopping early at epoch 206\n",
      "Stopping early at epoch 226\n",
      "Stopping early at epoch 227\n",
      "Stopping early at epoch 237\n",
      "Stopping early at epoch 256\n",
      "Stopping early at epoch 258\n",
      "Stopping early at epoch 280\n",
      "Stopping early at epoch 281\n",
      "Epoch: 300, Cumulative Loss: 458498.0001525879\n",
      "Stopping early at epoch 305\n",
      "Stopping early at epoch 306\n",
      "Stopping early at epoch 312\n",
      "Stopping early at epoch 328\n",
      "Stopping early at epoch 329\n",
      "Stopping early at epoch 330\n",
      "Stopping early at epoch 358\n",
      "Stopping early at epoch 376\n",
      "Stopping early at epoch 377\n",
      "Stopping early at epoch 379\n",
      "Epoch: 400, Cumulative Loss: 423268.99813079834\n",
      "Stopping early at epoch 405\n",
      "Stopping early at epoch 406\n",
      "Stopping early at epoch 433\n",
      "Stopping early at epoch 434\n",
      "Stopping early at epoch 461\n",
      "Stopping early at epoch 462\n",
      "Stopping early at epoch 464\n",
      "Stopping early at epoch 482\n",
      "Stopping early at epoch 485\n",
      "GNN training took 360.077 seconds.\n",
      "Best cumulative loss: 881.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp10_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 11 - loss (2 way mincut)\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function\n",
    "\n",
    "- expriment 11 of modifying the loss function (purely binary input) and find exact loss value (looped)\n",
    "- removing terminal loss\n",
    "- Adding a balancer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Use probabilities directly for nodes being in partitions k and l\n",
    "            # partition_k = s[:, k].unsqueeze(1)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "            # Edge weights between partitions\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k @ partition_l)\n",
    "            min_cut_loss += torch.sum(inter_partition_edges)\n",
    "\n",
    "\n",
    "    return min_cut_loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    ss = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    return C * HC + ss\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 3434887.759765625\n",
      "Epoch: 100, Cumulative Loss: 3315970.935546875\n",
      "Epoch: 200, Cumulative Loss: 3315970.935546875\n",
      "Epoch: 300, Cumulative Loss: 3315970.935546875\n",
      "Epoch: 400, Cumulative Loss: 3315970.935546875\n",
      "GNN training took 2398.958 seconds.\n",
      "Best cumulative loss: 14586.3505859375\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp11_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# check for 2 terminal and see what is the result for this neural netowrk.\n",
    "\n",
    "# add HA back and test without the change for terminal (bring terminal loss ), remove `override_fixed_nodes`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 12\n",
    "\n",
    "- expriment 6 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- keeping terminal loss\n",
    "- Using GATCOnv as base neural network\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GATConv(in_feats, hidden_size, num_heads=1).to(device)\n",
    "        self.conv2 = GATConv(hidden_size, num_classes, num_heads=1).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs).squeeze(1)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h).squeeze(1)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCNSoftmax(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 182201.0\n",
      "Epoch: 100, Cumulative Loss: 32866.000005722046\n",
      "Stopping early at epoch 141\n",
      "Stopping early at epoch 151\n",
      "Epoch: 200, Cumulative Loss: 34687.000005722046\n",
      "Stopping early at epoch 224\n",
      "Epoch: 300, Cumulative Loss: 32514.000007629395\n",
      "Stopping early at epoch 348\n",
      "Stopping early at epoch 371\n",
      "Epoch: 400, Cumulative Loss: 31886.00000190735\n",
      "Stopping early at epoch 403\n",
      "GNN training took 537.735 seconds.\n",
      "Best cumulative loss: 130.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp12_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 13\n",
    "\n",
    "- expriment 6 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- keeping terminal loss\n",
    "- Using Graph conv as base neural network, adding ndata attribute\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "    graph_dict = {}\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        graphCount = 0\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            if epochCount in graph_dict:\n",
    "                dgl_graph = graph_dict[epochCount]\n",
    "            else:\n",
    "                dgl_graph = build_node_feature(dgl_graph, graph)\n",
    "                graph_dict[epochCount] = dgl_graph\n",
    "\n",
    "            dgl_graph.ndata['feat'] = adjacency_matrix\n",
    "\n",
    "\n",
    "            logits = net(dgl_graph, dgl_graph.ndata['feat'])\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "            graphCount+=1\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCNSoftmax(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer\n",
    "\n",
    "def build_node_feature(g, graph):\n",
    "    nx_g = graph\n",
    "    degrees = np.array([val for (node, val) in nx_g.degree()])\n",
    "\n",
    "    # Clustering coefficient\n",
    "    clustering = np.array([val for (node, val) in nx.clustering(nx_g).items()])\n",
    "\n",
    "    # Betweenness centrality\n",
    "    betweenness = np.array([val for (node, val) in nx.betweenness_centrality(nx_g).items()])\n",
    "\n",
    "    # Stack all features to create a feature matrix\n",
    "    features = np.stack([degrees, clustering, betweenness], axis=1)\n",
    "\n",
    "    # Convert to tensor and assign to DGL graph\n",
    "    g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
    "    return g\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 124030.00007629395\n",
      "Epoch: 100, Cumulative Loss: 111005.0\n",
      "Epoch: 200, Cumulative Loss: 32663.000022888184\n",
      "Epoch: 300, Cumulative Loss: 31997.000007629395\n",
      "Epoch: 400, Cumulative Loss: 31707.0\n",
      "GNN training took 1154.943 seconds.\n",
      "Best cumulative loss: 166.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp13_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 14\n",
    "\n",
    "- expriment 6 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- keeping terminal loss\n",
    "- Using Graph conv as base neural network, adding ndata attribute with more node related information\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "    graph_dict = {}\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        graphCount = 0\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            if epochCount in graph_dict:\n",
    "                dgl_graph = graph_dict[epochCount]\n",
    "            else:\n",
    "                dgl_graph = build_node_feature(dgl_graph, graph)\n",
    "                graph_dict[epochCount] = dgl_graph\n",
    "\n",
    "            dgl_graph.ndata['feat'] = adjacency_matrix\n",
    "\n",
    "\n",
    "            logits = net(dgl_graph, dgl_graph.ndata['feat'])\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "            graphCount+=1\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size).to(device)\n",
    "        self.conv3 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv3(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCNSoftmax(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer\n",
    "\n",
    "def build_node_feature(g, graph):\n",
    "    nx_g = graph\n",
    "    degrees = np.array([val for (node, val) in nx_g.degree()])\n",
    "\n",
    "    # Clustering coefficient\n",
    "    clustering = np.array([val for (node, val) in nx.clustering(nx_g).items()])\n",
    "\n",
    "    # Betweenness centrality\n",
    "    betweenness = np.array([val for (node, val) in nx.betweenness_centrality(nx_g).items()])\n",
    "\n",
    "    # Stack all features to create a feature matrix\n",
    "    features = np.stack([degrees, clustering, betweenness], axis=1)\n",
    "\n",
    "    # Convert to tensor and assign to DGL graph\n",
    "    g.ndata['feat'] = torch.tensor(features, dtype=torch.float32)\n",
    "    return g\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 124640.00009155273\n",
      "Epoch: 100, Cumulative Loss: 111322.0\n",
      "Epoch: 200, Cumulative Loss: 111322.0\n",
      "Epoch: 300, Cumulative Loss: 111322.0\n",
      "Epoch: 400, Cumulative Loss: 111322.0\n",
      "GNN training took 2262.36 seconds.\n",
      "Best cumulative loss: 434.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp14_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 15\n",
    "\n",
    "- expriment 6 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- keeping terminal loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = EdgeConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = EdgeConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "class GATSoftmax(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GATSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GATConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GATConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        edge_index = g.edge_index\n",
    "        edge_weight = g.edge_attr  # assuming edge_attr is available and contains the weights\n",
    "\n",
    "        h = self.conv1(g, inputs, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h, edge_weight)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 94902.00007629395\n",
      "Epoch: 100, Cumulative Loss: 31554.0\n",
      "Epoch: 200, Cumulative Loss: 31264.0\n",
      "Epoch: 300, Cumulative Loss: 31275.0\n",
      "Epoch: 400, Cumulative Loss: 31205.0\n",
      "GNN training took 2230.426 seconds.\n",
      "Best cumulative loss: 130.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp12_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 16\n",
    "\n",
    "- expriment 16 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 8\n",
    "- trainign DS contains graph with edge val > 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 8 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 23216.000030517578\n",
      "Epoch: 100, Cumulative Loss: 10271.000054597855\n",
      "Epoch: 200, Cumulative Loss: 10160.000024795532\n",
      "Epoch: 300, Cumulative Loss: 10130.000019311905\n",
      "Epoch: 400, Cumulative Loss: 10130.00002169609\n",
      "GNN training took 102.355 seconds.\n",
      "Best cumulative loss: 33.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp16_loss.pth', './testData/prepareDS_8.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 17\n",
    "\n",
    "- expriment 17 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 8\n",
    "- trainign DS contains graph with edge val EQUALS 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 8 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 800.0000040531158\n",
      "Epoch: 100, Cumulative Loss: 485.0\n",
      "Epoch: 200, Cumulative Loss: 485.0\n",
      "Epoch: 300, Cumulative Loss: 485.0\n",
      "Epoch: 400, Cumulative Loss: 485.0\n",
      "GNN training took 191.449 seconds.\n",
      "Best cumulative loss: 1.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp16_loss.pth', './testData/prepareDS_8_1.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 18\n",
    "\n",
    "- expriment 18 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 8\n",
    "- trainign DS contains only 1 graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    edges = [\n",
    "        (0, 1, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (2, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 5, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (6, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (3, 4, {\"weight\": 1, \"capacity\": 1})  # Single edge between the two groups\n",
    "    ]\n",
    "\n",
    "    graph = CreateDummyFunction(edges)\n",
    "    graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "    graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "    q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "    dataset = {}\n",
    "    dataset[0] = [graph_dgl,q_torch, graph, [0,1] ]\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, adjacency_matrix)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 8 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def CreateDummyFunction(edges):\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(edges)\n",
    "    test_graph.order()\n",
    "    return test_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 3.0\n",
      "Stopping early at epoch 20\n",
      "Stopping early at epoch 21\n",
      "Stopping early at epoch 22\n",
      "Stopping early at epoch 23\n",
      "Stopping early at epoch 24\n",
      "Stopping early at epoch 25\n",
      "Stopping early at epoch 26\n",
      "Stopping early at epoch 27\n",
      "Stopping early at epoch 28\n",
      "Stopping early at epoch 29\n",
      "Stopping early at epoch 30\n",
      "Stopping early at epoch 31\n",
      "Stopping early at epoch 32\n",
      "Stopping early at epoch 33\n",
      "Stopping early at epoch 34\n",
      "Stopping early at epoch 35\n",
      "Stopping early at epoch 36\n",
      "Stopping early at epoch 37\n",
      "Stopping early at epoch 38\n",
      "Stopping early at epoch 39\n",
      "Stopping early at epoch 40\n",
      "Stopping early at epoch 41\n",
      "Stopping early at epoch 42\n",
      "Stopping early at epoch 43\n",
      "Stopping early at epoch 44\n",
      "Stopping early at epoch 45\n",
      "Stopping early at epoch 46\n",
      "Stopping early at epoch 47\n",
      "Stopping early at epoch 48\n",
      "Stopping early at epoch 49\n",
      "Stopping early at epoch 50\n",
      "Stopping early at epoch 51\n",
      "Stopping early at epoch 52\n",
      "Stopping early at epoch 53\n",
      "Stopping early at epoch 54\n",
      "Stopping early at epoch 55\n",
      "Stopping early at epoch 56\n",
      "Stopping early at epoch 57\n",
      "Stopping early at epoch 58\n",
      "Stopping early at epoch 59\n",
      "Stopping early at epoch 60\n",
      "Stopping early at epoch 61\n",
      "Stopping early at epoch 62\n",
      "Stopping early at epoch 63\n",
      "Stopping early at epoch 64\n",
      "Stopping early at epoch 65\n",
      "Stopping early at epoch 66\n",
      "Stopping early at epoch 67\n",
      "Stopping early at epoch 68\n",
      "Stopping early at epoch 69\n",
      "Stopping early at epoch 70\n",
      "Stopping early at epoch 71\n",
      "Stopping early at epoch 72\n",
      "Stopping early at epoch 73\n",
      "Stopping early at epoch 74\n",
      "Stopping early at epoch 75\n",
      "Stopping early at epoch 76\n",
      "Stopping early at epoch 77\n",
      "Stopping early at epoch 78\n",
      "Stopping early at epoch 79\n",
      "Stopping early at epoch 80\n",
      "Stopping early at epoch 81\n",
      "Stopping early at epoch 82\n",
      "Stopping early at epoch 83\n",
      "Stopping early at epoch 84\n",
      "Stopping early at epoch 85\n",
      "Stopping early at epoch 86\n",
      "Stopping early at epoch 87\n",
      "Stopping early at epoch 88\n",
      "Stopping early at epoch 89\n",
      "Stopping early at epoch 90\n",
      "Stopping early at epoch 91\n",
      "Stopping early at epoch 92\n",
      "Stopping early at epoch 93\n",
      "Stopping early at epoch 94\n",
      "Stopping early at epoch 95\n",
      "Stopping early at epoch 96\n",
      "Stopping early at epoch 97\n",
      "Stopping early at epoch 98\n",
      "Stopping early at epoch 99\n",
      "Stopping early at epoch 100\n",
      "Epoch: 100, Cumulative Loss: 3.0\n",
      "Stopping early at epoch 101\n",
      "Stopping early at epoch 102\n",
      "Stopping early at epoch 103\n",
      "Stopping early at epoch 104\n",
      "Stopping early at epoch 105\n",
      "Stopping early at epoch 106\n",
      "Stopping early at epoch 107\n",
      "Stopping early at epoch 108\n",
      "Stopping early at epoch 109\n",
      "Stopping early at epoch 110\n",
      "Stopping early at epoch 111\n",
      "Stopping early at epoch 112\n",
      "Stopping early at epoch 113\n",
      "Stopping early at epoch 114\n",
      "Stopping early at epoch 115\n",
      "Stopping early at epoch 116\n",
      "Stopping early at epoch 117\n",
      "Stopping early at epoch 118\n",
      "Stopping early at epoch 119\n",
      "Stopping early at epoch 120\n",
      "Stopping early at epoch 121\n",
      "Stopping early at epoch 122\n",
      "Stopping early at epoch 123\n",
      "Stopping early at epoch 124\n",
      "Stopping early at epoch 125\n",
      "Stopping early at epoch 126\n",
      "Stopping early at epoch 127\n",
      "Stopping early at epoch 128\n",
      "Stopping early at epoch 129\n",
      "Stopping early at epoch 130\n",
      "Stopping early at epoch 131\n",
      "Stopping early at epoch 132\n",
      "Stopping early at epoch 133\n",
      "Stopping early at epoch 134\n",
      "Stopping early at epoch 135\n",
      "Stopping early at epoch 136\n",
      "Stopping early at epoch 137\n",
      "Stopping early at epoch 138\n",
      "Stopping early at epoch 139\n",
      "Stopping early at epoch 140\n",
      "Stopping early at epoch 141\n",
      "Stopping early at epoch 142\n",
      "Stopping early at epoch 143\n",
      "Stopping early at epoch 144\n",
      "Stopping early at epoch 145\n",
      "Stopping early at epoch 146\n",
      "Stopping early at epoch 147\n",
      "Stopping early at epoch 148\n",
      "Stopping early at epoch 149\n",
      "Stopping early at epoch 150\n",
      "Stopping early at epoch 151\n",
      "Stopping early at epoch 152\n",
      "Stopping early at epoch 153\n",
      "Stopping early at epoch 154\n",
      "Stopping early at epoch 155\n",
      "Stopping early at epoch 156\n",
      "Stopping early at epoch 157\n",
      "Stopping early at epoch 158\n",
      "Stopping early at epoch 159\n",
      "Stopping early at epoch 160\n",
      "Stopping early at epoch 161\n",
      "Stopping early at epoch 162\n",
      "Stopping early at epoch 163\n",
      "Stopping early at epoch 164\n",
      "Stopping early at epoch 165\n",
      "Stopping early at epoch 166\n",
      "Stopping early at epoch 167\n",
      "Stopping early at epoch 168\n",
      "Stopping early at epoch 169\n",
      "Stopping early at epoch 170\n",
      "Stopping early at epoch 171\n",
      "Stopping early at epoch 172\n",
      "Stopping early at epoch 173\n",
      "Stopping early at epoch 174\n",
      "Stopping early at epoch 175\n",
      "Stopping early at epoch 176\n",
      "Stopping early at epoch 177\n",
      "Stopping early at epoch 178\n",
      "Stopping early at epoch 179\n",
      "Stopping early at epoch 180\n",
      "Stopping early at epoch 181\n",
      "Stopping early at epoch 182\n",
      "Stopping early at epoch 183\n",
      "Stopping early at epoch 184\n",
      "Stopping early at epoch 185\n",
      "Stopping early at epoch 186\n",
      "Stopping early at epoch 187\n",
      "Stopping early at epoch 188\n",
      "Stopping early at epoch 189\n",
      "Stopping early at epoch 190\n",
      "Stopping early at epoch 191\n",
      "Stopping early at epoch 192\n",
      "Stopping early at epoch 193\n",
      "Stopping early at epoch 194\n",
      "Stopping early at epoch 195\n",
      "Stopping early at epoch 196\n",
      "Stopping early at epoch 197\n",
      "Stopping early at epoch 198\n",
      "Stopping early at epoch 199\n",
      "Stopping early at epoch 200\n",
      "Epoch: 200, Cumulative Loss: 3.0\n",
      "Stopping early at epoch 201\n",
      "Stopping early at epoch 202\n",
      "Stopping early at epoch 203\n",
      "Stopping early at epoch 204\n",
      "Stopping early at epoch 205\n",
      "Stopping early at epoch 206\n",
      "Stopping early at epoch 207\n",
      "Stopping early at epoch 208\n",
      "Stopping early at epoch 209\n",
      "Stopping early at epoch 210\n",
      "Stopping early at epoch 211\n",
      "Stopping early at epoch 212\n",
      "Stopping early at epoch 213\n",
      "Stopping early at epoch 214\n",
      "Stopping early at epoch 215\n",
      "Stopping early at epoch 216\n",
      "Stopping early at epoch 217\n",
      "Stopping early at epoch 218\n",
      "Stopping early at epoch 219\n",
      "Stopping early at epoch 220\n",
      "Stopping early at epoch 221\n",
      "Stopping early at epoch 222\n",
      "Stopping early at epoch 223\n",
      "Stopping early at epoch 224\n",
      "Stopping early at epoch 225\n",
      "Stopping early at epoch 226\n",
      "Stopping early at epoch 227\n",
      "Stopping early at epoch 228\n",
      "Stopping early at epoch 229\n",
      "Stopping early at epoch 230\n",
      "Stopping early at epoch 231\n",
      "Stopping early at epoch 232\n",
      "Stopping early at epoch 233\n",
      "Stopping early at epoch 234\n",
      "Stopping early at epoch 235\n",
      "Stopping early at epoch 236\n",
      "Stopping early at epoch 237\n",
      "Stopping early at epoch 238\n",
      "Stopping early at epoch 239\n",
      "Stopping early at epoch 240\n",
      "Stopping early at epoch 241\n",
      "Stopping early at epoch 242\n",
      "Stopping early at epoch 243\n",
      "Stopping early at epoch 244\n",
      "Stopping early at epoch 245\n",
      "Stopping early at epoch 246\n",
      "Stopping early at epoch 247\n",
      "Stopping early at epoch 248\n",
      "Stopping early at epoch 249\n",
      "Stopping early at epoch 250\n",
      "Stopping early at epoch 251\n",
      "Stopping early at epoch 252\n",
      "Stopping early at epoch 253\n",
      "Stopping early at epoch 254\n",
      "Stopping early at epoch 255\n",
      "Stopping early at epoch 256\n",
      "Stopping early at epoch 257\n",
      "Stopping early at epoch 258\n",
      "Stopping early at epoch 259\n",
      "Stopping early at epoch 260\n",
      "Stopping early at epoch 261\n",
      "Stopping early at epoch 262\n",
      "Stopping early at epoch 263\n",
      "Stopping early at epoch 264\n",
      "Stopping early at epoch 265\n",
      "Stopping early at epoch 266\n",
      "Stopping early at epoch 267\n",
      "Stopping early at epoch 268\n",
      "Stopping early at epoch 269\n",
      "Stopping early at epoch 270\n",
      "Stopping early at epoch 271\n",
      "Stopping early at epoch 272\n",
      "Stopping early at epoch 273\n",
      "Stopping early at epoch 274\n",
      "Stopping early at epoch 275\n",
      "Stopping early at epoch 276\n",
      "Stopping early at epoch 277\n",
      "Stopping early at epoch 278\n",
      "Stopping early at epoch 279\n",
      "Stopping early at epoch 280\n",
      "Stopping early at epoch 281\n",
      "Stopping early at epoch 282\n",
      "Stopping early at epoch 283\n",
      "Stopping early at epoch 284\n",
      "Stopping early at epoch 285\n",
      "Stopping early at epoch 286\n",
      "Stopping early at epoch 287\n",
      "Stopping early at epoch 288\n",
      "Stopping early at epoch 289\n",
      "Stopping early at epoch 290\n",
      "Stopping early at epoch 291\n",
      "Stopping early at epoch 292\n",
      "Stopping early at epoch 293\n",
      "Stopping early at epoch 294\n",
      "Stopping early at epoch 295\n",
      "Stopping early at epoch 296\n",
      "Stopping early at epoch 297\n",
      "Stopping early at epoch 298\n",
      "Stopping early at epoch 299\n",
      "Stopping early at epoch 300\n",
      "Epoch: 300, Cumulative Loss: 3.0\n",
      "Stopping early at epoch 301\n",
      "Stopping early at epoch 302\n",
      "Stopping early at epoch 303\n",
      "Stopping early at epoch 304\n",
      "Stopping early at epoch 305\n",
      "Stopping early at epoch 306\n",
      "Stopping early at epoch 307\n",
      "Stopping early at epoch 308\n",
      "Stopping early at epoch 309\n",
      "Stopping early at epoch 310\n",
      "Stopping early at epoch 311\n",
      "Stopping early at epoch 312\n",
      "Stopping early at epoch 313\n",
      "Stopping early at epoch 314\n",
      "Stopping early at epoch 315\n",
      "Stopping early at epoch 316\n",
      "Stopping early at epoch 317\n",
      "Stopping early at epoch 318\n",
      "Stopping early at epoch 319\n",
      "Stopping early at epoch 320\n",
      "Stopping early at epoch 321\n",
      "Stopping early at epoch 322\n",
      "Stopping early at epoch 323\n",
      "Stopping early at epoch 324\n",
      "Stopping early at epoch 325\n",
      "Stopping early at epoch 326\n",
      "Stopping early at epoch 327\n",
      "Stopping early at epoch 328\n",
      "Stopping early at epoch 329\n",
      "Stopping early at epoch 330\n",
      "Stopping early at epoch 331\n",
      "Stopping early at epoch 332\n",
      "Stopping early at epoch 333\n",
      "Stopping early at epoch 334\n",
      "Stopping early at epoch 335\n",
      "Stopping early at epoch 336\n",
      "Stopping early at epoch 337\n",
      "Stopping early at epoch 338\n",
      "Stopping early at epoch 339\n",
      "Stopping early at epoch 340\n",
      "Stopping early at epoch 341\n",
      "Stopping early at epoch 342\n",
      "Stopping early at epoch 343\n",
      "Stopping early at epoch 344\n",
      "Stopping early at epoch 345\n",
      "Stopping early at epoch 346\n",
      "Stopping early at epoch 347\n",
      "Stopping early at epoch 348\n",
      "Stopping early at epoch 349\n",
      "Stopping early at epoch 350\n",
      "Stopping early at epoch 351\n",
      "Stopping early at epoch 352\n",
      "Stopping early at epoch 353\n",
      "Stopping early at epoch 354\n",
      "Stopping early at epoch 355\n",
      "Stopping early at epoch 356\n",
      "Stopping early at epoch 357\n",
      "Stopping early at epoch 358\n",
      "Stopping early at epoch 359\n",
      "Stopping early at epoch 360\n",
      "Stopping early at epoch 361\n",
      "Stopping early at epoch 362\n",
      "Stopping early at epoch 363\n",
      "Stopping early at epoch 364\n",
      "Stopping early at epoch 365\n",
      "Stopping early at epoch 366\n",
      "Stopping early at epoch 367\n",
      "Stopping early at epoch 368\n",
      "Stopping early at epoch 369\n",
      "Stopping early at epoch 370\n",
      "Stopping early at epoch 371\n",
      "Stopping early at epoch 372\n",
      "Stopping early at epoch 373\n",
      "Stopping early at epoch 374\n",
      "Stopping early at epoch 375\n",
      "Stopping early at epoch 376\n",
      "Stopping early at epoch 377\n",
      "Stopping early at epoch 378\n",
      "Stopping early at epoch 379\n",
      "Stopping early at epoch 380\n",
      "Stopping early at epoch 381\n",
      "Stopping early at epoch 382\n",
      "Stopping early at epoch 383\n",
      "Stopping early at epoch 384\n",
      "Stopping early at epoch 385\n",
      "Stopping early at epoch 386\n",
      "Stopping early at epoch 387\n",
      "Stopping early at epoch 388\n",
      "Stopping early at epoch 389\n",
      "Stopping early at epoch 390\n",
      "Stopping early at epoch 391\n",
      "Stopping early at epoch 392\n",
      "Stopping early at epoch 393\n",
      "Stopping early at epoch 394\n",
      "Stopping early at epoch 395\n",
      "Stopping early at epoch 396\n",
      "Stopping early at epoch 397\n",
      "Stopping early at epoch 398\n",
      "Stopping early at epoch 399\n",
      "Stopping early at epoch 400\n",
      "Epoch: 400, Cumulative Loss: 3.0\n",
      "Stopping early at epoch 401\n",
      "Stopping early at epoch 402\n",
      "Stopping early at epoch 403\n",
      "Stopping early at epoch 404\n",
      "Stopping early at epoch 405\n",
      "Stopping early at epoch 406\n",
      "Stopping early at epoch 407\n",
      "Stopping early at epoch 408\n",
      "Stopping early at epoch 409\n",
      "Stopping early at epoch 410\n",
      "Stopping early at epoch 411\n",
      "Stopping early at epoch 412\n",
      "Stopping early at epoch 413\n",
      "Stopping early at epoch 414\n",
      "Stopping early at epoch 415\n",
      "Stopping early at epoch 416\n",
      "Stopping early at epoch 417\n",
      "Stopping early at epoch 418\n",
      "Stopping early at epoch 419\n",
      "Stopping early at epoch 420\n",
      "Stopping early at epoch 421\n",
      "Stopping early at epoch 422\n",
      "Stopping early at epoch 423\n",
      "Stopping early at epoch 424\n",
      "Stopping early at epoch 425\n",
      "Stopping early at epoch 426\n",
      "Stopping early at epoch 427\n",
      "Stopping early at epoch 428\n",
      "Stopping early at epoch 429\n",
      "Stopping early at epoch 430\n",
      "Stopping early at epoch 431\n",
      "Stopping early at epoch 432\n",
      "Stopping early at epoch 433\n",
      "Stopping early at epoch 434\n",
      "Stopping early at epoch 435\n",
      "Stopping early at epoch 436\n",
      "Stopping early at epoch 437\n",
      "Stopping early at epoch 438\n",
      "Stopping early at epoch 439\n",
      "Stopping early at epoch 440\n",
      "Stopping early at epoch 441\n",
      "Stopping early at epoch 442\n",
      "Stopping early at epoch 443\n",
      "Stopping early at epoch 444\n",
      "Stopping early at epoch 445\n",
      "Stopping early at epoch 446\n",
      "Stopping early at epoch 447\n",
      "Stopping early at epoch 448\n",
      "Stopping early at epoch 449\n",
      "Stopping early at epoch 450\n",
      "Stopping early at epoch 451\n",
      "Stopping early at epoch 452\n",
      "Stopping early at epoch 453\n",
      "Stopping early at epoch 454\n",
      "Stopping early at epoch 455\n",
      "Stopping early at epoch 456\n",
      "Stopping early at epoch 457\n",
      "Stopping early at epoch 458\n",
      "Stopping early at epoch 459\n",
      "Stopping early at epoch 460\n",
      "Stopping early at epoch 461\n",
      "Stopping early at epoch 462\n",
      "Stopping early at epoch 463\n",
      "Stopping early at epoch 464\n",
      "Stopping early at epoch 465\n",
      "Stopping early at epoch 466\n",
      "Stopping early at epoch 467\n",
      "Stopping early at epoch 468\n",
      "Stopping early at epoch 469\n",
      "Stopping early at epoch 470\n",
      "Stopping early at epoch 471\n",
      "Stopping early at epoch 472\n",
      "Stopping early at epoch 473\n",
      "Stopping early at epoch 474\n",
      "Stopping early at epoch 475\n",
      "Stopping early at epoch 476\n",
      "Stopping early at epoch 477\n",
      "Stopping early at epoch 478\n",
      "Stopping early at epoch 479\n",
      "Stopping early at epoch 480\n",
      "Stopping early at epoch 481\n",
      "Stopping early at epoch 482\n",
      "Stopping early at epoch 483\n",
      "Stopping early at epoch 484\n",
      "Stopping early at epoch 485\n",
      "Stopping early at epoch 486\n",
      "Stopping early at epoch 487\n",
      "Stopping early at epoch 488\n",
      "Stopping early at epoch 489\n",
      "Stopping early at epoch 490\n",
      "Stopping early at epoch 491\n",
      "Stopping early at epoch 492\n",
      "Stopping early at epoch 493\n",
      "Stopping early at epoch 494\n",
      "Stopping early at epoch 495\n",
      "Stopping early at epoch 496\n",
      "Stopping early at epoch 497\n",
      "Stopping early at epoch 498\n",
      "Stopping early at epoch 499\n",
      "GNN training took 0.663 seconds.\n",
      "Best cumulative loss: 3.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp18_loss.pth', './testData/prepareDS_8_1.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 19\n",
    "\n",
    "- expriment 19 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 8\n",
    "- trainign DS contains only graph\n",
    "- Instead of adjacent matrix input, we are inputing an embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    edges = [\n",
    "        (0, 1, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (2, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 5, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (6, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (3, 4, {\"weight\": 1, \"capacity\": 1})  # Single edge between the two groups\n",
    "    ]\n",
    "\n",
    "    graph = CreateDummyFunction(edges)\n",
    "    graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "    graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "    q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "    dataset = {}\n",
    "    dataset[0] = [graph_dgl,q_torch, graph, [0,1] ]\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(8, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            embed = nn.Embedding(8, dim_embedding)\n",
    "            embed = embed.type(torch_dtype).to(torch_device)\n",
    "            inputs = embed.weight\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, inputs)\n",
    "            #logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 8 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def CreateDummyFunction(edges):\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(edges)\n",
    "    test_graph.order()\n",
    "    return test_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 5.0\n",
      "Epoch: 100, Cumulative Loss: 1000.0\n",
      "Epoch: 200, Cumulative Loss: 4.0\n",
      "Epoch: 300, Cumulative Loss: 1000.0\n",
      "Epoch: 400, Cumulative Loss: 1003.9999389648438\n",
      "GNN training took 0.661 seconds.\n",
      "Best cumulative loss: 3.0\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp19_loss.pth', './testData/prepareDS_8_1.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 20\n",
    "\n",
    "- expriment 20 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 8\n",
    "- Instead of adjacent matrix input, we are inputing an embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "    #\n",
    "    edges = [\n",
    "        (0, 1, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (2, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 5, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (6, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (3, 4, {\"weight\": 1, \"capacity\": 1})  # Single edge between the two groups\n",
    "    ]\n",
    "\n",
    "    graph = CreateDummyFunction(edges)\n",
    "    graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "    graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "    q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "    # dataset = {}\n",
    "    dataset[0] = [graph_dgl,q_torch, graph, [0,1] ]\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(8, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = torch.ones((8, 8))\n",
    "\n",
    "    embd = {}\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            # embed = nn.Embedding(8, dim_embedding)\n",
    "            # embed = embed.type(torch_dtype).to(torch_device)\n",
    "            # inputs = embed.weight\n",
    "            embed = nn.Embedding(8, dim_embedding)\n",
    "            embed = embed.type(torch_dtype).to(torch_device)\n",
    "            inputs = embed.weight\n",
    "\n",
    "            if key not in embd:\n",
    "                embd[key] = inputs\n",
    "\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, embd[key])\n",
    "            #logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 8 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def CreateDummyFunction(edges):\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(edges)\n",
    "    test_graph.order()\n",
    "    return test_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 171089.99511921406\n",
      "Epoch: 100, Cumulative Loss: 122145.99756252766\n",
      "Epoch: 200, Cumulative Loss: 114167.99835264683\n",
      "Epoch: 300, Cumulative Loss: 108170.99920773506\n",
      "Epoch: 400, Cumulative Loss: 108174.99914598465\n",
      "GNN training took 202.976 seconds.\n",
      "Best cumulative loss: 999.9998779296875\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp20_loss.pth', './testData/prepareDS_8_1.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 21\n",
    "\n",
    "- expriment 21 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 8\n",
    "- Instead of adjacent matrix input, we are inputing an embedding\n",
    "- Using modified neural network, using message parsing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    edges = [\n",
    "        (0, 1, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (0, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 2, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (1, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (2, 3, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 5, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (4, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 6, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (5, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (6, 7, {\"weight\": 1, \"capacity\": 1}),\n",
    "        (3, 4, {\"weight\": 1, \"capacity\": 1})  # Single edge between the two groups\n",
    "    ]\n",
    "\n",
    "    graph = CreateDummyFunction(edges)\n",
    "    graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "    graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "    q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "    dataset = {}\n",
    "    dataset[0] = [graph_dgl,q_torch, graph, [0,1] ]\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            nx_directed_graph = graph.to_directed()\n",
    "            dgl_graph = dgl.from_networkx(nx_graph=nx_directed_graph, edge_attrs=['capacity'])\n",
    "            dgl_graph = dgl_graph.to(TORCH_DEVICE)\n",
    "            edge_features = dgl_graph.edata['capacity'].unsqueeze(1).to(TORCH_DTYPE)\n",
    "            logits = net(dgl_graph, adjacency_matrix, edge_features)\n",
    "            logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "\n",
    "class CustomGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(CustomGCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.edge_linear = nn.Linear(1, out_feats)\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        with g.local_scope():\n",
    "            # # Apply linear transformation to node features\n",
    "            # h = self.linear(h)\n",
    "            # g.ndata['h'] = h\n",
    "\n",
    "            # Apply linear transformation to edge capacities\n",
    "            e = self.edge_linear(e)\n",
    "            g.edata['e'] = e\n",
    "\n",
    "            # Message passing with edge capacities\n",
    "            g.update_all(message_func=fn.u_add_e('e', 'm'),\n",
    "                         reduce_func=fn.mean('m', 'h_new'))\n",
    "\n",
    "            return g.ndata['h_new']\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.layer1 = CustomGCNLayer(in_feats, h_feats)\n",
    "        self.layer2 = CustomGCNLayer(h_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, node_features, edge_features):\n",
    "        h = self.layer1(g, node_features, edge_features)\n",
    "        h = torch.relu(h)\n",
    "        h = self.layer2(g, h,h)\n",
    "        return h\n",
    "\n",
    "# class GCNSoftmax(nn.Module):\n",
    "#     def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "#         super(GCNSoftmax, self).__init__()\n",
    "#         self.dropout_frac = dropout\n",
    "#         self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "#         self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "#\n",
    "#     def forward(self, g, inputs):\n",
    "#         # Basic forward pass\n",
    "#         h = self.conv1(g, inputs)\n",
    "#         h = F.relu(h)\n",
    "#         h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "#         h = self.conv2(g, h)\n",
    "#         h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "#         # h = F.sigmoid(h)\n",
    "#         # h = override_fixed_nodes(h)\n",
    "#\n",
    "#         return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    # output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 8 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def CreateDummyFunction(edges):\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(edges)\n",
    "    test_graph.order()\n",
    "    return test_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "func() missing 1 required positional argument: 'out'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[99], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_2wayNeural\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossExp19_loss.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./testData/prepareDS_8_1.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[33], line 105\u001B[0m, in \u001B[0;36mtrain_2wayNeural\u001B[0;34m(modelName, filename)\u001B[0m\n\u001B[1;32m     84\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    103\u001B[0m \n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m--> 105\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[98], line 87\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     85\u001B[0m dgl_graph \u001B[38;5;241m=\u001B[39m dgl_graph\u001B[38;5;241m.\u001B[39mto(TORCH_DEVICE)\n\u001B[1;32m     86\u001B[0m edge_features \u001B[38;5;241m=\u001B[39m dgl_graph\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapacity\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(TORCH_DTYPE)\n\u001B[0;32m---> 87\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdgl_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madjacency_matrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m logits \u001B[38;5;241m=\u001B[39m override_fixed_nodes(logits)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Apply max to one-hot encoding\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[98], line 200\u001B[0m, in \u001B[0;36mGCNSoftmax.forward\u001B[0;34m(self, g, node_features, edge_features)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, g, node_features, edge_features):\n\u001B[0;32m--> 200\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m     h \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(h)\n\u001B[1;32m    202\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(g, h,h)\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[98], line 188\u001B[0m, in \u001B[0;36mCustomGCNLayer.forward\u001B[0;34m(self, g, h, e)\u001B[0m\n\u001B[1;32m    185\u001B[0m g\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124me\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m e\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# Message passing with edge capacities\u001B[39;00m\n\u001B[0;32m--> 188\u001B[0m g\u001B[38;5;241m.\u001B[39mupdate_all(message_func\u001B[38;5;241m=\u001B[39m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mu_add_e\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43me\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    189\u001B[0m              reduce_func\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh_new\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m g\u001B[38;5;241m.\u001B[39mndata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh_new\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mTypeError\u001B[0m: func() missing 1 required positional argument: 'out'"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train_2wayNeural('_80wayCut_LossExp19_loss.pth', './testData/prepareDS_8_1.pkl')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 22\n",
    "\n",
    "- expriment 22 of modifying the loss function (purely binary input) and find exact loss value (vectorized)\n",
    "- removing terminal loss\n",
    "- training on dataset with 80\n",
    "- Instead of adjacent matrix input, we are inputing an embedding\n",
    "- Using modified neural network, using message parsing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "def apply_max_to_one_hot(output):\n",
    "    return torch.stack([max_to_one_hot(output[i]) for i in range(output.size(0))])\n",
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    dss = {}\n",
    "    dss[0] = dataset[0]\n",
    "    # dss[1] = dataset[1]\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dss.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, inputs)\n",
    "            #logits = override_fixed_nodes(logits)\n",
    "            # Apply max to one-hot encoding\n",
    "            one_hot_output = apply_max_to_one_hot(logits)\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( one_hot_output, adjacency_matrix)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        # h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0],requires_grad=True) + h[0] - h[0].detach()\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0],requires_grad=True)+ h[1] - h[1].detach()\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0],requires_grad=True)+ h[2] - h[2].detach()\n",
    "    return output\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    return C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=1000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 80 #int(np.sqrt(4096))    # e.g. 10, used to be the one before\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def CreateDummyFunction(edges):\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(edges)\n",
    "    test_graph.order()\n",
    "    return test_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 3228.0\n",
      "Stopping early at epoch 51\n",
      "Stopping early at epoch 52\n",
      "Stopping early at epoch 77\n",
      "Stopping early at epoch 78\n",
      "Stopping early at epoch 79\n",
      "Stopping early at epoch 80\n",
      "Stopping early at epoch 81\n",
      "Stopping early at epoch 82\n",
      "Stopping early at epoch 83\n",
      "Epoch: 100, Cumulative Loss: 98.00000762939453\n",
      "Stopping early at epoch 116\n",
      "Stopping early at epoch 117\n",
      "Stopping early at epoch 118\n",
      "Stopping early at epoch 119\n",
      "Stopping early at epoch 120\n",
      "Stopping early at epoch 121\n",
      "Stopping early at epoch 122\n",
      "Stopping early at epoch 123\n",
      "Stopping early at epoch 124\n",
      "Stopping early at epoch 125\n",
      "Stopping early at epoch 126\n",
      "Stopping early at epoch 127\n",
      "Stopping early at epoch 128\n",
      "Stopping early at epoch 129\n",
      "Stopping early at epoch 130\n",
      "Stopping early at epoch 131\n",
      "Stopping early at epoch 132\n",
      "Stopping early at epoch 133\n",
      "Stopping early at epoch 134\n",
      "Stopping early at epoch 135\n",
      "Stopping early at epoch 136\n",
      "Stopping early at epoch 137\n",
      "Stopping early at epoch 138\n",
      "Stopping early at epoch 139\n",
      "Stopping early at epoch 140\n",
      "Stopping early at epoch 141\n",
      "Stopping early at epoch 142\n",
      "Stopping early at epoch 143\n",
      "Stopping early at epoch 144\n",
      "Stopping early at epoch 145\n",
      "Stopping early at epoch 146\n",
      "Stopping early at epoch 147\n",
      "Stopping early at epoch 148\n",
      "Stopping early at epoch 149\n",
      "Stopping early at epoch 150\n",
      "Stopping early at epoch 151\n",
      "Stopping early at epoch 152\n",
      "Stopping early at epoch 153\n",
      "Stopping early at epoch 154\n",
      "Stopping early at epoch 155\n",
      "Stopping early at epoch 156\n",
      "Stopping early at epoch 157\n",
      "Stopping early at epoch 158\n",
      "Stopping early at epoch 159\n",
      "Stopping early at epoch 160\n",
      "Stopping early at epoch 161\n",
      "Stopping early at epoch 162\n",
      "Stopping early at epoch 163\n",
      "Stopping early at epoch 164\n",
      "Stopping early at epoch 165\n",
      "Stopping early at epoch 166\n",
      "Stopping early at epoch 167\n",
      "Stopping early at epoch 168\n",
      "Stopping early at epoch 169\n",
      "Stopping early at epoch 170\n",
      "Stopping early at epoch 171\n",
      "Stopping early at epoch 172\n",
      "Stopping early at epoch 173\n",
      "Stopping early at epoch 174\n",
      "Stopping early at epoch 175\n",
      "Stopping early at epoch 176\n",
      "Stopping early at epoch 177\n",
      "Stopping early at epoch 178\n",
      "Stopping early at epoch 179\n",
      "Stopping early at epoch 180\n",
      "Stopping early at epoch 181\n",
      "Stopping early at epoch 182\n",
      "Stopping early at epoch 183\n",
      "Stopping early at epoch 184\n",
      "Stopping early at epoch 185\n",
      "Stopping early at epoch 186\n",
      "Stopping early at epoch 187\n",
      "Stopping early at epoch 188\n",
      "Stopping early at epoch 189\n",
      "Stopping early at epoch 190\n",
      "Stopping early at epoch 191\n",
      "Stopping early at epoch 192\n",
      "Stopping early at epoch 193\n",
      "Stopping early at epoch 194\n",
      "Stopping early at epoch 195\n",
      "Stopping early at epoch 196\n",
      "Stopping early at epoch 197\n",
      "Stopping early at epoch 198\n",
      "Stopping early at epoch 199\n",
      "Stopping early at epoch 200\n",
      "Epoch: 200, Cumulative Loss: 98.00004577636719\n",
      "Stopping early at epoch 201\n",
      "Stopping early at epoch 202\n",
      "Stopping early at epoch 203\n",
      "Stopping early at epoch 204\n",
      "Stopping early at epoch 205\n",
      "Stopping early at epoch 206\n",
      "Stopping early at epoch 207\n",
      "Stopping early at epoch 208\n",
      "Stopping early at epoch 209\n",
      "Stopping early at epoch 210\n",
      "Stopping early at epoch 211\n",
      "Stopping early at epoch 212\n",
      "Stopping early at epoch 213\n",
      "Stopping early at epoch 214\n",
      "Stopping early at epoch 215\n",
      "Stopping early at epoch 216\n",
      "Stopping early at epoch 217\n",
      "Stopping early at epoch 218\n",
      "Stopping early at epoch 219\n",
      "Stopping early at epoch 220\n",
      "Stopping early at epoch 221\n",
      "Stopping early at epoch 222\n",
      "Stopping early at epoch 223\n",
      "Stopping early at epoch 224\n",
      "Stopping early at epoch 225\n",
      "Stopping early at epoch 226\n",
      "Stopping early at epoch 227\n",
      "Stopping early at epoch 228\n",
      "Stopping early at epoch 229\n",
      "Stopping early at epoch 230\n",
      "Stopping early at epoch 231\n",
      "Stopping early at epoch 232\n",
      "Stopping early at epoch 233\n",
      "Stopping early at epoch 234\n",
      "Stopping early at epoch 235\n",
      "Stopping early at epoch 236\n",
      "Stopping early at epoch 237\n",
      "Stopping early at epoch 238\n",
      "Stopping early at epoch 239\n",
      "Stopping early at epoch 240\n",
      "Stopping early at epoch 241\n",
      "Stopping early at epoch 242\n",
      "Stopping early at epoch 243\n",
      "Stopping early at epoch 244\n",
      "Stopping early at epoch 245\n",
      "Stopping early at epoch 246\n",
      "Stopping early at epoch 247\n",
      "Stopping early at epoch 248\n",
      "Stopping early at epoch 249\n",
      "Stopping early at epoch 250\n",
      "Stopping early at epoch 251\n",
      "Stopping early at epoch 252\n",
      "Stopping early at epoch 253\n",
      "Stopping early at epoch 254\n",
      "Stopping early at epoch 255\n",
      "Stopping early at epoch 256\n",
      "Stopping early at epoch 257\n",
      "Stopping early at epoch 258\n",
      "Stopping early at epoch 259\n",
      "Stopping early at epoch 260\n",
      "Stopping early at epoch 261\n",
      "Stopping early at epoch 262\n",
      "Stopping early at epoch 263\n",
      "Stopping early at epoch 264\n",
      "Stopping early at epoch 265\n",
      "Stopping early at epoch 266\n",
      "Stopping early at epoch 267\n",
      "Stopping early at epoch 268\n",
      "Stopping early at epoch 269\n",
      "Stopping early at epoch 270\n",
      "Stopping early at epoch 271\n",
      "Stopping early at epoch 272\n",
      "Stopping early at epoch 273\n",
      "Stopping early at epoch 274\n",
      "Stopping early at epoch 275\n",
      "Stopping early at epoch 276\n",
      "Stopping early at epoch 277\n",
      "Stopping early at epoch 278\n",
      "Stopping early at epoch 279\n",
      "Stopping early at epoch 280\n",
      "Stopping early at epoch 281\n",
      "Stopping early at epoch 282\n",
      "Stopping early at epoch 283\n",
      "Stopping early at epoch 284\n",
      "Stopping early at epoch 285\n",
      "Stopping early at epoch 286\n",
      "Stopping early at epoch 287\n",
      "Stopping early at epoch 288\n",
      "Stopping early at epoch 289\n",
      "Stopping early at epoch 290\n",
      "Stopping early at epoch 291\n",
      "Stopping early at epoch 292\n",
      "Stopping early at epoch 293\n",
      "Stopping early at epoch 294\n",
      "Stopping early at epoch 295\n",
      "Stopping early at epoch 296\n",
      "Stopping early at epoch 297\n",
      "Stopping early at epoch 298\n",
      "Stopping early at epoch 299\n",
      "Stopping early at epoch 300\n",
      "Epoch: 300, Cumulative Loss: 98.00006103515625\n",
      "Stopping early at epoch 301\n",
      "Stopping early at epoch 302\n",
      "Stopping early at epoch 303\n",
      "Stopping early at epoch 304\n",
      "Stopping early at epoch 305\n",
      "Stopping early at epoch 306\n",
      "Stopping early at epoch 307\n",
      "Stopping early at epoch 308\n",
      "Stopping early at epoch 309\n",
      "Stopping early at epoch 310\n",
      "Stopping early at epoch 311\n",
      "Stopping early at epoch 312\n",
      "Stopping early at epoch 313\n",
      "Stopping early at epoch 314\n",
      "Stopping early at epoch 315\n",
      "Stopping early at epoch 316\n",
      "Stopping early at epoch 317\n",
      "Stopping early at epoch 318\n",
      "Stopping early at epoch 319\n",
      "Stopping early at epoch 320\n",
      "Stopping early at epoch 321\n",
      "Stopping early at epoch 322\n",
      "Stopping early at epoch 323\n",
      "Stopping early at epoch 324\n",
      "Stopping early at epoch 325\n",
      "Stopping early at epoch 326\n",
      "Stopping early at epoch 327\n",
      "Stopping early at epoch 328\n",
      "Stopping early at epoch 329\n",
      "Stopping early at epoch 330\n",
      "Stopping early at epoch 331\n",
      "Stopping early at epoch 332\n",
      "Stopping early at epoch 333\n",
      "Stopping early at epoch 334\n",
      "Stopping early at epoch 335\n",
      "Stopping early at epoch 336\n",
      "Stopping early at epoch 337\n",
      "Stopping early at epoch 338\n",
      "Stopping early at epoch 339\n",
      "Stopping early at epoch 340\n",
      "Stopping early at epoch 341\n",
      "Stopping early at epoch 342\n",
      "Stopping early at epoch 343\n",
      "Stopping early at epoch 344\n",
      "Stopping early at epoch 345\n",
      "Stopping early at epoch 346\n",
      "Stopping early at epoch 347\n",
      "Stopping early at epoch 348\n",
      "Stopping early at epoch 349\n",
      "Stopping early at epoch 350\n",
      "Stopping early at epoch 351\n",
      "Stopping early at epoch 352\n",
      "Stopping early at epoch 353\n",
      "Stopping early at epoch 354\n",
      "Stopping early at epoch 355\n",
      "Stopping early at epoch 356\n",
      "Stopping early at epoch 357\n",
      "Stopping early at epoch 358\n",
      "Stopping early at epoch 359\n",
      "Stopping early at epoch 360\n",
      "Stopping early at epoch 361\n",
      "Stopping early at epoch 362\n",
      "Stopping early at epoch 363\n",
      "Stopping early at epoch 364\n",
      "Stopping early at epoch 365\n",
      "Stopping early at epoch 366\n",
      "Stopping early at epoch 367\n",
      "Stopping early at epoch 368\n",
      "Stopping early at epoch 369\n",
      "Stopping early at epoch 370\n",
      "Stopping early at epoch 371\n",
      "Stopping early at epoch 372\n",
      "Stopping early at epoch 373\n",
      "Stopping early at epoch 374\n",
      "Stopping early at epoch 375\n",
      "Stopping early at epoch 376\n",
      "Stopping early at epoch 377\n",
      "Stopping early at epoch 378\n",
      "Stopping early at epoch 379\n",
      "Stopping early at epoch 380\n",
      "Stopping early at epoch 381\n",
      "Stopping early at epoch 382\n",
      "Stopping early at epoch 383\n",
      "Stopping early at epoch 384\n",
      "Stopping early at epoch 385\n",
      "Stopping early at epoch 386\n",
      "Stopping early at epoch 387\n",
      "Stopping early at epoch 388\n",
      "Stopping early at epoch 389\n",
      "Stopping early at epoch 390\n",
      "Stopping early at epoch 391\n",
      "Stopping early at epoch 392\n",
      "Stopping early at epoch 393\n",
      "Stopping early at epoch 394\n",
      "Stopping early at epoch 395\n",
      "Stopping early at epoch 396\n",
      "Stopping early at epoch 397\n",
      "Stopping early at epoch 398\n",
      "Stopping early at epoch 399\n",
      "Stopping early at epoch 400\n",
      "Epoch: 400, Cumulative Loss: 98.00004577636719\n",
      "Stopping early at epoch 401\n",
      "Stopping early at epoch 402\n",
      "Stopping early at epoch 403\n",
      "Stopping early at epoch 404\n",
      "Stopping early at epoch 405\n",
      "Stopping early at epoch 406\n",
      "Stopping early at epoch 407\n",
      "Stopping early at epoch 408\n",
      "Stopping early at epoch 409\n",
      "Stopping early at epoch 410\n",
      "Stopping early at epoch 411\n",
      "Stopping early at epoch 412\n",
      "Stopping early at epoch 413\n",
      "Stopping early at epoch 414\n",
      "Stopping early at epoch 415\n",
      "Stopping early at epoch 416\n",
      "Stopping early at epoch 417\n",
      "Stopping early at epoch 418\n",
      "Stopping early at epoch 419\n",
      "Stopping early at epoch 420\n",
      "Stopping early at epoch 421\n",
      "Stopping early at epoch 422\n",
      "Stopping early at epoch 423\n",
      "Stopping early at epoch 424\n",
      "Stopping early at epoch 425\n",
      "Stopping early at epoch 426\n",
      "Stopping early at epoch 427\n",
      "Stopping early at epoch 428\n",
      "Stopping early at epoch 429\n",
      "Stopping early at epoch 430\n",
      "Stopping early at epoch 431\n",
      "Stopping early at epoch 432\n",
      "Stopping early at epoch 433\n",
      "Stopping early at epoch 434\n",
      "Stopping early at epoch 435\n",
      "Stopping early at epoch 436\n",
      "Stopping early at epoch 437\n",
      "Stopping early at epoch 438\n",
      "Stopping early at epoch 439\n",
      "Stopping early at epoch 440\n",
      "Stopping early at epoch 441\n",
      "Stopping early at epoch 442\n",
      "Stopping early at epoch 443\n",
      "Stopping early at epoch 444\n",
      "Stopping early at epoch 445\n",
      "Stopping early at epoch 446\n",
      "Stopping early at epoch 447\n",
      "Stopping early at epoch 448\n",
      "Stopping early at epoch 449\n",
      "Stopping early at epoch 450\n",
      "Stopping early at epoch 451\n",
      "Stopping early at epoch 452\n",
      "Stopping early at epoch 453\n",
      "Stopping early at epoch 454\n",
      "Stopping early at epoch 455\n",
      "Stopping early at epoch 456\n",
      "Stopping early at epoch 457\n",
      "Stopping early at epoch 458\n",
      "Stopping early at epoch 459\n",
      "Stopping early at epoch 460\n",
      "Stopping early at epoch 461\n",
      "Stopping early at epoch 462\n",
      "Stopping early at epoch 463\n",
      "Stopping early at epoch 464\n",
      "Stopping early at epoch 465\n",
      "Stopping early at epoch 466\n",
      "Stopping early at epoch 467\n",
      "Stopping early at epoch 468\n",
      "Stopping early at epoch 469\n",
      "Stopping early at epoch 470\n",
      "Stopping early at epoch 471\n",
      "Stopping early at epoch 472\n",
      "Stopping early at epoch 473\n",
      "Stopping early at epoch 474\n",
      "Stopping early at epoch 475\n",
      "Stopping early at epoch 476\n",
      "Stopping early at epoch 477\n",
      "Stopping early at epoch 478\n",
      "Stopping early at epoch 479\n",
      "Stopping early at epoch 480\n",
      "Stopping early at epoch 481\n",
      "Stopping early at epoch 482\n",
      "Stopping early at epoch 483\n",
      "Stopping early at epoch 484\n",
      "Stopping early at epoch 485\n",
      "Stopping early at epoch 486\n",
      "Stopping early at epoch 487\n",
      "Stopping early at epoch 488\n",
      "Stopping early at epoch 489\n",
      "Stopping early at epoch 490\n",
      "Stopping early at epoch 491\n",
      "Stopping early at epoch 492\n",
      "Stopping early at epoch 493\n",
      "Stopping early at epoch 494\n",
      "Stopping early at epoch 495\n",
      "Stopping early at epoch 496\n",
      "Stopping early at epoch 497\n",
      "Stopping early at epoch 498\n",
      "Stopping early at epoch 499\n",
      "GNN training took 1.47 seconds.\n",
      "Best cumulative loss: 98.00000762939453\n"
     ]
    }
   ],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossExp21_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# make two small graphs (fully connected) and connect them with one edge, all edges to be 1\n",
    "\n",
    "\n",
    "# see if the original paper gives code for maximum cut\n",
    "\n",
    "# train random graph with weight 1, then use the above graph to test\n",
    "\n",
    "and then start adding weight to the edges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing loss function locally with examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences:\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000, -0.2000, -0.1000],\n",
      "         [ 0.1000, -0.1000,  0.0000]],\n",
      "\n",
      "        [[-0.3000,  0.2000,  0.1000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.2000,  0.1000,  0.1000]],\n",
      "\n",
      "        [[-0.1000,  0.1000,  0.0000],\n",
      "         [ 0.2000, -0.1000, -0.1000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]])\n",
      "Absolute differences:\n",
      " tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.3000, 0.2000, 0.1000],\n",
      "         [0.1000, 0.1000, 0.0000]],\n",
      "\n",
      "        [[0.3000, 0.2000, 0.1000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.2000, 0.1000, 0.1000]],\n",
      "\n",
      "        [[0.1000, 0.1000, 0.0000],\n",
      "         [0.2000, 0.1000, 0.1000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "Summed differences:\n",
      " tensor([[0.0000, 0.6000, 0.2000],\n",
      "        [0.6000, 0.0000, 0.4000],\n",
      "        [0.2000, 0.4000, 0.0000]])\n",
      "Weighted summed differences:\n",
      " tensor([[0.0000, 0.6000, 0.1000],\n",
      "        [0.6000, 0.0000, 0.0800],\n",
      "        [0.1000, 0.0800, 0.0000]])\n",
      "Min Cut Loss:\n",
      " tensor(1.5600)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Adjusted example for 3 partitions\n",
    "s = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.5, 0.3], [0.4, 0.4, 0.2]])\n",
    "adjacency_matrix = torch.tensor([[0.0, 1.0, 0.5], [1.0, 0.0, 0.2], [0.5, 0.2, 0.0]])\n",
    "\n",
    "# Compute differences\n",
    "diff = s.unsqueeze(1) - s.unsqueeze(0)\n",
    "abs_diff = torch.abs(diff)\n",
    "sum_diff = torch.sum(abs_diff, dim=2)\n",
    "min_cut_loss = torch.sum(sum_diff * adjacency_matrix)\n",
    "\n",
    "# Print results\n",
    "print(\"Differences:\\n\", diff)\n",
    "print(\"Absolute differences:\\n\", abs_diff)\n",
    "print(\"Summed differences:\\n\", sum_diff)\n",
    "print(\"Weighted summed differences:\\n\", sum_diff * adjacency_matrix)\n",
    "print(\"Min Cut Loss:\\n\", min_cut_loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1.], grad_fn=<SubBackward0>)\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def max_to_one_hot(tensor):\n",
    "    # Find the index of the maximum value\n",
    "    max_index = torch.argmax(tensor)\n",
    "\n",
    "    # Create a one-hot encoded tensor\n",
    "    one_hot_tensor = torch.zeros_like(tensor)\n",
    "    one_hot_tensor[max_index] = 1.0\n",
    "\n",
    "    one_hot_tensor = one_hot_tensor + tensor - tensor.detach()\n",
    "\n",
    "    return one_hot_tensor\n",
    "\n",
    "# Example usage\n",
    "tensor = torch.tensor([0.3, 0.1, 0.6], requires_grad=True)\n",
    "one_hot_tensor = max_to_one_hot(tensor)\n",
    "print(one_hot_tensor)\n",
    "\n",
    "# To check if it backpropagates correctly\n",
    "one_hot_tensor.sum().backward()\n",
    "print(tensor.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate over each pair of nodes to compute the contribution to the cut value\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):  # Only consider upper triangle to avoid double counting\n",
    "            # if adjacency_matrix[i, j] > 0:\n",
    "                # Check if nodes i and j are in different partitions\n",
    "                # in_different_partitions = (s[i] * s[j]).sum() == 0\n",
    "                # if in_different_partitions:\n",
    "                    loss += (1 - (s[i] * s[j]).sum())*adjacency_matrix[i, j]\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "s = torch.tensor([[1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [0, 1]], dtype=torch.float32)\n",
    "\n",
    "adjacency_matrix = torch.tensor([[0, 2, 1, 0],\n",
    "                                 [2, 0, 5, 1],\n",
    "                                 [1, 5, 0, 1],\n",
    "                                 [0, 1, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Compute the loss\n",
    "loss_value = min_cut_loss(s, adjacency_matrix)\n",
    "print(f\"Loss: {loss_value.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vectorized_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the minimum cut loss, which is the total weight of edges cut between partitions using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    s (torch.Tensor): Binary partition matrix of shape (num_nodes, num_partitions)\n",
    "    adjacency_matrix (torch.Tensor): Adjacency matrix of the graph of shape (num_nodes, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Scalar loss value representing the total weight of edges cut\n",
    "    \"\"\"\n",
    "    num_nodes, num_partitions = s.shape\n",
    "\n",
    "    # Compute the partition probability matrix for all partitions\n",
    "    partition_prob_matrix = s @ s.T\n",
    "\n",
    "    # Compute the cut value by summing weights of edges that connect nodes in different partitions\n",
    "    cut_value = adjacency_matrix * (1 - partition_prob_matrix)\n",
    "\n",
    "    # Sum up the contributions for all edges\n",
    "    loss = torch.sum(cut_value) / 2  # Divide by 2 to correct for double-counting\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "s = torch.tensor([[1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [0, 1]], dtype=torch.float32)\n",
    "\n",
    "adjacency_matrix = torch.tensor([[0, 2, 1, 0],\n",
    "                                 [2, 0, 5, 1],\n",
    "                                 [1, 5, 0, 1],\n",
    "                                 [0, 1, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Compute the loss\n",
    "loss_value = vectorized_min_cut_loss(s, adjacency_matrix)\n",
    "print(f\"Loss: {loss_value.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node logits after Custom GCN forward pass:\n",
      "tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.9134, -0.5275],\n",
      "        [ 1.8280, -0.8404],\n",
      "        [ 2.7426, -1.1532]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test item\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "# Define node features\n",
    "node_features = torch.tensor([\n",
    "    [1, 0, 1],  # Node 0\n",
    "    [0, 1, 0],  # Node 1\n",
    "    [1, 1, 0],  # Node 2\n",
    "    [0, 0, 1]   # Node 3\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Define edges and their capacities\n",
    "edges = [(0, 1), (0, 2), (1, 2), (1, 3)]\n",
    "capacities = [1.0, 2.0, 1.5, 2.5]  # Capacities for each edge\n",
    "\n",
    "# Create the DGL graph\n",
    "g = dgl.graph(edges)\n",
    "g.edata['capacity'] = torch.tensor(capacities, dtype=torch.float32)\n",
    "\n",
    "# Visualize the graph with NetworkX for illustration purposes\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx_g = g.to_networkx().to_undirected()\n",
    "pos = nx.spring_layout(nx_g)\n",
    "nx.draw(nx_g, pos, with_labels=True, node_size=700, node_color=\"lightblue\")\n",
    "edge_labels = {edge: f'{cap}' for edge, cap in zip(edges, capacities)}\n",
    "nx.draw_networkx_edge_labels(nx_g, pos, edge_labels=edge_labels)\n",
    "plt.show()\n",
    "\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class CustomGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(CustomGCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.edge_linear = nn.Linear(1, out_feats)\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        with g.local_scope():\n",
    "            # Apply linear transformation to node features\n",
    "            h = self.linear(h)\n",
    "            g.ndata['h'] = h\n",
    "\n",
    "            # Apply linear transformation to edge capacities\n",
    "            e = self.edge_linear(e)\n",
    "            g.edata['e'] = e\n",
    "\n",
    "            # Message passing with edge capacities\n",
    "            g.update_all(message_func=fn.u_add_e('h', 'e', 'm'),\n",
    "                         reduce_func=fn.mean('m', 'h_new'))\n",
    "\n",
    "            return g.ndata['h_new']\n",
    "\n",
    "class CustomGCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats):\n",
    "        super(CustomGCN, self).__init__()\n",
    "        self.layer1 = CustomGCNLayer(in_feats, h_feats)\n",
    "        self.layer2 = CustomGCNLayer(h_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, node_features, edge_features):\n",
    "        h = self.layer1(g, node_features, edge_features)\n",
    "        h = torch.relu(h)\n",
    "        h = self.layer2(g, h, edge_features)\n",
    "        return h\n",
    "\n",
    "# Create the custom GCN model\n",
    "in_feats = node_features.shape[1]\n",
    "h_feats = 4  # Number of hidden units\n",
    "out_feats = 2  # Number of output units (for example, for 2 classes in classification)\n",
    "\n",
    "model = CustomGCN(in_feats, h_feats, out_feats)\n",
    "\n",
    "# Convert edge capacities to a tensor for the model\n",
    "edge_features = g.edata['capacity'].unsqueeze(1)\n",
    "\n",
    "# Forward pass through the model\n",
    "logits = model(g, node_features, edge_features)\n",
    "\n",
    "print(\"Node logits after Custom GCN forward pass:\")\n",
    "print(logits)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 81\u001B[0m\n\u001B[1;32m     78\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(edge_features)  \u001B[38;5;66;03m# Replace with appropriate loss calculation\u001B[39;00m\n\u001B[1;32m     80\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 81\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "\n",
    "# Define node features (initially simple; you can add more relevant features)\n",
    "node_features = torch.tensor([\n",
    "    [1, 0, 1],  # Node 0\n",
    "    [0, 1, 0],  # Node 1\n",
    "    [1, 1, 0],  # Node 2\n",
    "    [0, 0, 1]   # Node 3\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Define edges and their capacities\n",
    "edges = [(0, 1), (0, 2), (1, 2), (1, 3)]\n",
    "capacities = [1.0, 2.0, 1.5, 2.5]  # Capacities for each edge\n",
    "\n",
    "# Create the DGL graph\n",
    "g = dgl.graph(edges)\n",
    "g.edata['capacity'] = torch.tensor(capacities, dtype=torch.float32)\n",
    "\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class CustomGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(CustomGCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.edge_linear = nn.Linear(1, out_feats)\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        with g.local_scope():\n",
    "            # Apply linear transformation to node features\n",
    "            h = self.linear(h)\n",
    "            g.ndata['h'] = h\n",
    "\n",
    "            # Apply linear transformation to edge capacities\n",
    "            e = self.edge_linear(e)\n",
    "            g.edata['e'] = e\n",
    "\n",
    "            # Message passing with edge capacities\n",
    "            g.update_all(message_func=fn.u_add_e('h', 'e', 'm'),\n",
    "                         reduce_func=fn.mean('m', 'h_new'))\n",
    "\n",
    "            return g.ndata['h_new']\n",
    "\n",
    "class CustomGCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats):\n",
    "        super(CustomGCN, self).__init__()\n",
    "        self.layer1 = CustomGCNLayer(in_feats, h_feats)\n",
    "        self.layer2 = CustomGCNLayer(h_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, node_features, edge_features):\n",
    "        h = self.layer1(g, node_features, edge_features)\n",
    "        h = torch.relu(h)\n",
    "        h = self.layer2(g, h, edge_features)\n",
    "        return h\n",
    "\n",
    "# Create the custom GCN model\n",
    "in_feats = node_features.shape[1]\n",
    "h_feats = 4  # Number of hidden units\n",
    "out_feats = 2  # Number of output units (for example, for 2 classes in classification)\n",
    "\n",
    "model = CustomGCN(in_feats, h_feats, out_feats)\n",
    "\n",
    "# Convert edge capacities to a tensor for the model\n",
    "edge_features = g.edata['capacity'].unsqueeze(1)\n",
    "\n",
    "# Simple training loop (in practice, you'd need a more complex setup with a proper dataset and optimizer)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    logits = model(g, node_features, edge_features)\n",
    "\n",
    "    # Calculate the loss (this is a placeholder; the actual loss function should be based on the min cut)\n",
    "    # Example: Minimize the sum of the capacities of the edges in the cut\n",
    "    loss = torch.sum(edge_features)  # Replace with appropriate loss calculation\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}