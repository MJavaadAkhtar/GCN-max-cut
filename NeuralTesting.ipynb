{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from commons import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.float32\n",
    "\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCNSoftmax(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer\n",
    "\n",
    "def partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the partition of each edge (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.not_equal.outer(s, s).astype(int)\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut(q_torch, s):\n",
    "    '''\n",
    "\n",
    "    :param q_torch: The adjacent matrix of the graph\n",
    "    :param s: The binary output from the neural network. s will be in form of [[prob1, prob2, ..., prob n], ...]\n",
    "    :return: The calculated cut loss value\n",
    "    '''\n",
    "    if len(s) > 0:\n",
    "        totalCuts = len(s[0])\n",
    "        CutValue = 0\n",
    "        for i in range(totalCuts):\n",
    "            CutValue += partition_weight(q_torch, s[:,i])\n",
    "        return CutValue/2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def printCombo(orig):\n",
    "    # Original dictionary\n",
    "    input_dict = orig\n",
    "\n",
    "    # Generate all permutations of the dictionary values\n",
    "    value_permutations = list(permutations(input_dict.values()))\n",
    "\n",
    "    # Create a list of dictionaries from the permutations\n",
    "    permuted_dicts = [{key: value for key, value in zip(input_dict.keys(), perm)} for perm in value_permutations]\n",
    "\n",
    "    return permuted_dicts\n",
    "\n",
    "def GetOptimal(net, dgl_graph, inp, q_torch, terminal = None):\n",
    "\n",
    "    probs = net(dgl_graph, inp, terminal)\n",
    "    binary_partitions = (probs >= 0.5).float()\n",
    "\n",
    "    for i in range(len(binary_partitions)-1):\n",
    "        if torch.sum(binary_partitions[i]) != 1:\n",
    "            binary_partitions[i] = torch.tensor([0,1,0])\n",
    "\n",
    "    cut_value_item = calculateAllCut(q_torch, binary_partitions)\n",
    "\n",
    "    return cut_value_item, binary_partitions\n",
    "\n",
    "def GetOptimalNetValue(net, dgl_graph, inp, q_torch, terminal_dict):\n",
    "    net.eval()\n",
    "    best_loss = float('inf')\n",
    "    best_binary = []\n",
    "    # if (dgl_graph.number_of_nodes() < 30):\n",
    "    #     inp = torch.ones((dgl_graph.number_of_nodes(), 30))\n",
    "\n",
    "    # find all potential combination of terminal nodes with respective indices\n",
    "\n",
    "    perm_items = printCombo(terminal_dict)\n",
    "    for i in perm_items:\n",
    "        probs = net(dgl_graph, inp, i)\n",
    "        binary_partitions = (probs >= 0.5).float()\n",
    "        # print([m for m in binary_partitions if sum(m)>1 or sum(m)==0])\n",
    "        # print(binary_partitions, q_torch)\n",
    "        cut_value_item = calculateAllCut(q_torch, binary_partitions)\n",
    "        if cut_value_item < best_loss:\n",
    "            best_loss = cut_value_item\n",
    "            best_binary = binary_partitions\n",
    "    return best_loss, best_binary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = 80    # e.g. 10\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def test1(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "    # test_item_2 = {}\n",
    "    # test_item_2[0]=test_item[1]\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "        # embed = nn.Embedding(graph.number_of_nodes(), dim_embedding)\n",
    "        # embed = embed.type(TORCH_DTYPE).to(TORCH_DEVICE)\n",
    "        # inputs = embed.weight\n",
    "        #inputs = model.embed.weight\n",
    "        # cut_val, partition = GetOptimal(model,dgl_graph, inputs, adjacency_matrix, {terminal[0]:0, terminal[1]:1, terminal[2]:2})\n",
    "        # neural_cut.append(cut_val)\n",
    "        print(inputs, inputs.size())\n",
    "        logits = net(dgl_graph, inputs)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "        # for i in range(len(binary_partitions)-1):\n",
    "        #     if torch.sum(binary_partitions[i]) != 1:\n",
    "        #         binary_partitions[i] = torch.tensor([1,0,0])\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        # cut_value, (part_1, part_2) = nx.minimum_cut(test_item_2[0][2], test_item_2[0][3][1], test_item_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "        print(\"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_LossOrig.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_LossMinCut.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_LossNew.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Losscomb.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss.pth')\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss_2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss_3.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss_4.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss_5.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss_6.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test1('./final__80wayCut_Lossinter_min_cut_loss_7.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#with no terminal loss\n",
    "test1('./final__80wayCut_Lossinter_min_cut_loss_9.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with terminal loss\n",
    "test1('./final__80wayCut_Lossinter_min_cut_loss_9.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with terminal loss in NN\n",
    "test1('./final__80wayCut_Lossinter_min_cut_loss_9.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with terminal loss in NN\n",
    "test1('./final__80wayCut_Lossinter_min_cut_loss_9.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with terminal loss in NN\n",
    "test1('./final__80wayCut_Lossinter_min_cut_loss_9.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def test2(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "    # test_item_2 = {}\n",
    "    # test_item_2[0]=test_item[1]\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "        # embed = nn.Embedding(graph.number_of_nodes(), dim_embedding)\n",
    "        # embed = embed.type(TORCH_DTYPE).to(TORCH_DEVICE)\n",
    "        # inputs = embed.weight\n",
    "        #inputs = model.embed.weight\n",
    "        # cut_val, partition = GetOptimal(model,dgl_graph, inputs, adjacency_matrix, {terminal[0]:0, terminal[1]:1, terminal[2]:2})\n",
    "        # neural_cut.append(cut_val)\n",
    "        # print(inputs, inputs.size())\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "        # for i in range(len(binary_partitions)-1):\n",
    "        #     if torch.sum(binary_partitions[i]) != 1:\n",
    "        #         binary_partitions[i] = torch.tensor([1,0,0])\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        # cut_value, (part_1, part_2) = nx.minimum_cut(test_item_2[0][2], test_item_2[0][3][1], test_item_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural 3-way min-cut value: tensor(672., dtype=torch.float64) 7.0 11.0 62.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(277., dtype=torch.float64) 9.0 5.0 66.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(498., dtype=torch.float64) 5.0 6.0 69.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(216., dtype=torch.float64) 9.0 6.0 65.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(202., dtype=torch.float64) 9.0 15.0 56.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(872., dtype=torch.float64) 10.0 12.0 58.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(752., dtype=torch.float64) 8.0 5.0 67.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(585., dtype=torch.float64) 3.0 13.0 64.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(359., dtype=torch.float64) 4.0 8.0 68.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(817., dtype=torch.float64) 13.0 8.0 59.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(337., dtype=torch.float64) 6.0 10.0 64.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2('./final__80wayCut_LossOrig_2.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural 3-way min-cut value: tensor(321., dtype=torch.float64) 38.0 0.0 42.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(37., dtype=torch.float64) 32.0 0.0 48.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(102., dtype=torch.float64) 25.0 0.0 55.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(135., dtype=torch.float64) 31.0 0.0 49.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(384., dtype=torch.float64) 23.0 0.0 57.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(59., dtype=torch.float64) 32.0 0.0 48.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(64., dtype=torch.float64) 29.0 0.0 51.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(776., dtype=torch.float64) 27.0 0.0 53.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(170., dtype=torch.float64) 32.0 0.0 48.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(132., dtype=torch.float64) 32.0 0.0 48.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(179., dtype=torch.float64) 39.0 0.0 41.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2('./final__80wayCut_Lossinter_min_cut_loss_9_new.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural 3-way min-cut value: tensor(629., dtype=torch.float64) 51.0 21.0 8.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(565., dtype=torch.float64) 69.0 5.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(734., dtype=torch.float64) 61.0 18.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(525., dtype=torch.float64) 56.0 21.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(252., dtype=torch.float64) 49.0 16.0 15.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(381., dtype=torch.float64) 61.0 18.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(209., dtype=torch.float64) 46.0 27.0 7.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(414., dtype=torch.float64) 38.0 30.0 12.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(365., dtype=torch.float64) 45.0 31.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(343., dtype=torch.float64) 48.0 20.0 12.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(601., dtype=torch.float64) 25.0 52.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2('./final__80wayCut_Lossinter_min_cut_loss_10_new.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural 3-way min-cut value: tensor(653., dtype=torch.float64) 76.0 2.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(10., dtype=torch.float64) 71.0 0.0 9.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(338., dtype=torch.float64) 75.0 4.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(206., dtype=torch.float64) 71.0 7.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(47., dtype=torch.float64) 72.0 0.0 8.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(91., dtype=torch.float64) 75.0 0.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(92., dtype=torch.float64) 67.0 9.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(232., dtype=torch.float64) 76.0 1.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(178., dtype=torch.float64) 69.0 5.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(297., dtype=torch.float64) 77.0 1.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "Neural 3-way min-cut value: tensor(28., dtype=torch.float64) 75.0 0.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2('./final__80wayCut_Lossinter_min_cut_loss_10_new.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(653., dtype=torch.float64) 76.0 2.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(10., dtype=torch.float64) 71.0 0.0 9.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(338., dtype=torch.float64) 75.0 4.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(206., dtype=torch.float64) 71.0 7.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(47., dtype=torch.float64) 62.0 10.0 8.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(91., dtype=torch.float64) 64.0 11.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(92., dtype=torch.float64) 67.0 9.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(232., dtype=torch.float64) 76.0 1.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(178., dtype=torch.float64) 69.0 5.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(297., dtype=torch.float64) 77.0 1.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(28., dtype=torch.float64) 22.0 8.0 50.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2('./final__80wayCut_Lossinter_min_cut_loss_10_new.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(338., dtype=torch.float64) 14.0 39.0 27.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(84., dtype=torch.float64) 7.0 55.0 18.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(478., dtype=torch.float64) 15.0 39.0 26.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(604., dtype=torch.float64) 8.0 44.0 28.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(270., dtype=torch.float64) 13.0 44.0 23.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(506., dtype=torch.float64) 18.0 38.0 24.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(610., dtype=torch.float64) 11.0 42.0 27.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(454., dtype=torch.float64) 8.0 48.0 24.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(542.5000, dtype=torch.float64) 20.0 38.0 21.0 Total Nodes:79.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 1\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(720., dtype=torch.float64) 15.0 34.0 31.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(814., dtype=torch.float64) 12.0 37.0 31.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2('./final__80wayCut_LossOrig_2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Associated with Exp2\n",
    "Removing terminal penalty, now using cloning\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(130., dtype=torch.float64) 70.0 6.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(12., dtype=torch.float64) 70.0 6.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(50., dtype=torch.float64) 72.0 4.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(200., dtype=torch.float64) 72.0 3.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(70., dtype=torch.float64) 70.0 5.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(180., dtype=torch.float64) 73.0 5.0 2.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(200., dtype=torch.float64) 72.0 4.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(171., dtype=torch.float64) 73.0 4.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(167., dtype=torch.float64) 68.0 7.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(31., dtype=torch.float64) 72.0 2.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(210., dtype=torch.float64) 71.0 5.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp2(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp2('./final__80wayCut_LossExp2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Associated with Exp1 - loss\n",
    "modified loss with terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(653., dtype=torch.float64) 56.0 6.0 18.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(157., dtype=torch.float64) 69.0 1.0 10.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(619., dtype=torch.float64) 60.0 12.0 8.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(224., dtype=torch.float64) 53.0 15.0 12.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(118., dtype=torch.float64) 55.0 16.0 9.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(906., dtype=torch.float64) 50.0 20.0 10.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(627., dtype=torch.float64) 55.0 15.0 10.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(767., dtype=torch.float64) 46.0 11.0 23.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(193., dtype=torch.float64) 62.0 10.0 8.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(418., dtype=torch.float64) 53.0 13.0 14.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(802., dtype=torch.float64) 52.0 17.0 11.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        # logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp1_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Associated with Exp2 - loss\n",
    "modified loss with fixed terminal loss (using clone)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(559., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(262., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(338., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(498., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(429., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(427., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(543., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(492., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(473., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(295., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(443., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp2_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp3\n",
    "using binary loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(441., dtype=torch.float64) 12.0 5.0 63.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(134., dtype=torch.float64) 1.0 8.0 71.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(375., dtype=torch.float64) 1.0 5.0 74.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(218., dtype=torch.float64) 1.0 8.0 71.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(299., dtype=torch.float64) 1.0 10.0 69.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(105., dtype=torch.float64) 0.0 8.0 72.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(129., dtype=torch.float64) 7.0 4.0 69.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([1., 0., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(413., dtype=torch.float64) 4.0 1.0 75.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(427., dtype=torch.float64) 1.0 1.0 78.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(330., dtype=torch.float64) 9.0 6.0 65.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(73., dtype=torch.float64) 0.0 6.0 74.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        # logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp3_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp4\n",
    "binary loss function with fixed penality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(429., dtype=torch.float64) 75.0 1.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(150., dtype=torch.float64) 75.0 1.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(171., dtype=torch.float64) 75.0 1.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(345., dtype=torch.float64) 74.0 1.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(177., dtype=torch.float64) 75.0 1.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(335., dtype=torch.float64) 73.0 1.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(375., dtype=torch.float64) 75.0 1.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(350., dtype=torch.float64) 76.0 1.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(316., dtype=torch.float64) 74.0 1.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(83., dtype=torch.float64) 73.0 1.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(321., dtype=torch.float64) 75.0 1.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp4_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 5 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(127., dtype=torch.float64) 5.0 71.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(28., dtype=torch.float64) 8.0 68.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(60., dtype=torch.float64) 7.0 69.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(210., dtype=torch.float64) 5.0 72.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(133., dtype=torch.float64) 7.0 68.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(256., dtype=torch.float64) 5.0 71.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(227., dtype=torch.float64) 5.0 71.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(155., dtype=torch.float64) 5.0 72.0 3.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(176., dtype=torch.float64) 4.0 71.0 5.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(105., dtype=torch.float64) 6.0 68.0 6.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(137., dtype=torch.float64) 6.0 70.0 4.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp5_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 6 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function + terminal loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(472., dtype=torch.float64) 10.0 4.0 66.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(129., dtype=torch.float64) 1.0 8.0 71.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(302., dtype=torch.float64) 1.0 12.0 67.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(430., dtype=torch.float64) 7.0 12.0 61.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 1., 0.])2-tensor([1., 0., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(399., dtype=torch.float64) 1.0 16.0 63.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(178., dtype=torch.float64) 7.0 18.0 55.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 1., 0.]) 1-tensor([0., 0., 1.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(302., dtype=torch.float64) 1.0 12.0 67.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(521., dtype=torch.float64) 7.0 24.0 49.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([1., 0., 0.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(525., dtype=torch.float64) 2.0 16.0 62.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(28., dtype=torch.float64) 0.0 6.0 74.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([0., 0., 1.]) 1-tensor([0., 0., 1.])2-tensor([0., 1., 0.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(640., dtype=torch.float64) 1.0 11.0 68.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        # logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp6_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp 7 - loss\n",
    "Changing the loss function to intake binary input and find exact loss value +  loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def testexp1_loss(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    test_item = open_file('./testData/prepareDS.pkl')\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "    model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, modelName)\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    neural_cut = []\n",
    "    for key, (dgl_graph, adjacency_matrix,graph, terminal) in test_item.items():\n",
    "\n",
    "        logits = net(dgl_graph, adjacency_matrix)\n",
    "        logits = override_fixed_nodes(logits)\n",
    "        binary_partitions = (logits >= 0.5).float()\n",
    "\n",
    "\n",
    "        cut = calculateAllCut(adjacency_matrix, binary_partitions)\n",
    "        # print(calculateAllCut)\n",
    "        totSum = np.sum(binary_partitions.numpy(), axis=0)\n",
    "\n",
    "        print(i, \": number :\", \"Neural 3-way min-cut value: \" + str(cut) + \" \"  +  str(totSum[0])+ \" \"  +  str(totSum[1])\n",
    "              + \" \"  +  str(totSum[2])+ \" Total Nodes:\" + str(np.sum(totSum)))\n",
    "\n",
    "        print(\"Terminalss: 0-\"+ str(binary_partitions[0])+\n",
    "              \" 1-\"+ str(binary_partitions[1])+\n",
    "              \"2-\"+ str(binary_partitions[2]))\n",
    "\n",
    "        invalidsCount = [m for m in binary_partitions if sum(m)!=1]\n",
    "        print(\"Invalids Nodes: \" + str( len(invalidsCount)))\n",
    "\n",
    "\n",
    "        print(\"---------- \\n\")\n",
    "        i+=1\n",
    "\n",
    "        if (i>10):\n",
    "            break\n",
    "testexp1_loss('./final__80wayCut_LossExp7_loss.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(559., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(262., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(338., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(498., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(429., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(427., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(543., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(492., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(473., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(295., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(443., dtype=torch.float64) 78.0 1.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "test2('./final__80wayCut_LossExp1.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : number : Neural 3-way min-cut value: tensor(257., dtype=torch.float64) 5.0 74.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "1 : number : Neural 3-way min-cut value: tensor(144., dtype=torch.float64) 10.0 69.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "2 : number : Neural 3-way min-cut value: tensor(227., dtype=torch.float64) 7.0 72.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "3 : number : Neural 3-way min-cut value: tensor(343., dtype=torch.float64) 5.0 74.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "4 : number : Neural 3-way min-cut value: tensor(367., dtype=torch.float64) 7.0 72.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "5 : number : Neural 3-way min-cut value: tensor(267., dtype=torch.float64) 5.0 74.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "6 : number : Neural 3-way min-cut value: tensor(366., dtype=torch.float64) 4.0 75.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "7 : number : Neural 3-way min-cut value: tensor(297., dtype=torch.float64) 5.0 74.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "8 : number : Neural 3-way min-cut value: tensor(392., dtype=torch.float64) 3.0 76.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "9 : number : Neural 3-way min-cut value: tensor(361., dtype=torch.float64) 6.0 73.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n",
      "10 : number : Neural 3-way min-cut value: tensor(259., dtype=torch.float64) 6.0 73.0 1.0 Total Nodes:80.0\n",
      "Terminalss: 0-tensor([1., 0., 0.]) 1-tensor([0., 1., 0.])2-tensor([0., 0., 1.])\n",
      "Invalids Nodes: 0\n",
      "---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "        h = override_fixed_nodes(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def override_fixed_nodes(h):\n",
    "    output = h.clone()\n",
    "    # Set the output for node 0 to [1, 0, 0]\n",
    "    output[0] = torch.tensor([1.0, 0.0, 0.0])\n",
    "    # Set the output for node 1 to [0, 1, 0]\n",
    "    output[1] = torch.tensor([0.0, 1.0, 0.0])\n",
    "    # Set the output for node 2 to [0, 0, 1]\n",
    "    output[2] = torch.tensor([0.0, 0.0, 1.0])\n",
    "    return output\n",
    "\n",
    "\n",
    "test2('./final__80wayCut_LossExp1.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "# def test2():\n",
    "#     n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=4096,patience=40)\n",
    "#\n",
    "#     # Establish pytorch GNN + optimizer\n",
    "#     opt_params = {'lr': learning_rate}\n",
    "#     gnn_hypers = {\n",
    "#         'dim_embedding': dim_embedding,\n",
    "#         'hidden_dim': hidden_dim,\n",
    "#         'dropout': 0.0,\n",
    "#         'number_classes': 2,\n",
    "#         'prob_threshold': PROB_THRESHOLD,\n",
    "#         'number_epochs': number_epochs,\n",
    "#         'tolerance': tol,\n",
    "#         'patience': patience,\n",
    "#         'nodes':n\n",
    "#     }\n",
    "#     test_item = LoadData(fileName='./testData/dummy.pkl')\n",
    "#     net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "#     # model = net\n",
    "#     model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, '/Users/javaad/Documents/research/COP/models/2Way/small/exp_2way_epoch0_loss4895329096.pth')\n",
    "#     model.eval()\n",
    "#\n",
    "#     neural_cut1 = []\n",
    "#     for key, (dgl_graph, adjacency_matrix,graph) in test_item.items():\n",
    "#         embed = nn.Embedding(graph.number_of_nodes(), dim_embedding)\n",
    "#         embed = embed.type(TORCH_DTYPE).to(TORCH_DEVICE)\n",
    "#         inputs = embed.weight\n",
    "#         # inputs = torch.ones((dgl_graph.number_of_nodes(), dim_embedding))\n",
    "#         neural_cut1.append(GetOptimalNetValue(model,dgl_graph, inputs, adjacency_matrix, {0:0, 5:1}))\n",
    "#\n",
    "#         print(\"Neural 2-way min-cut value: \" + str(neural_cut1[-1].item()))\n",
    "#     return neural_cut"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural 2-way min-cut value: 100.0\n",
      "Neural 2-way min-cut value: 100.0\n"
     ]
    }
   ],
   "source": [
    "# neural_cut = test2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# heurestic_cut_k = LoadData('./testData/heurestic_cut_k.pkl')\n",
    "# #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# from matplotlib.ticker import ScalarFormatter\n",
    "# def barPlot_2(heurestic_cut, neural_cut):\n",
    "#     # Example data\n",
    "#     n_groups = len(heurestic_cut)\n",
    "#     index = np.arange(n_groups)\n",
    "#     bar_width = 0.35\n",
    "#\n",
    "#     # Create bars\n",
    "#     plt.figure(figsize=(30, 6))\n",
    "#     bar1 = plt.bar(index, heurestic_cut, bar_width, label='Heurestic')\n",
    "#     bar2 = plt.bar(index + bar_width, neural_cut, bar_width, label='Neural Network')\n",
    "#\n",
    "#     # Add details\n",
    "#     plt.xlabel('Graph Number')\n",
    "#     plt.ylabel('Minimum Cut Value')\n",
    "#     plt.title('Comparison of Minimum Cut Values by Algorithm')\n",
    "#     # plt.xticks(index + bar_width / 2, range(1, n_groups + 1))\n",
    "#     plt.legend()\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.gca().yaxis.set(major_formatter=ScalarFormatter(), minor_formatter=ScalarFormatter());\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 3000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "barPlot_2( heurestic_cut_k, neural_cut)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}