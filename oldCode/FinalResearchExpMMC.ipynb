{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use device: cpu, torch dtype: torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from GraphCreator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def gen_adj_matrix(nx_G):\n",
    "    adj_dict = defaultdict(int)\n",
    "\n",
    "    for(u,v) in nx_G.edges:\n",
    "        adj_dict[(u, v)] = nx_G[u][v]['weight']\n",
    "        adj_dict[(v, u)] = nx_G[u][v]['weight']\n",
    "\n",
    "    for u in nx_G.nodes:\n",
    "        for i in nx_G.nodes:\n",
    "            if not adj_dict[(u, i)]:\n",
    "                adj_dict[(u, i)] = 0\n",
    "\n",
    "    return adj_dict\n",
    "\n",
    "def DrawGraph(graph):\n",
    "    pos = nx.spring_layout(graph,seed=1)\n",
    "\n",
    "    # Visualize graph\n",
    "    options = {\n",
    "        \"font_size\": 16,\n",
    "        \"node_size\": 800,\n",
    "        \"node_color\": \"white\",\n",
    "        \"edgecolors\": \"black\",\n",
    "        \"linewidths\": 5,\n",
    "        \"width\": 5,\n",
    "    }\n",
    "    nx.draw(graph, pos, with_labels=True, **options)\n",
    "\n",
    "    labels = nx.get_edge_attributes(graph,'weight')\n",
    "    nx.draw_networkx_edge_labels(graph,pos,edge_labels=labels)\n",
    "\n",
    "def CreateDummyFunction(edges):\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(edges)\n",
    "    test_graph.order()\n",
    "    return test_graph\n",
    "\n",
    "def qubo_dict_to_torch(nx_G, Q, torch_dtype=None, torch_device=None):\n",
    "    \"\"\"\n",
    "    Output Q matrix as torch tensor for given Q in dictionary format.\n",
    "\n",
    "    Input:\n",
    "        Q: QUBO matrix as defaultdict\n",
    "        nx_G: graph as networkx object (needed for node lables can vary 0,1,... vs 1,2,... vs a,b,...)\n",
    "    Output:\n",
    "        Q: QUBO as torch tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # get number of nodes\n",
    "    n_nodes = len(nx_G.nodes)\n",
    "\n",
    "    # get QUBO Q as torch tensor\n",
    "    Q_mat = torch.zeros(n_nodes, n_nodes)\n",
    "    for (x_coord, y_coord), val in Q.items():\n",
    "        Q_mat[x_coord][y_coord] = val\n",
    "\n",
    "    if torch_dtype is not None:\n",
    "        Q_mat = Q_mat.type(torch_dtype)\n",
    "\n",
    "    if torch_device is not None:\n",
    "        Q_mat = Q_mat.to(torch_device)\n",
    "\n",
    "    return Q_mat\n",
    "\n",
    "def calculateMinCut(adj_matrix, output, terminal1 = 0, terminal2 = 4):\n",
    "    #output = (output.detach() >= 0.5) * 1\n",
    "\n",
    "    # if output[terminal1] == output[terminal2]:\n",
    "    #     return float(\"inf\")\n",
    "    #print(adj_matrix, output)\n",
    "    loss = 0\n",
    "    for i in range(len(adj_matrix)):\n",
    "        for j in range(len(adj_matrix)):\n",
    "            if (output[i] > 0.5 and output[j] < 0.5) or (output[i] < 0.5 and output[j] > 0.5) :\n",
    "                loss+=adj_matrix[i][j]\n",
    "\n",
    "    return loss\n",
    "\n",
    "def partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the partition of each edge (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.not_equal.outer(s, s).astype(int)\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "    return weight\n",
    "\n",
    "def expected_partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the expected sum of weights of edges that are in different partitions,\n",
    "    based on the probabilities in s.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the probability of each edge being in a certain partition.\n",
    "    :return: Expected sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.outer(s, 1 - s) + np.outer(1 - s, s)\n",
    "    expected_weight = (adj * partition_matrix).sum() / 2\n",
    "    return expected_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hamiltonian Partitioning\n",
    "We are going to experiment with hamilotnian paritioning on couple of graphs and adjacent matrix to see how well it works"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def Hamiltonian(s, w, A = 1, B = 1):\n",
    "    '''\n",
    "\n",
    "    :param s: In original H paritioning, s is either 1 or -1\n",
    "    :param w: The adjacent matrix of the graph\n",
    "    :param A: a hyper parameter\n",
    "    :param B: another hyper parameter\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Calculate HA\n",
    "    #HA = ((s.sum() ** 2) if len(s) % 2 == 0 else ((s.sum() ** 2) / 2))\n",
    "    HA = ((np.sum(s)** 2) if len(s) % 2 == 0 else ((np.sum(s) ** 2) / 2))\n",
    "\n",
    "    # Calculate HB\n",
    "    # HB = np.sum(w * (1 - np.outer(s, s)) / 2)\n",
    "\n",
    "    HB = (w * (1 - np.outer(s, s)) / 2).sum()\n",
    "    # Hamiltonian H\n",
    "    H = A * HA + B * HB\n",
    "\n",
    "    return H\n",
    "\n",
    "def HamiltonianLoss(s, w, A = 1, B = 1):\n",
    "    '''\n",
    "\n",
    "    :param s: In original H paritioning, s is either 1 or -1\n",
    "    :param w: The adjacent matrix of the graph\n",
    "    :param A: a hyper parameter\n",
    "    :param B: another hyper parameter\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Calculate HA\n",
    "    HA = ((s.sum() ** 2) if len(s) % 2 == 0 else ((s.sum() ** 2) / 2))\n",
    "\n",
    "    # Calculate HB\n",
    "    # HB = np.sum(w * (1 - np.outer(s, s)) / 2)\n",
    "\n",
    "    HB = (w * (1 - torch.outer(s, s)) / 2).sum()\n",
    "    # Hamiltonian H\n",
    "    H = A * HA + B * HB\n",
    "\n",
    "    return H\n",
    "\n",
    "def HamiltonianLoss2(s, w, A = 9, B = 1):\n",
    "    '''\n",
    "\n",
    "    :param s: In original H paritioning, s is either 1 or -1\n",
    "    :param w: The adjacent matrix of the graph\n",
    "    :param A: a hyper parameter\n",
    "    :param B: another hyper parameter\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Calculate HA\n",
    "    num_nodes = s.size(0)\n",
    "    ideal_balance = num_nodes / 2.0\n",
    "\n",
    "    # HA is a penalty term for the deviation of sum of probabilities from the ideal balance\n",
    "    HA = (torch.sum(s) - ideal_balance) ** 2\n",
    "\n",
    "    # Calculate HB\n",
    "    # HB = np.sum(w * (1 - np.outer(s, s)) / 2)\n",
    "\n",
    "    HB = (w * (1 - torch.outer(s, s)) / 2).sum()\n",
    "    # Hamiltonian H\n",
    "    H = A * HA + B * HB\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def HamiltonianLoss3(s, w, A = 1, B = 1):\n",
    "    '''\n",
    "\n",
    "    :param s: In original H paritioning, s is either 1 or -1\n",
    "    :param w: The adjacent matrix of the graph\n",
    "    :param A: a hyper parameter\n",
    "    :param B: another hyper parameter\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Calculate HA\n",
    "    num_nodes = s.size(0)\n",
    "    ideal_balance = num_nodes / 2.0\n",
    "\n",
    "    # HA is a penalty term for the deviation of sum of probabilities from the ideal balance\n",
    "    HA = (torch.sum(s) - ideal_balance) ** 2\n",
    "\n",
    "    HB = expected_partition_weight2(w, s)\n",
    "    # Hamiltonian H\n",
    "    H = A * HA + B * HB\n",
    "\n",
    "    return H\n",
    "\n",
    "def expected_partition_weight2(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the expected sum of weights of edges that are in different partitions,\n",
    "    based on the probabilities in s.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the probability of each edge being in a certain partition.\n",
    "    :return: Expected sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    #s = torch.array(s)\n",
    "    partition_matrix = torch.outer(s, 1 - s) + torch.outer(1 - s, s)\n",
    "    expected_weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    # # Penalty term\n",
    "    # edge_probs = s[list((0,4))]\n",
    "    # penalty = 10 * torch.prod(edge_probs) + 10 * torch.prod(1 - edge_probs)\n",
    "    # expected_weight += penalty\n",
    "    return expected_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generic Partition formula\n",
    "\n",
    "Formula for generic partition\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(0, 0.0, 6)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate HA\n",
    "def calculate_HA(s):\n",
    "    # HA = ∑v∈V(∑k=1K(sv,k−1))^2\n",
    "    HA = sum((s[v].sum() - 1) ** 2 for v in range(len(s)))\n",
    "    return HA\n",
    "\n",
    "# Function to calculate HB\n",
    "def calculate_HB(s, V):\n",
    "    # HB = ∑k=1K(∑v∈V(sv,k−|V|/K))^2\n",
    "    K = len(s[0])\n",
    "    HB = sum((s[:, k].sum() - V / K) ** 2 for k in range(K))\n",
    "    return HB\n",
    "\n",
    "# Function to calculate HC\n",
    "def calculate_HC(s, graph):\n",
    "    # HC = ∑(u,v)∈E∑k=1K(1−su,k∗sv,k)\n",
    "    K = len(s[0])\n",
    "    HC = sum(graph[u, v] * (1 - sum(s[u, k] * s[v, k] for k in range(K)))\n",
    "             for u in range(len(graph)) for v in range(len(graph)) if u != v)\n",
    "    return HC\n",
    "\n",
    "# Example input\n",
    "s = np.array([[0, 1], [1, 0], [1, 0], [0, 1]])  # partition matrix\n",
    "graph = np.array([[0, 1, 1, 0],  # adjacency matrix\n",
    "                  [1, 0, 0, 1],\n",
    "                  [1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0]])\n",
    "\n",
    "# Number of vertices\n",
    "V = s.shape[0]\n",
    "\n",
    "# Calculate HA, HB, HC\n",
    "HA = calculate_HA(s)\n",
    "HB = calculate_HB(s, V)\n",
    "HC = calculate_HC(s, graph)\n",
    "\n",
    "HA, HB, HC\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorized form for partition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# Vectorized form of the calculations for HA, HB, and HC\n",
    "\n",
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "\n",
    "def calculate_HB_vectorized(s, V, K):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HB.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param V: The number of vertices.\n",
    "    :param K: The number of partitions.\n",
    "    :return: The HB value.\n",
    "    \"\"\"\n",
    "    # HB = ∑k∈K(∑v∈V(sv,k)−|V|/K)^2\n",
    "    average_partition_size = V / K\n",
    "    HB = torch.sum((torch.sum(s, axis=0) - average_partition_size) ** 2)\n",
    "    return HB\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # HC = ∑(u,v)∈E(1−∑k∈K(su,k*sv,k))*adjacency_matrix[u,v]\n",
    "    K = s.shape[1]\n",
    "    # Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\n",
    "    prod = adjacency_matrix * (1 - s @ s.T)\n",
    "    HC = torch.sum(prod)\n",
    "    return HC\n",
    "\n",
    "def calculate_H(s, adjacency_matrix, A = 12, B = 1, C = 1):\n",
    "    V = s.shape[0]  # Total vertices\n",
    "    K = s.shape[1]  # Total partitions\n",
    "    HA_vectorized = calculate_HA_vectorized(s)\n",
    "    HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "    HC_vectorized = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "\n",
    "    return A*HA_vectorized + B*HB_vectorized + C*HC_vectorized\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m HA_vectorized \u001B[38;5;241m=\u001B[39m calculate_HA_vectorized(s)\n\u001B[1;32m     10\u001B[0m HB_vectorized \u001B[38;5;241m=\u001B[39m calculate_HB_vectorized(s, V, K)\n\u001B[0;32m---> 11\u001B[0m HC_vectorized \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_HC_vectorized\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madjacency_matrix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m HA_vectorized, HB_vectorized, HC_vectorized\n",
      "Cell \u001B[0;32mIn[6], line 37\u001B[0m, in \u001B[0;36mcalculate_HC_vectorized\u001B[0;34m(s, adjacency_matrix)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\u001B[39;00m\n\u001B[1;32m     36\u001B[0m prod \u001B[38;5;241m=\u001B[39m adjacency_matrix \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m s \u001B[38;5;241m@\u001B[39m s\u001B[38;5;241m.\u001B[39mT)\n\u001B[0;32m---> 37\u001B[0m HC \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprod\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m HC\n",
      "\u001B[0;31mTypeError\u001B[0m: sum(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "s = np.array([[0, 1], [1, 0], [1, 0], [0, 1]])  # partition matrix\n",
    "adjacency_matrix = np.array([[0, 1, 1, 0],  # adjacency matrix\n",
    "                             [1, 0, 0, 1],\n",
    "                             [1, 0, 0, 0],\n",
    "                             [0, 1, 0, 0]])\n",
    "# Example usage with the same s and adjacency_matrix as before\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "\n",
    "HA_vectorized, HB_vectorized, HC_vectorized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph 1 experiments\n",
    "## Experiment with H partitiioning\n",
    "\n",
    "We are going to run H paritioning on some graphs with different s values. Lets talk our first graph:\n",
    "\n",
    "Vertex 0 --(1)-- Vertex 1\n",
    "  |     \\\n",
    "  |       \\\n",
    " (3)      (2)\n",
    "  |          \\\n",
    "  |            \\\n",
    "Vertex 2 --(1)-- Vertex 3\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edges = [(0,1, {\"weight\": 10, \"capacity\":10}),\n",
    "         (1,2, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (3,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (2,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (5,6, {\"weight\": 10, \"capacity\":10}),]\n",
    "graph = CreateDummyFunction(edges)\n",
    "graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "DrawGraph(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "q_torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(0.6667) tensor(4.) tensor(4.6667) tensor(4.) tensor(4.6667)\n",
      "tensor(1) tensor(1.0000) tensor(2.) tensor(4.) tensor(4.) tensor(23.)\n",
      "tensor(0) tensor(4.6667) tensor(24.) tensor(28.6667) tensor(24.) tensor(28.6667)\n",
      "tensor(28) tensor(65.3333) tensor(-132.) tensor(-38.6667) tensor(-76.) tensor(493.3334)\n",
      "tensor(3) tensor(3.6667) tensor(-18.) tensor(-11.3333) tensor(-12.) tensor(45.6667)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = torch.from_numpy(np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0,0, 1]]) ) # partition matrix\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_H(s, q_torch))\n",
    "\n",
    "s = torch.from_numpy(np.array([[1, 0, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0,0, 1]]))  # partition matrix\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_H(s, q_torch))\n",
    "\n",
    "s = torch.from_numpy(np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0,1, 0]]))  # partition matrix\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_H(s, q_torch))\n",
    "\n",
    "s = torch.from_numpy(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1,1, 1]]) ) # partition matrix)\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_H(s, q_torch))\n",
    "\n",
    "\n",
    "\n",
    "s = torch.from_numpy(np.array([[0, 0, 1],\n",
    "                               [0, 0, 1],\n",
    "                               [1, 1, 0],\n",
    "                               [0, 0, 1],\n",
    "                               [0, 0, 1],\n",
    "                               [1, 1, 0],\n",
    "                               [1, 1, 0]]) ) # partition matrix)\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_H(s, q_torch))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DrawGraph(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using k-parition formula, adding regularization to it\n",
    "\n",
    "We want to add regularization to ensure a edge goes closer to 0 or 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_regularization_term(s, gamma):\n",
    "    return gamma * torch.sum(s * (1 - s))\n",
    "\n",
    "def calculate_H_prime(s, adjacency_matrix,  gamma = 1, A = 20, B = 1, C = 1):\n",
    "    V = s.shape[0]  # Total vertices\n",
    "    K = s.shape[1]  # Total partitions\n",
    "    HA = calculate_HA_vectorized(s)\n",
    "    HB = calculate_HB_vectorized(s, V, K)\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    R = calculate_regularization_term(s, gamma)\n",
    "    return A*HA + B*HB + C*HC + R"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(0.6667) tensor(4.) tensor(4.6667) tensor(4.) tensor(0) tensor(4.6667) tensor(4.6667)\n",
      "tensor(1) tensor(1.0000) tensor(2.) tensor(4.) tensor(4.) tensor(0) tensor(23.) tensor(23.)\n",
      "tensor(0) tensor(4.6667) tensor(24.) tensor(28.6667) tensor(24.) tensor(0) tensor(28.6667) tensor(28.6667)\n",
      "tensor(28) tensor(65.3333) tensor(-132.) tensor(-38.6667) tensor(-76.) tensor(0) tensor(493.3334) tensor(493.3334)\n",
      "tensor(3) tensor(3.6667) tensor(-18.) tensor(-11.3333) tensor(-12.) tensor(0) tensor(45.6667) tensor(45.6667)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = torch.from_numpy(np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0,0, 1]]) ) # partition matrix\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "calculate_regularization_term(s, gamma = 1)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_regularization_term(s, gamma = 1), calculate_H(s, q_torch), calculate_H_prime(s, q_torch))\n",
    "\n",
    "s = torch.from_numpy(np.array([[1, 0, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0,0, 1]]))  # partition matrix\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "calculate_regularization_term(s, gamma = 1)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_regularization_term(s, gamma = 1), calculate_H(s, q_torch), calculate_H_prime(s, q_torch))\n",
    "\n",
    "s = torch.from_numpy(np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0,1, 0]]))  # partition matrix\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "calculate_regularization_term(s, gamma = 1)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_regularization_term(s, gamma = 1), calculate_H(s, q_torch), calculate_H_prime(s, q_torch))\n",
    "\n",
    "s = torch.from_numpy(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1,1, 1]]) ) # partition matrix)\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "calculate_regularization_term(s, gamma = 1)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_regularization_term(s, gamma = 1), calculate_H(s, q_torch), calculate_H_prime(s, q_torch))\n",
    "\n",
    "\n",
    "\n",
    "s = torch.from_numpy(np.array([[0, 0, 1],\n",
    "                               [0, 0, 1],\n",
    "                               [1, 1, 0],\n",
    "                               [0, 0, 1],\n",
    "                               [0, 0, 1],\n",
    "                               [1, 1, 0],\n",
    "                               [1, 1, 0]]) ) # partition matrix)\n",
    "V = s.shape[0]  # Total vertices\n",
    "K = s.shape[1]  # Total partitions\n",
    "\n",
    "HA_vectorized = calculate_HA_vectorized(s)\n",
    "HB_vectorized = calculate_HB_vectorized(s, V, K)\n",
    "HC_vectorized = calculate_HC_vectorized(s, q_torch)\n",
    "calculate_regularization_term(s, gamma = 1)\n",
    "\n",
    "print(HA_vectorized, HB_vectorized, HC_vectorized, HA_vectorized+ HB_vectorized + HC_vectorized, 2*HA_vectorized+  HC_vectorized, calculate_regularization_term(s, gamma = 1), calculate_H(s, q_torch), calculate_H_prime(s, q_torch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classic 3-way multicut algorithm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "Invalid key \"0\". Must be one of the edge types.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mDGLError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 36\u001B[0m\n\u001B[1;32m     33\u001B[0m B \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m     34\u001B[0m C \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m---> 36\u001B[0m cutset \u001B[38;5;241m=\u001B[39m \u001B[43mthree_way_min_cut\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph_dgl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mC\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3-way min cut edges:\u001B[39m\u001B[38;5;124m\"\u001B[39m, cutset)\n",
      "Cell \u001B[0;32mIn[37], line 3\u001B[0m, in \u001B[0;36mthree_way_min_cut\u001B[0;34m(G, A, B, C)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mthree_way_min_cut\u001B[39m(G, A, B, C):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Step 1: Find min cut between A and B\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     cut_value_AB, partition_AB \u001B[38;5;241m=\u001B[39m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimum_cut\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     reachable, non_reachable \u001B[38;5;241m=\u001B[39m partition_AB\n\u001B[1;32m      5\u001B[0m     cutset_AB \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39medge_boundary(G, reachable, non_reachable)\n",
      "File \u001B[0;32m~/Documents/research/COP/lib/python3.10/site-packages/networkx/algorithms/flow/maxflow.py:450\u001B[0m, in \u001B[0;36mminimum_cut\u001B[0;34m(flowG, _s, _t, capacity, flow_func, **kwargs)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcutoff\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m flow_func \u001B[38;5;129;01mis\u001B[39;00m preflow_push:\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m nx\u001B[38;5;241m.\u001B[39mNetworkXError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcutoff should not be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 450\u001B[0m R \u001B[38;5;241m=\u001B[39m \u001B[43mflow_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflowG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_s\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcapacity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapacity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;66;03m# Remove saturated edges from the residual network\u001B[39;00m\n\u001B[1;32m    452\u001B[0m cutset \u001B[38;5;241m=\u001B[39m [(u, v, d) \u001B[38;5;28;01mfor\u001B[39;00m u, v, d \u001B[38;5;129;01min\u001B[39;00m R\u001B[38;5;241m.\u001B[39medges(data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflow\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapacity\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n",
      "File \u001B[0;32m~/Documents/research/COP/lib/python3.10/site-packages/networkx/algorithms/flow/preflowpush.py:421\u001B[0m, in \u001B[0;36mpreflow_push\u001B[0;34m(G, s, t, capacity, residual, global_relabel_freq, value_only)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreflow_push\u001B[39m(\n\u001B[1;32m    292\u001B[0m     G, s, t, capacity\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapacity\u001B[39m\u001B[38;5;124m\"\u001B[39m, residual\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, global_relabel_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, value_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    293\u001B[0m ):\n\u001B[1;32m    294\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Find a maximum single-commodity flow using the highest-label\u001B[39;00m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;124;03m    preflow-push algorithm.\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    419\u001B[0m \n\u001B[1;32m    420\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 421\u001B[0m     R \u001B[38;5;241m=\u001B[39m \u001B[43mpreflow_push_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcapacity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresidual\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_relabel_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    422\u001B[0m     R\u001B[38;5;241m.\u001B[39mgraph[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malgorithm\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpreflow_push\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m R\n",
      "File \u001B[0;32m~/Documents/research/COP/lib/python3.10/site-packages/networkx/algorithms/flow/preflowpush.py:24\u001B[0m, in \u001B[0;36mpreflow_push_impl\u001B[0;34m(G, s, t, capacity, residual, global_relabel_freq, value_only)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreflow_push_impl\u001B[39m(G, s, t, capacity, residual, global_relabel_freq, value_only):\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implementation of the highest-label preflow-push algorithm.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mG\u001B[49m:\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m nx\u001B[38;5;241m.\u001B[39mNetworkXError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(s)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in graph\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m t \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m G:\n",
      "File \u001B[0;32m~/Documents/research/COP/lib/python3.10/site-packages/dgl/heterograph.py:2409\u001B[0m, in \u001B[0;36mDGLGraph.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   2406\u001B[0m etypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_find_etypes(key)\n\u001B[1;32m   2408\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(etypes) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2409\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DGLError(\n\u001B[1;32m   2410\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInvalid key \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m. Must be one of the edge types.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2411\u001B[0m             orig_key\n\u001B[1;32m   2412\u001B[0m         )\n\u001B[1;32m   2413\u001B[0m     )\n\u001B[1;32m   2415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(etypes) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2416\u001B[0m     \u001B[38;5;66;03m# no ambiguity: return the unitgraph itself\u001B[39;00m\n\u001B[1;32m   2417\u001B[0m     srctype, etype, dsttype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_canonical_etypes[etypes[\u001B[38;5;241m0\u001B[39m]]\n",
      "\u001B[0;31mDGLError\u001B[0m: Invalid key \"0\". Must be one of the edge types."
     ]
    }
   ],
   "source": [
    "\n",
    "def three_way_min_cut(G, A, B, C):\n",
    "    # Step 1: Find min cut between A and B\n",
    "    cut_value_AB, partition_AB = nx.minimum_cut(G, A, B)\n",
    "    reachable, non_reachable = partition_AB\n",
    "    cutset_AB = nx.edge_boundary(G, reachable, non_reachable)\n",
    "\n",
    "    # Step 2: Merge A and B into a super-node and find min cut with C\n",
    "    # First, create a copy of the graph to modify\n",
    "    G_copy = G.copy()\n",
    "    # Merge A and B into a super-node (by contracting A into B)\n",
    "    G_copy = nx.contracted_nodes(G_copy, B, A, self_loops=False)\n",
    "\n",
    "    # Step 3: Find min cut between super-node (B) and C\n",
    "    cut_value_BC, partition_BC = nx.minimum_cut(G_copy, B, C)\n",
    "    reachable, non_reachable = partition_BC\n",
    "    cutset_BC = nx.edge_boundary(G_copy, reachable, non_reachable)\n",
    "\n",
    "    # Translate the cutset back to the original nodes, if necessary\n",
    "    cutset_BC_translated = set()\n",
    "    for u, v in cutset_BC:\n",
    "        if u == B:\n",
    "            u = A\n",
    "        if v == B:\n",
    "            v = A\n",
    "        cutset_BC_translated.add((u, v))\n",
    "\n",
    "    # Combine the two cutsets\n",
    "    total_cutset = cutset_AB.union(cutset_BC_translated)\n",
    "\n",
    "    return total_cutset\n",
    "\n",
    "A = (1, 3)\n",
    "B = (1, 5)\n",
    "C = (3, 5)\n",
    "\n",
    "cutset = three_way_min_cut(graph_dgl,  A, B, C)\n",
    "print(\"3-way min cut edges:\", cutset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Code\n",
    "\n",
    "Neural Network Architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class GCN_dev(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, number_classes, dropout, device):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of the core GCN model of provided size.\n",
    "        Dropout is added in forward step.\n",
    "\n",
    "        Inputs:\n",
    "            in_feats: Dimension of the input (embedding) layer\n",
    "            hidden_size: Hidden layer size\n",
    "            dropout: Fraction of dropout to add between intermediate layer. Value is cached for later use.\n",
    "            device: Specifies device (CPU vs GPU) to load variables onto\n",
    "        \"\"\"\n",
    "        super(GCN_dev, self).__init__()\n",
    "\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, number_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        \"\"\"\n",
    "        Run forward propagation step of instantiated model.\n",
    "\n",
    "        Input:\n",
    "            self: GCN_dev instance\n",
    "            g: DGL graph object, i.e. problem definition\n",
    "            inputs: Input (embedding) layer weights, to be propagated through network\n",
    "        Output:\n",
    "            h: Output layer weights\n",
    "        \"\"\"\n",
    "\n",
    "        # input step\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac)\n",
    "\n",
    "        # output step\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.sigmoid(h)\n",
    "\n",
    "\n",
    "\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Construct graph to learn on\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCN_dev(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def run_gnn_training(q_torch, dgl_graph, net, embed, optimizer, number_epochs, tol, patience, prob_threshold, loss_func):\n",
    "    \"\"\"\n",
    "    Wrapper function to run and monitor GNN training. Includes early stopping.\n",
    "    \"\"\"\n",
    "    # Assign variable for user reference\n",
    "    inputs = embed.weight\n",
    "\n",
    "    prev_loss = 1.  # initial loss value (arbitrary)\n",
    "    count = 0       # track number times early stopping is triggered\n",
    "\n",
    "    # initialize optimal solution\n",
    "    #best_bitstring = torch.zeros((dgl_graph.number_of_nodes(),)).type(q_torch.dtype).to(q_torch.device)\n",
    "    best_bitstring = torch.zeros(7,3).type(q_torch.dtype).to(q_torch.device)\n",
    "    best_loss = loss_func(best_bitstring.float(), q_torch)\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # Training logic\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        # get logits/activations\n",
    "        probs = net(dgl_graph, inputs)  # collapse extra dimension output from model\n",
    "\n",
    "        # build cost value with QUBO cost function\n",
    "        loss = loss_func(probs, q_torch)\n",
    "        loss_ = loss.detach().item()\n",
    "\n",
    "        # Apply projection\n",
    "        bitstring = (probs.detach() >= prob_threshold) * 1\n",
    "        #calc = partition_weight(q_torch, bitstring)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_bitstring = bitstring\n",
    "\n",
    "        # if calc < best_loss and calc != 0:\n",
    "        #     best_loss = loss\n",
    "        #     best_bitstring = bitstring\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss_}')\n",
    "            print(probs, q_torch)\n",
    "\n",
    "        # early stopping check\n",
    "        # If loss increases or change in loss is too small, trigger\n",
    "        if (abs(loss_ - prev_loss) <= tol) | ((loss_ - prev_loss) > 0):\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        if count >= patience:\n",
    "            print(f'Stopping early on epoch {epoch} (patience: {patience})')\n",
    "            break\n",
    "\n",
    "        # update loss tracking\n",
    "        prev_loss = loss_\n",
    "\n",
    "        # run optimization with backpropagation\n",
    "        optimizer.zero_grad()  # clear gradient for step\n",
    "        loss.backward()        # calculate gradient through compute graph\n",
    "        optimizer.step()       # take step, update weights\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "    print(f'GNN training (n={dgl_graph.number_of_nodes()}) took {round(t_gnn, 3)}')\n",
    "    print(f'GNN final continuous loss: {loss_}')\n",
    "    print(f'GNN best continuous loss: {best_loss}')\n",
    "\n",
    "    final_bitstring = (probs.detach() >= prob_threshold) * 1\n",
    "\n",
    "    return net, epoch, final_bitstring, best_bitstring\n",
    "def run_gnn_training2(q_torch, dgl_graph, net, embed, optimizer, number_epochs, tol, patience, prob_threshold, loss_func):\n",
    "    \"\"\"\n",
    "    Wrapper function to run and monitor GNN training. Includes early stopping.\n",
    "    \"\"\"\n",
    "    # Assign variable for user reference\n",
    "    inputs = embed.weight\n",
    "\n",
    "    prev_loss = 1.  # initial loss value (arbitrary)\n",
    "    count = 0       # track number times early stopping is triggered\n",
    "\n",
    "    # initialize optimal solution\n",
    "    best_bitstring = torch.zeros((dgl_graph.number_of_nodes(),)).type(q_torch.dtype).to(q_torch.device)\n",
    "    best_loss = loss_func(best_bitstring.float(), q_torch)\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # Training logic\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        # get logits/activations\n",
    "        probs = net(dgl_graph, inputs)[:, 0]  # collapse extra dimension output from model\n",
    "\n",
    "        # build cost value with QUBO cost function\n",
    "        loss = loss_func(probs, q_torch)\n",
    "        loss_ = loss.detach().item()\n",
    "\n",
    "        # Apply projection\n",
    "        bitstring = (probs.detach() >= prob_threshold) * 1\n",
    "        calc = partition_weight(q_torch, bitstring)\n",
    "        # if loss < best_loss:\n",
    "        #     best_loss = loss\n",
    "        #     best_bitstring = bitstring\n",
    "        #add reward\n",
    "        loss = loss-calc\n",
    "\n",
    "        if calc < best_loss and calc != 0:\n",
    "            best_loss = loss\n",
    "            best_bitstring = bitstring\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss_}')\n",
    "            print(probs, q_torch)\n",
    "\n",
    "        # early stopping check\n",
    "        # If loss increases or change in loss is too small, trigger\n",
    "        if (abs(loss_ - prev_loss) <= tol) | ((loss_ - prev_loss) > 0):\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        if count >= patience:\n",
    "            print(f'Stopping early on epoch {epoch} (patience: {patience})')\n",
    "            break\n",
    "\n",
    "        # update loss tracking\n",
    "        prev_loss = loss_\n",
    "\n",
    "        # run optimization with backpropagation\n",
    "        optimizer.zero_grad()  # clear gradient for step\n",
    "        loss.backward()        # calculate gradient through compute graph\n",
    "        optimizer.step()       # take step, update weights\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "    print(f'GNN training (n={dgl_graph.number_of_nodes()}) took {round(t_gnn, 3)}')\n",
    "    print(f'GNN final continuous loss: {loss_}')\n",
    "    print(f'GNN best continuous loss: {best_loss}')\n",
    "\n",
    "    final_bitstring = (probs.detach() >= prob_threshold) * 1\n",
    "\n",
    "    return net, epoch, final_bitstring, best_bitstring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = int(np.sqrt(n))    # e.g. 10\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training GCN on graph 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "edges = [(0,1, {\"weight\": 10, \"capacity\":10}),\n",
    "         (1,2, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (3,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (2,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (5,6, {\"weight\": 10, \"capacity\":10}),]\n",
    "graph = CreateDummyFunction(edges)\n",
    "graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "\n",
    "n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=7,patience=10000)\n",
    "# Establish pytorch GNN + optimizer\n",
    "opt_params = {'lr': learning_rate}\n",
    "gnn_hypers = {\n",
    "    'dim_embedding': dim_embedding,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'dropout': 0.0,\n",
    "    'number_classes': 3,\n",
    "    'prob_threshold': PROB_THRESHOLD,\n",
    "    'number_epochs': number_epochs,\n",
    "    'tolerance': tol,\n",
    "    'patience': patience\n",
    "}\n",
    "\n",
    "net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "# For tracking hyperparameters in results object\n",
    "gnn_hypers.update(opt_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GNN...\n",
      "Epoch: 0, Loss: 42.83835983276367\n",
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5219, 0.4701, 0.5370],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5379, 0.4483, 0.5638],\n",
      "        [0.5910, 0.3771, 0.6505],\n",
      "        [0.5000, 0.5000, 0.5000]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 1000, Loss: 39.86567687988281\n",
      "tensor([[0.4772, 0.4772, 0.4772],\n",
      "        [0.4772, 0.4772, 0.4772],\n",
      "        [0.4810, 0.4710, 0.4839],\n",
      "        [0.4772, 0.4772, 0.4772],\n",
      "        [0.4838, 0.4664, 0.4888],\n",
      "        [0.5347, 0.3839, 0.5785],\n",
      "        [0.4772, 0.4772, 0.4772]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 2000, Loss: 38.58766555786133\n",
      "tensor([[0.4585, 0.4585, 0.4584],\n",
      "        [0.4585, 0.4585, 0.4584],\n",
      "        [0.4585, 0.4585, 0.4584],\n",
      "        [0.4585, 0.4585, 0.4584],\n",
      "        [0.4585, 0.4585, 0.4584],\n",
      "        [0.5008, 0.3833, 0.5354],\n",
      "        [0.4585, 0.4585, 0.4584]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 3000, Loss: 38.02777862548828\n",
      "tensor([[0.4437, 0.4433, 0.4436],\n",
      "        [0.4437, 0.4433, 0.4436],\n",
      "        [0.4437, 0.4433, 0.4436],\n",
      "        [0.4437, 0.4433, 0.4436],\n",
      "        [0.4437, 0.4433, 0.4436],\n",
      "        [0.4776, 0.3796, 0.5066],\n",
      "        [0.4437, 0.4433, 0.4436]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 4000, Loss: 37.83669662475586\n",
      "tensor([[0.4334, 0.4323, 0.4335],\n",
      "        [0.4334, 0.4323, 0.4335],\n",
      "        [0.4334, 0.4323, 0.4335],\n",
      "        [0.4334, 0.4323, 0.4335],\n",
      "        [0.4334, 0.4323, 0.4335],\n",
      "        [0.4637, 0.3751, 0.4899],\n",
      "        [0.4334, 0.4323, 0.4335]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 5000, Loss: 37.79689407348633\n",
      "tensor([[0.4282, 0.4253, 0.4289],\n",
      "        [0.4282, 0.4253, 0.4289],\n",
      "        [0.4282, 0.4253, 0.4289],\n",
      "        [0.4282, 0.4253, 0.4289],\n",
      "        [0.4282, 0.4253, 0.4289],\n",
      "        [0.4596, 0.3693, 0.4865],\n",
      "        [0.4282, 0.4253, 0.4289]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 6000, Loss: 37.78687286376953\n",
      "tensor([[0.4271, 0.4202, 0.4290],\n",
      "        [0.4271, 0.4202, 0.4290],\n",
      "        [0.4271, 0.4202, 0.4290],\n",
      "        [0.4271, 0.4202, 0.4290],\n",
      "        [0.4271, 0.4202, 0.4290],\n",
      "        [0.4634, 0.3615, 0.4940],\n",
      "        [0.4271, 0.4202, 0.4290]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 7000, Loss: 37.77509307861328\n",
      "tensor([[0.4282, 0.4131, 0.4330],\n",
      "        [0.4282, 0.4131, 0.4330],\n",
      "        [0.4282, 0.4131, 0.4330],\n",
      "        [0.4282, 0.4131, 0.4330],\n",
      "        [0.4282, 0.4131, 0.4330],\n",
      "        [0.4695, 0.3529, 0.5050],\n",
      "        [0.4282, 0.4131, 0.4330]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 8000, Loss: 37.74639892578125\n",
      "tensor([[0.4314, 0.3991, 0.4430],\n",
      "        [0.4314, 0.3991, 0.4430],\n",
      "        [0.4314, 0.3991, 0.4430],\n",
      "        [0.4314, 0.3991, 0.4430],\n",
      "        [0.4314, 0.3991, 0.4430],\n",
      "        [0.4770, 0.3364, 0.5218],\n",
      "        [0.4314, 0.3991, 0.4430]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 9000, Loss: 37.6567268371582\n",
      "tensor([[0.4368, 0.3737, 0.4637],\n",
      "        [0.4368, 0.3737, 0.4637],\n",
      "        [0.4368, 0.3737, 0.4637],\n",
      "        [0.4368, 0.3737, 0.4637],\n",
      "        [0.4368, 0.3737, 0.4637],\n",
      "        [0.4912, 0.2901, 0.5633],\n",
      "        [0.4368, 0.3737, 0.4637]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 10000, Loss: 37.450584411621094\n",
      "tensor([[0.4385, 0.3431, 0.4947],\n",
      "        [0.4385, 0.3431, 0.4947],\n",
      "        [0.4385, 0.3431, 0.4947],\n",
      "        [0.4385, 0.3431, 0.4947],\n",
      "        [0.4385, 0.3431, 0.4947],\n",
      "        [0.5025, 0.2094, 0.6370],\n",
      "        [0.4385, 0.3431, 0.4947]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 11000, Loss: 37.14232635498047\n",
      "tensor([[0.4285, 0.3159, 0.5299],\n",
      "        [0.4285, 0.3159, 0.5299],\n",
      "        [0.4285, 0.3159, 0.5299],\n",
      "        [0.4285, 0.3159, 0.5299],\n",
      "        [0.4285, 0.3159, 0.5299],\n",
      "        [0.4872, 0.1366, 0.7280],\n",
      "        [0.4285, 0.3159, 0.5299]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 12000, Loss: 36.574005126953125\n",
      "tensor([[0.4042, 0.2918, 0.5664],\n",
      "        [0.4092, 0.2416, 0.6203],\n",
      "        [0.4059, 0.2745, 0.5845],\n",
      "        [0.4092, 0.2416, 0.6203],\n",
      "        [0.4071, 0.2622, 0.5976],\n",
      "        [0.4408, 0.0611, 0.8669],\n",
      "        [0.4042, 0.2918, 0.5664]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 13000, Loss: 35.228057861328125\n",
      "tensor([[0.3703, 0.1998, 0.6836],\n",
      "        [0.3687, 0.1533, 0.7441],\n",
      "        [0.3686, 0.1498, 0.7489],\n",
      "        [0.3674, 0.1225, 0.7870],\n",
      "        [0.3678, 0.1316, 0.7740],\n",
      "        [0.3564, 0.0151, 0.9661],\n",
      "        [0.3723, 0.2703, 0.6003]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 14000, Loss: 33.254066467285156\n",
      "tensor([[0.3278, 0.0992, 0.8296],\n",
      "        [0.3213, 0.0609, 0.8891],\n",
      "        [0.3211, 0.0598, 0.8910],\n",
      "        [0.3173, 0.0447, 0.9159],\n",
      "        [0.3198, 0.0544, 0.8999],\n",
      "        [0.2844, 0.0028, 0.9935],\n",
      "        [0.3418, 0.2534, 0.6278]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 15000, Loss: 32.0394401550293\n",
      "tensor([[2.9957e-01, 5.0085e-02, 9.1137e-01],\n",
      "        [2.9266e-01, 2.6116e-02, 9.5141e-01],\n",
      "        [2.9242e-01, 2.5506e-02, 9.5247e-01],\n",
      "        [2.8924e-01, 1.8761e-02, 9.6432e-01],\n",
      "        [2.9210e-01, 2.4745e-02, 9.5379e-01],\n",
      "        [2.5732e-01, 7.1701e-04, 9.9838e-01],\n",
      "        [3.1837e-01, 2.4138e-01, 6.4973e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 16000, Loss: 31.368267059326172\n",
      "tensor([[2.8231e-01, 2.6781e-02, 9.5290e-01],\n",
      "        [2.7641e-01, 1.2463e-02, 9.7717e-01],\n",
      "        [2.7600e-01, 1.1806e-02, 9.7831e-01],\n",
      "        [2.7382e-01, 8.8456e-03, 9.8353e-01],\n",
      "        [2.7624e-01, 1.2178e-02, 9.7766e-01],\n",
      "        [2.4741e-01, 2.3112e-04, 9.9950e-01],\n",
      "        [3.0089e-01, 2.3211e-01, 6.6918e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 17000, Loss: 30.968685150146484\n",
      "tensor([[2.7153e-01, 1.4894e-02, 9.7450e-01],\n",
      "        [2.6678e-01, 6.4134e-03, 9.8865e-01],\n",
      "        [2.6618e-01, 5.7593e-03, 9.8976e-01],\n",
      "        [2.6483e-01, 4.5194e-03, 9.9190e-01],\n",
      "        [2.6666e-01, 6.2845e-03, 9.8887e-01],\n",
      "        [2.4354e-01, 8.6481e-05, 9.9982e-01],\n",
      "        [2.8838e-01, 2.2390e-01, 6.8776e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 18000, Loss: 30.70296859741211\n",
      "tensor([[2.6484e-01, 8.4649e-03, 9.8600e-01],\n",
      "        [2.6099e-01, 3.4706e-03, 9.9410e-01],\n",
      "        [2.6021e-01, 2.8913e-03, 9.9506e-01],\n",
      "        [2.5947e-01, 2.4344e-03, 9.9582e-01],\n",
      "        [2.6080e-01, 3.3226e-03, 9.9435e-01],\n",
      "        [2.4189e-01, 3.5742e-05, 9.9993e-01],\n",
      "        [2.8008e-01, 2.1542e-01, 7.0600e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 19000, Loss: 30.505680084228516\n",
      "tensor([[2.6086e-01, 4.8384e-03, 9.9225e-01],\n",
      "        [2.5760e-01, 1.9429e-03, 9.9682e-01],\n",
      "        [2.5659e-01, 1.4625e-03, 9.9759e-01],\n",
      "        [2.5633e-01, 1.3589e-03, 9.9776e-01],\n",
      "        [2.5725e-01, 1.7620e-03, 9.9711e-01],\n",
      "        [2.4090e-01, 1.5919e-05, 9.9997e-01],\n",
      "        [2.7534e-01, 2.0553e-01, 7.2393e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 20000, Loss: 30.345232009887695\n",
      "tensor([[2.5862e-01, 2.7449e-03, 9.9571e-01],\n",
      "        [2.5571e-01, 1.1206e-03, 9.9822e-01],\n",
      "        [2.5433e-01, 7.3130e-04, 9.9883e-01],\n",
      "        [2.5455e-01, 7.8289e-04, 9.9875e-01],\n",
      "        [2.5506e-01, 9.1835e-04, 9.9854e-01],\n",
      "        [2.3994e-01, 7.6513e-06, 9.9999e-01],\n",
      "        [2.7342e-01, 1.9382e-01, 7.4146e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 21000, Loss: 29.30557632446289\n",
      "tensor([[2.5245e-01, 1.1883e-03, 9.9828e-01],\n",
      "        [2.5021e-01, 5.1956e-04, 9.9925e-01],\n",
      "        [2.4650e-01, 1.3147e-04, 9.9981e-01],\n",
      "        [2.4929e-01, 3.7041e-04, 9.9946e-01],\n",
      "        [2.4873e-01, 3.0095e-04, 9.9957e-01],\n",
      "        [2.3703e-01, 3.6604e-06, 9.9999e-01],\n",
      "        [2.6404e-01, 7.3099e-02, 8.9717e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 22000, Loss: 28.961271286010742\n",
      "tensor([[2.4969e-01, 5.9128e-04, 9.9918e-01],\n",
      "        [2.4772e-01, 2.9213e-04, 9.9960e-01],\n",
      "        [2.4167e-01, 3.2625e-05, 9.9996e-01],\n",
      "        [2.4681e-01, 2.1064e-04, 9.9971e-01],\n",
      "        [2.4515e-01, 1.1579e-04, 9.9984e-01],\n",
      "        [2.3428e-01, 2.1316e-06, 1.0000e+00],\n",
      "        [2.6122e-01, 3.3283e-02, 9.5240e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 23000, Loss: 28.8216495513916\n",
      "tensor([[2.4974e-01, 3.0277e-04, 9.9957e-01],\n",
      "        [2.4839e-01, 2.1250e-04, 9.9970e-01],\n",
      "        [2.3658e-01, 9.1405e-06, 9.9999e-01],\n",
      "        [2.4719e-01, 1.5509e-04, 9.9978e-01],\n",
      "        [2.4255e-01, 4.5391e-05, 9.9994e-01],\n",
      "        [2.3074e-01, 1.8532e-06, 1.0000e+00],\n",
      "        [2.6568e-01, 1.7530e-02, 9.7440e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 24000, Loss: 28.739099502563477\n",
      "tensor([[2.5046e-01, 1.4447e-04, 9.9979e-01],\n",
      "        [2.5265e-01, 2.1356e-04, 9.9968e-01],\n",
      "        [2.2827e-01, 2.3985e-06, 1.0000e+00],\n",
      "        [2.5092e-01, 1.5690e-04, 9.9977e-01],\n",
      "        [2.3852e-01, 1.6437e-05, 9.9998e-01],\n",
      "        [2.2815e-01, 2.3451e-06, 1.0000e+00],\n",
      "        [2.7463e-01, 9.5246e-03, 9.8565e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 25000, Loss: 28.67928123474121\n",
      "tensor([[2.5017e-01, 6.4132e-05, 9.9990e-01],\n",
      "        [2.6055e-01, 2.6295e-04, 9.9959e-01],\n",
      "        [2.1778e-01, 5.9535e-07, 1.0000e+00],\n",
      "        [2.5802e-01, 1.8701e-04, 9.9971e-01],\n",
      "        [2.3326e-01, 5.8902e-06, 9.9999e-01],\n",
      "        [2.2946e-01, 3.3926e-06, 9.9999e-01],\n",
      "        [2.8327e-01, 5.0973e-03, 9.9201e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 26000, Loss: 28.636098861694336\n",
      "tensor([[2.4917e-01, 2.8811e-05, 9.9995e-01],\n",
      "        [2.6827e-01, 2.7570e-04, 9.9955e-01],\n",
      "        [2.0855e-01, 1.5437e-07, 1.0000e+00],\n",
      "        [2.6428e-01, 1.7357e-04, 9.9971e-01],\n",
      "        [2.2900e-01, 2.3290e-06, 1.0000e+00],\n",
      "        [2.3421e-01, 4.5272e-06, 9.9999e-01],\n",
      "        [2.8830e-01, 2.6404e-03, 9.9570e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 27000, Loss: 28.60293960571289\n",
      "tensor([[2.4874e-01, 1.3925e-05, 9.9998e-01],\n",
      "        [2.7423e-01, 2.0990e-04, 9.9964e-01],\n",
      "        [2.0036e-01, 4.5322e-08, 1.0000e+00],\n",
      "        [2.6732e-01, 1.0225e-04, 9.9982e-01],\n",
      "        [2.2645e-01, 1.1083e-06, 1.0000e+00],\n",
      "        [2.4024e-01, 5.4034e-06, 9.9999e-01],\n",
      "        [2.9238e-01, 1.3166e-03, 9.9779e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 28000, Loss: 28.57273292541504\n",
      "tensor([[2.4914e-01, 7.6115e-06, 9.9999e-01],\n",
      "        [2.7897e-01, 1.2102e-04, 9.9979e-01],\n",
      "        [1.9116e-01, 1.6923e-08, 1.0000e+00],\n",
      "        [2.6831e-01, 4.6079e-05, 9.9992e-01],\n",
      "        [2.2589e-01, 7.5353e-07, 1.0000e+00],\n",
      "        [2.4720e-01, 6.3160e-06, 9.9999e-01],\n",
      "        [2.9759e-01, 6.2030e-04, 9.9893e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 29000, Loss: 28.53890609741211\n",
      "tensor([[2.5098e-01, 5.1681e-06, 9.9999e-01],\n",
      "        [2.8182e-01, 5.4975e-05, 9.9990e-01],\n",
      "        [1.7944e-01, 8.6724e-09, 1.0000e+00],\n",
      "        [2.6796e-01, 1.9412e-05, 9.9996e-01],\n",
      "        [2.2806e-01, 7.8445e-07, 1.0000e+00],\n",
      "        [2.5586e-01, 7.6000e-06, 9.9999e-01],\n",
      "        [3.0372e-01, 2.6796e-04, 9.9952e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 30000, Loss: 28.50002670288086\n",
      "tensor([[2.5636e-01, 4.9377e-06, 9.9999e-01],\n",
      "        [2.8286e-01, 2.6583e-05, 9.9995e-01],\n",
      "        [1.6661e-01, 5.4405e-09, 1.0000e+00],\n",
      "        [2.6580e-01, 9.1040e-06, 9.9998e-01],\n",
      "        [2.3103e-01, 8.8496e-07, 1.0000e+00],\n",
      "        [2.6702e-01, 9.8417e-06, 9.9998e-01],\n",
      "        [3.0625e-01, 1.0858e-04, 9.9980e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 31000, Loss: 28.465099334716797\n",
      "tensor([[2.6403e-01, 5.6551e-06, 9.9999e-01],\n",
      "        [2.8438e-01, 1.7266e-05, 9.9997e-01],\n",
      "        [1.5539e-01, 3.8685e-09, 1.0000e+00],\n",
      "        [2.6239e-01, 5.1573e-06, 9.9999e-01],\n",
      "        [2.3408e-01, 9.8319e-07, 1.0000e+00],\n",
      "        [2.7986e-01, 1.3536e-05, 9.9997e-01],\n",
      "        [3.0322e-01, 4.6518e-05, 9.9991e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 32000, Loss: 28.44306755065918\n",
      "tensor([[2.7069e-01, 5.9412e-06, 9.9999e-01],\n",
      "        [2.8548e-01, 1.2347e-05, 9.9998e-01],\n",
      "        [1.4665e-01, 2.8413e-09, 1.0000e+00],\n",
      "        [2.5945e-01, 3.3510e-06, 9.9999e-01],\n",
      "        [2.3825e-01, 1.0861e-06, 1.0000e+00],\n",
      "        [2.9231e-01, 1.7182e-05, 9.9997e-01],\n",
      "        [2.9853e-01, 2.3112e-05, 9.9996e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 33000, Loss: 28.43286895751953\n",
      "tensor([[2.7275e-01, 4.5440e-06, 9.9999e-01],\n",
      "        [2.8397e-01, 7.6677e-06, 9.9999e-01],\n",
      "        [1.4015e-01, 1.8514e-09, 1.0000e+00],\n",
      "        [2.6009e-01, 2.4769e-06, 1.0000e+00],\n",
      "        [2.4470e-01, 1.1536e-06, 1.0000e+00],\n",
      "        [3.0109e-01, 1.6642e-05, 9.9997e-01],\n",
      "        [2.9606e-01, 1.3290e-05, 9.9998e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Stopping early on epoch 33264 (patience: 10000)\n",
      "GNN training (n=7) took 37.802\n",
      "GNN final continuous loss: 28.431201934814453\n",
      "GNN best continuous loss: 28.431201934814453\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1]]),\n tensor([[0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1],\n         [0, 0, 1]]))"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Running GNN...')\n",
    "gnn_start = time()\n",
    "\n",
    "_, epoch, final_bitstring, best_bitstring = run_gnn_training(\n",
    "    q_torch, graph_dgl, net, embed, optimizer, gnn_hypers['number_epochs'],\n",
    "    gnn_hypers['tolerance'], gnn_hypers['patience'], gnn_hypers['prob_threshold'], calculate_H)\n",
    "final_bitstring, best_bitstring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1]]),\n [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bitstring, best_bitstring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 38.458824157714844\n",
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.4798, 0.4288, 0.4591],\n",
      "        [0.4996, 0.4987, 0.4993],\n",
      "        [0.4982, 0.4935, 0.4963],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.4982, 0.4935, 0.4963],\n",
      "        [0.4994, 0.4978, 0.4987]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 1000, Loss: 38.12750244140625\n",
      "tensor([[0.5170, 0.5046, 0.5143],\n",
      "        [0.5033, 0.4429, 0.4812],\n",
      "        [0.5121, 0.4824, 0.5024],\n",
      "        [0.5170, 0.5046, 0.5143],\n",
      "        [0.5170, 0.5046, 0.5143],\n",
      "        [0.5170, 0.5046, 0.5143],\n",
      "        [0.5085, 0.4662, 0.4937]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 2000, Loss: 37.891517639160156\n",
      "tensor([[0.5453, 0.4654, 0.5387],\n",
      "        [0.5358, 0.4056, 0.5093],\n",
      "        [0.5391, 0.4266, 0.5197],\n",
      "        [0.5453, 0.4654, 0.5387],\n",
      "        [0.5453, 0.4654, 0.5387],\n",
      "        [0.5453, 0.4654, 0.5387],\n",
      "        [0.5346, 0.3987, 0.5058]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 3000, Loss: 37.27753448486328\n",
      "tensor([[0.5778, 0.3861, 0.5614],\n",
      "        [0.5749, 0.3376, 0.5432],\n",
      "        [0.5741, 0.3248, 0.5382],\n",
      "        [0.5801, 0.4270, 0.5759],\n",
      "        [0.5801, 0.4270, 0.5759],\n",
      "        [0.5801, 0.4270, 0.5759],\n",
      "        [0.5721, 0.2927, 0.5250]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 4000, Loss: 36.416831970214844\n",
      "tensor([[0.6228, 0.2870, 0.5909],\n",
      "        [0.6271, 0.2454, 0.5822],\n",
      "        [0.6309, 0.2128, 0.5747],\n",
      "        [0.6142, 0.3816, 0.6081],\n",
      "        [0.6131, 0.3938, 0.6101],\n",
      "        [0.6142, 0.3816, 0.6081],\n",
      "        [0.6341, 0.1867, 0.5679]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 5000, Loss: 34.801414489746094\n",
      "tensor([[0.6853, 0.1838, 0.6335],\n",
      "        [0.7050, 0.1253, 0.6307],\n",
      "        [0.7108, 0.1109, 0.6298],\n",
      "        [0.6549, 0.3055, 0.6376],\n",
      "        [0.6425, 0.3645, 0.6392],\n",
      "        [0.6614, 0.2766, 0.6367],\n",
      "        [0.7184, 0.0942, 0.6287]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 6000, Loss: 33.089027404785156\n",
      "tensor([[0.7472, 0.1111, 0.6704],\n",
      "        [0.7838, 0.0564, 0.6754],\n",
      "        [0.7859, 0.0540, 0.6757],\n",
      "        [0.6990, 0.2296, 0.6644],\n",
      "        [0.6662, 0.3402, 0.6606],\n",
      "        [0.7193, 0.1728, 0.6669],\n",
      "        [0.7947, 0.0450, 0.6770]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 7000, Loss: 31.732210159301758\n",
      "tensor([[0.8070, 0.0660, 0.6728],\n",
      "        [0.8562, 0.0243, 0.6713],\n",
      "        [0.8504, 0.0278, 0.6715],\n",
      "        [0.7604, 0.1372, 0.6741],\n",
      "        [0.6873, 0.3194, 0.6757],\n",
      "        [0.7830, 0.0983, 0.6735],\n",
      "        [0.8570, 0.0239, 0.6713]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 8000, Loss: 30.553457260131836\n",
      "tensor([[0.8668, 0.0368, 0.6391],\n",
      "        [0.9166, 0.0104, 0.6122],\n",
      "        [0.9086, 0.0133, 0.6174],\n",
      "        [0.8256, 0.0771, 0.6550],\n",
      "        [0.7088, 0.3005, 0.6871],\n",
      "        [0.8480, 0.0528, 0.6468],\n",
      "        [0.9124, 0.0118, 0.6150]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 9000, Loss: 29.546245574951172\n",
      "tensor([[0.9162, 0.0187, 0.5945],\n",
      "        [0.9548, 0.0046, 0.5414],\n",
      "        [0.9509, 0.0055, 0.5485],\n",
      "        [0.8777, 0.0452, 0.6273],\n",
      "        [0.7288, 0.2825, 0.6992],\n",
      "        [0.9002, 0.0281, 0.6097],\n",
      "        [0.9529, 0.0051, 0.5451]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 10000, Loss: 28.72484588623047\n",
      "tensor([[0.9506, 0.0090, 0.5529],\n",
      "        [0.9747, 0.0023, 0.4866],\n",
      "        [0.9754, 0.0022, 0.4837],\n",
      "        [0.9050, 0.0347, 0.6184],\n",
      "        [0.7456, 0.2647, 0.7182],\n",
      "        [0.9344, 0.0161, 0.5813],\n",
      "        [0.9760, 0.0021, 0.4813]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 11000, Loss: 28.005897521972656\n",
      "tensor([[9.7219e-01, 4.1546e-03, 5.1139e-01],\n",
      "        [9.8424e-01, 1.4522e-03, 4.5026e-01],\n",
      "        [9.8807e-01, 8.7100e-04, 4.2100e-01],\n",
      "        [9.1467e-01, 3.4454e-02, 6.3296e-01],\n",
      "        [7.5957e-01, 2.4702e-01, 7.4290e-01],\n",
      "        [9.5606e-01, 9.7703e-03, 5.6116e-01],\n",
      "        [9.8791e-01, 8.9162e-04, 4.2233e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 12000, Loss: 27.375978469848633\n",
      "tensor([[9.8480e-01, 1.8537e-03, 4.7065e-01],\n",
      "        [9.8907e-01, 1.0454e-03, 4.3411e-01],\n",
      "        [9.9416e-01, 3.5387e-04, 3.6725e-01],\n",
      "        [9.1872e-01, 3.5911e-02, 6.5794e-01],\n",
      "        [7.7098e-01, 2.2966e-01, 7.6663e-01],\n",
      "        [9.7094e-01, 5.7575e-03, 5.4367e-01],\n",
      "        [9.9364e-01, 4.1005e-04, 3.7610e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 13000, Loss: 26.822900772094727\n",
      "tensor([[9.9181e-01, 7.9542e-04, 4.2631e-01],\n",
      "        [9.9207e-01, 7.5378e-04, 4.2269e-01],\n",
      "        [9.9690e-01, 1.5853e-04, 3.2258e-01],\n",
      "        [9.1944e-01, 3.7787e-02, 6.8532e-01],\n",
      "        [7.8000e-01, 2.1296e-01, 7.8762e-01],\n",
      "        [9.8144e-01, 3.1141e-03, 5.2003e-01],\n",
      "        [9.9610e-01, 2.3192e-04, 3.4593e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 14000, Loss: 26.353307723999023\n",
      "tensor([[9.9564e-01, 3.1977e-04, 3.8329e-01],\n",
      "        [9.9457e-01, 4.5505e-04, 4.0745e-01],\n",
      "        [9.9823e-01, 7.4532e-05, 2.9051e-01],\n",
      "        [9.2620e-01, 3.2963e-02, 7.0306e-01],\n",
      "        [7.8642e-01, 1.9708e-01, 8.0651e-01],\n",
      "        [9.8909e-01, 1.4115e-03, 4.8752e-01],\n",
      "        [9.9724e-01, 1.5214e-04, 3.3438e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 15000, Loss: 25.987293243408203\n",
      "tensor([[9.9767e-01, 1.2343e-04, 3.5306e-01],\n",
      "        [9.9635e-01, 2.5238e-04, 4.0105e-01],\n",
      "        [9.9898e-01, 3.3255e-05, 2.7276e-01],\n",
      "        [9.3751e-01, 2.4441e-02, 7.1373e-01],\n",
      "        [7.9084e-01, 1.8214e-01, 8.2322e-01],\n",
      "        [9.9371e-01, 5.9904e-04, 4.6162e-01],\n",
      "        [9.9803e-01, 9.4921e-05, 3.3610e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 16000, Loss: 25.67816925048828\n",
      "tensor([[9.9875e-01, 4.5771e-05, 3.2829e-01],\n",
      "        [9.9767e-01, 1.2187e-04, 3.9162e-01],\n",
      "        [9.9941e-01, 1.3898e-05, 2.5901e-01],\n",
      "        [9.4639e-01, 1.7975e-02, 7.2491e-01],\n",
      "        [7.9302e-01, 1.6821e-01, 8.3815e-01],\n",
      "        [9.9623e-01, 2.5983e-04, 4.4335e-01],\n",
      "        [9.9859e-01, 5.5048e-05, 3.3983e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 17000, Loss: 25.395902633666992\n",
      "tensor([[9.9933e-01, 1.5696e-05, 3.0423e-01],\n",
      "        [9.9862e-01, 4.8993e-05, 3.7418e-01],\n",
      "        [9.9966e-01, 5.4453e-06, 2.4633e-01],\n",
      "        [9.5271e-01, 1.3321e-02, 7.3699e-01],\n",
      "        [7.9210e-01, 1.5530e-01, 8.5173e-01],\n",
      "        [9.9755e-01, 1.2122e-04, 4.3408e-01],\n",
      "        [9.9897e-01, 3.0876e-05, 3.4497e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 18000, Loss: 25.117229461669922\n",
      "tensor([[9.9966e-01, 4.7751e-06, 2.7447e-01],\n",
      "        [9.9924e-01, 1.6523e-05, 3.4589e-01],\n",
      "        [9.9979e-01, 2.1350e-06, 2.3340e-01],\n",
      "        [9.5633e-01, 1.0165e-02, 7.4991e-01],\n",
      "        [7.8730e-01, 1.4335e-01, 8.6426e-01],\n",
      "        [9.9812e-01, 6.8998e-05, 4.3745e-01],\n",
      "        [9.9916e-01, 1.9553e-05, 3.5624e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 19000, Loss: 24.808534622192383\n",
      "tensor([[9.9983e-01, 1.3220e-06, 2.3415e-01],\n",
      "        [9.9960e-01, 4.9893e-06, 3.0422e-01],\n",
      "        [9.9986e-01, 9.6618e-07, 2.1935e-01],\n",
      "        [9.5684e-01, 8.3414e-03, 7.6386e-01],\n",
      "        [7.7910e-01, 1.3230e-01, 8.7592e-01],\n",
      "        [9.9807e-01, 5.8565e-05, 4.5911e-01],\n",
      "        [9.9911e-01, 1.7447e-05, 3.7987e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 20000, Loss: 24.419506072998047\n",
      "tensor([[9.9990e-01, 4.3769e-07, 1.8468e-01],\n",
      "        [9.9975e-01, 1.8073e-06, 2.5196e-01],\n",
      "        [9.9987e-01, 6.8201e-07, 2.0410e-01],\n",
      "        [9.5258e-01, 7.9743e-03, 7.7935e-01],\n",
      "        [7.6797e-01, 1.2207e-01, 8.8689e-01],\n",
      "        [9.9724e-01, 8.3524e-05, 4.9606e-01],\n",
      "        [9.9861e-01, 2.8122e-05, 4.2060e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 21000, Loss: 23.90489959716797\n",
      "tensor([[9.9994e-01, 1.5617e-07, 1.3337e-01],\n",
      "        [9.9984e-01, 7.6689e-07, 1.9794e-01],\n",
      "        [9.9986e-01, 6.1814e-07, 1.8797e-01],\n",
      "        [9.4359e-01, 8.5826e-03, 7.9740e-01],\n",
      "        [7.5216e-01, 1.1259e-01, 8.9724e-01],\n",
      "        [9.9516e-01, 1.6706e-04, 5.4948e-01],\n",
      "        [9.9734e-01, 6.4858e-05, 4.7945e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 22000, Loss: 23.255210876464844\n",
      "tensor([[9.9997e-01, 5.8481e-08, 8.7137e-02],\n",
      "        [9.9989e-01, 3.9681e-07, 1.4953e-01],\n",
      "        [9.9985e-01, 6.5005e-07, 1.7068e-01],\n",
      "        [9.2863e-01, 1.0117e-02, 8.1783e-01],\n",
      "        [7.3081e-01, 1.0382e-01, 9.0689e-01],\n",
      "        [9.9053e-01, 4.0440e-04, 6.1574e-01],\n",
      "        [9.9427e-01, 1.8441e-04, 5.5501e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 23000, Loss: 22.476688385009766\n",
      "tensor([[9.9998e-01, 2.7361e-08, 4.7608e-02],\n",
      "        [9.9990e-01, 3.5985e-07, 1.1088e-01],\n",
      "        [9.9981e-01, 9.1009e-07, 1.4772e-01],\n",
      "        [8.9959e-01, 1.3843e-02, 8.4146e-01],\n",
      "        [7.0233e-01, 9.5680e-02, 9.1574e-01],\n",
      "        [9.7819e-01, 1.2240e-03, 6.9083e-01],\n",
      "        [9.8499e-01, 6.8867e-04, 6.4560e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 24000, Loss: 21.60871696472168\n",
      "tensor([[9.9999e-01, 1.4759e-08, 1.9076e-02],\n",
      "        [9.9984e-01, 6.3045e-07, 8.3133e-02],\n",
      "        [9.9972e-01, 1.5178e-06, 1.1504e-01],\n",
      "        [8.4263e-01, 2.2109e-02, 8.6987e-01],\n",
      "        [6.6757e-01, 8.7951e-02, 9.2376e-01],\n",
      "        [9.4261e-01, 4.2905e-03, 7.7208e-01],\n",
      "        [9.5428e-01, 3.0133e-03, 7.4548e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 25000, Loss: 20.751529693603516\n",
      "tensor([[9.9999e-01, 6.1536e-09, 6.7624e-03],\n",
      "        [9.9978e-01, 8.0612e-07, 6.0503e-02],\n",
      "        [9.9959e-01, 2.0274e-06, 8.9678e-02],\n",
      "        [7.6424e-01, 3.2997e-02, 8.9728e-01],\n",
      "        [6.3105e-01, 8.0255e-02, 9.3087e-01],\n",
      "        [8.6317e-01, 1.2672e-02, 8.4770e-01],\n",
      "        [8.7076e-01, 1.1518e-02, 8.4186e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 26000, Loss: 19.7764835357666\n",
      "tensor([[9.9999e-01, 2.7847e-09, 2.7180e-03],\n",
      "        [9.9962e-01, 8.6284e-07, 4.6224e-02],\n",
      "        [9.9910e-01, 3.1296e-06, 8.4676e-02],\n",
      "        [6.5912e-01, 4.7597e-02, 9.2243e-01],\n",
      "        [5.9068e-01, 7.2371e-02, 9.3699e-01],\n",
      "        [6.8239e-01, 4.0828e-02, 9.1647e-01],\n",
      "        [6.5039e-01, 5.0333e-02, 9.2452e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 27000, Loss: 18.925832748413086\n",
      "tensor([[9.9999e-01, 7.8216e-10, 8.4238e-04],\n",
      "        [9.9957e-01, 6.6127e-07, 3.0934e-02],\n",
      "        [9.9918e-01, 1.7934e-06, 5.1832e-02],\n",
      "        [5.5713e-01, 6.3529e-02, 9.4141e-01],\n",
      "        [5.5247e-01, 6.5275e-02, 9.4227e-01],\n",
      "        [5.5713e-01, 6.3529e-02, 9.4141e-01],\n",
      "        [5.5247e-01, 6.5275e-02, 9.4227e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 28000, Loss: 18.416793823242188\n",
      "tensor([[1.0000e+00, 3.2578e-10, 3.8481e-04],\n",
      "        [9.9976e-01, 2.5718e-07, 1.6229e-02],\n",
      "        [9.9940e-01, 1.0399e-06, 3.4970e-02],\n",
      "        [5.2328e-01, 5.9822e-02, 9.4737e-01],\n",
      "        [5.2328e-01, 5.9822e-02, 9.4737e-01],\n",
      "        [5.2328e-01, 5.9822e-02, 9.4737e-01],\n",
      "        [5.2328e-01, 5.9822e-02, 9.4737e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 29000, Loss: 17.999610900878906\n",
      "tensor([[1.0000e+00, 1.5623e-10, 2.0232e-04],\n",
      "        [9.9985e-01, 1.3187e-07, 1.0135e-02],\n",
      "        [9.9955e-01, 6.5610e-07, 2.5399e-02],\n",
      "        [4.9687e-01, 5.5170e-02, 9.5208e-01],\n",
      "        [4.9686e-01, 5.5173e-02, 9.5208e-01],\n",
      "        [4.9687e-01, 5.5170e-02, 9.5208e-01],\n",
      "        [4.9686e-01, 5.5173e-02, 9.5208e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 30000, Loss: 17.622133255004883\n",
      "tensor([[1.0000e+00, 7.8392e-11, 1.0939e-04],\n",
      "        [9.9990e-01, 7.5667e-08, 6.7105e-03],\n",
      "        [9.9967e-01, 4.2553e-07, 1.8684e-02],\n",
      "        [4.7138e-01, 5.1031e-02, 9.5633e-01],\n",
      "        [4.7138e-01, 5.1031e-02, 9.5633e-01],\n",
      "        [4.7138e-01, 5.1031e-02, 9.5633e-01],\n",
      "        [4.7138e-01, 5.1031e-02, 9.5633e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 31000, Loss: 17.272438049316406\n",
      "tensor([[1.0000e+00, 4.0306e-11, 5.9166e-05],\n",
      "        [9.9993e-01, 4.6404e-08, 4.5642e-03],\n",
      "        [9.9976e-01, 2.8018e-07, 1.3718e-02],\n",
      "        [4.4644e-01, 4.7290e-02, 9.6019e-01],\n",
      "        [4.4644e-01, 4.7290e-02, 9.6019e-01],\n",
      "        [4.4644e-01, 4.7290e-02, 9.6019e-01],\n",
      "        [4.4644e-01, 4.7290e-02, 9.6019e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 32000, Loss: 16.945552825927734\n",
      "tensor([[1.0000e+00, 2.1252e-11, 3.1823e-05],\n",
      "        [9.9995e-01, 3.0132e-08, 3.1664e-03],\n",
      "        [9.9982e-01, 1.8735e-07, 1.0023e-02],\n",
      "        [4.2198e-01, 4.3902e-02, 9.6370e-01],\n",
      "        [4.2198e-01, 4.3902e-02, 9.6370e-01],\n",
      "        [4.2198e-01, 4.3902e-02, 9.6370e-01],\n",
      "        [4.2198e-01, 4.3902e-02, 9.6370e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 33000, Loss: 16.638776779174805\n",
      "tensor([[1.0000e+00, 1.1236e-11, 1.6720e-05],\n",
      "        [9.9997e-01, 2.0388e-08, 2.2179e-03],\n",
      "        [9.9988e-01, 1.2560e-07, 7.2165e-03],\n",
      "        [3.9806e-01, 4.0842e-02, 9.6689e-01],\n",
      "        [3.9806e-01, 4.0842e-02, 9.6689e-01],\n",
      "        [3.9806e-01, 4.0842e-02, 9.6689e-01],\n",
      "        [3.9806e-01, 4.0842e-02, 9.6689e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 34000, Loss: 16.350570678710938\n",
      "tensor([[1.0000e+00, 6.0680e-12, 8.6608e-06],\n",
      "        [9.9998e-01, 1.4440e-08, 1.5731e-03],\n",
      "        [9.9991e-01, 8.5356e-08, 5.1486e-03],\n",
      "        [3.7472e-01, 3.8098e-02, 9.6979e-01],\n",
      "        [3.7472e-01, 3.8098e-02, 9.6979e-01],\n",
      "        [3.7472e-01, 3.8098e-02, 9.6979e-01],\n",
      "        [3.7472e-01, 3.8098e-02, 9.6979e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 35000, Loss: 16.079648971557617\n",
      "tensor([[1.0000e+00, 3.3496e-12, 4.4181e-06],\n",
      "        [9.9998e-01, 1.0712e-08, 1.1312e-03],\n",
      "        [9.9994e-01, 5.8845e-08, 3.6384e-03],\n",
      "        [3.5206e-01, 3.5672e-02, 9.7244e-01],\n",
      "        [3.5206e-01, 3.5672e-02, 9.7244e-01],\n",
      "        [3.5206e-01, 3.5672e-02, 9.7244e-01],\n",
      "        [3.5206e-01, 3.5672e-02, 9.7244e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 36000, Loss: 15.824983596801758\n",
      "tensor([[1.0000e+00, 1.8525e-12, 2.1865e-06],\n",
      "        [9.9999e-01, 8.2234e-09, 8.1818e-04],\n",
      "        [9.9996e-01, 4.0717e-08, 2.5252e-03],\n",
      "        [3.3013e-01, 3.3592e-02, 9.7485e-01],\n",
      "        [3.3013e-01, 3.3592e-02, 9.7485e-01],\n",
      "        [3.3013e-01, 3.3592e-02, 9.7485e-01],\n",
      "        [3.3013e-01, 3.3592e-02, 9.7485e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 37000, Loss: 15.585494041442871\n",
      "tensor([[1.0000e+00, 1.0439e-12, 1.0593e-06],\n",
      "        [9.9999e-01, 6.5208e-09, 5.9378e-04],\n",
      "        [9.9997e-01, 2.8601e-08, 1.7304e-03],\n",
      "        [3.0899e-01, 3.1939e-02, 9.7705e-01],\n",
      "        [3.0899e-01, 3.1939e-02, 9.7705e-01],\n",
      "        [3.0899e-01, 3.1939e-02, 9.7705e-01],\n",
      "        [3.0899e-01, 3.1939e-02, 9.7705e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 38000, Loss: 15.359819412231445\n",
      "tensor([[1.0000e+00, 5.8887e-13, 4.9256e-07],\n",
      "        [9.9999e-01, 5.3907e-09, 4.3294e-04],\n",
      "        [9.9998e-01, 2.0269e-08, 1.1576e-03],\n",
      "        [2.8865e-01, 3.0943e-02, 9.7906e-01],\n",
      "        [2.8865e-01, 3.0943e-02, 9.7906e-01],\n",
      "        [2.8865e-01, 3.0943e-02, 9.7906e-01],\n",
      "        [2.8865e-01, 3.0943e-02, 9.7906e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 39000, Loss: 15.145158767700195\n",
      "tensor([[1.0000e+00, 3.6742e-13, 2.3180e-07],\n",
      "        [1.0000e+00, 4.8146e-09, 3.1946e-04],\n",
      "        [9.9999e-01, 1.5523e-08, 7.7959e-04],\n",
      "        [2.6909e-01, 3.1341e-02, 9.8089e-01],\n",
      "        [2.6909e-01, 3.1341e-02, 9.8089e-01],\n",
      "        [2.6909e-01, 3.1341e-02, 9.8089e-01],\n",
      "        [2.6909e-01, 3.1341e-02, 9.8089e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 40000, Loss: 14.934340476989746\n",
      "tensor([[1.0000e+00, 2.4675e-13, 1.0544e-07],\n",
      "        [1.0000e+00, 4.7342e-09, 2.3549e-04],\n",
      "        [9.9999e-01, 1.2872e-08, 5.1467e-04],\n",
      "        [2.5021e-01, 3.4552e-02, 9.8255e-01],\n",
      "        [2.5021e-01, 3.4552e-02, 9.8255e-01],\n",
      "        [2.5021e-01, 3.4552e-02, 9.8255e-01],\n",
      "        [2.5021e-01, 3.4552e-02, 9.8255e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 41000, Loss: 14.721759796142578\n",
      "tensor([[1.0000e+00, 1.7798e-13, 4.7728e-08],\n",
      "        [1.0000e+00, 5.0455e-09, 1.7717e-04],\n",
      "        [1.0000e+00, 1.1325e-08, 3.3872e-04],\n",
      "        [2.3200e-01, 3.9668e-02, 9.8407e-01],\n",
      "        [2.3200e-01, 3.9668e-02, 9.8407e-01],\n",
      "        [2.3200e-01, 3.9668e-02, 9.8407e-01],\n",
      "        [2.3200e-01, 3.9668e-02, 9.8407e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 42000, Loss: 14.508182525634766\n",
      "tensor([[1.0000e+00, 1.2619e-13, 2.0826e-08],\n",
      "        [1.0000e+00, 5.2908e-09, 1.3095e-04],\n",
      "        [1.0000e+00, 9.8504e-09, 2.1822e-04],\n",
      "        [2.1464e-01, 4.5339e-02, 9.8545e-01],\n",
      "        [2.1464e-01, 4.5339e-02, 9.8545e-01],\n",
      "        [2.1464e-01, 4.5339e-02, 9.8545e-01],\n",
      "        [2.1464e-01, 4.5339e-02, 9.8545e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 43000, Loss: 14.294190406799316\n",
      "tensor([[1.0000e+00, 8.8095e-14, 8.8380e-09],\n",
      "        [1.0000e+00, 5.5076e-09, 9.6324e-05],\n",
      "        [1.0000e+00, 8.4659e-09, 1.3832e-04],\n",
      "        [1.9826e-01, 5.1436e-02, 9.8672e-01],\n",
      "        [1.9826e-01, 5.1436e-02, 9.8672e-01],\n",
      "        [1.9826e-01, 5.1436e-02, 9.8672e-01],\n",
      "        [1.9826e-01, 5.1436e-02, 9.8672e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 44000, Loss: 14.079204559326172\n",
      "tensor([[1.0000e+00, 6.7544e-14, 3.9574e-09],\n",
      "        [1.0000e+00, 5.9483e-09, 7.2681e-05],\n",
      "        [1.0000e+00, 7.6642e-09, 9.0434e-05],\n",
      "        [1.8288e-01, 5.8023e-02, 9.8787e-01],\n",
      "        [1.8288e-01, 5.8023e-02, 9.8787e-01],\n",
      "        [1.8288e-01, 5.8023e-02, 9.8787e-01],\n",
      "        [1.8288e-01, 5.8023e-02, 9.8787e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 45000, Loss: 13.862615585327148\n",
      "tensor([[1.0000e+00, 5.1127e-14, 1.7057e-09],\n",
      "        [1.0000e+00, 6.5612e-09, 5.5476e-05],\n",
      "        [1.0000e+00, 6.8766e-09, 5.7826e-05],\n",
      "        [1.6848e-01, 6.5173e-02, 9.8892e-01],\n",
      "        [1.6848e-01, 6.5173e-02, 9.8892e-01],\n",
      "        [1.6848e-01, 6.5173e-02, 9.8892e-01],\n",
      "        [1.6848e-01, 6.5173e-02, 9.8892e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 46000, Loss: 13.643133163452148\n",
      "tensor([[1.0000e+00, 3.8831e-14, 7.1740e-10],\n",
      "        [1.0000e+00, 7.0577e-09, 4.1141e-05],\n",
      "        [1.0000e+00, 6.1749e-09, 3.6456e-05],\n",
      "        [1.5504e-01, 7.2954e-02, 9.8988e-01],\n",
      "        [1.5504e-01, 7.2954e-02, 9.8988e-01],\n",
      "        [1.5504e-01, 7.2954e-02, 9.8988e-01],\n",
      "        [1.5504e-01, 7.2954e-02, 9.8988e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 47000, Loss: 13.419831275939941\n",
      "tensor([[1.0000e+00, 2.9965e-14, 2.9662e-10],\n",
      "        [1.0000e+00, 7.8562e-09, 3.1188e-05],\n",
      "        [1.0000e+00, 5.5911e-09, 2.2756e-05],\n",
      "        [1.4250e-01, 8.1427e-02, 9.9075e-01],\n",
      "        [1.4250e-01, 8.1427e-02, 9.9075e-01],\n",
      "        [1.4250e-01, 8.1427e-02, 9.9075e-01],\n",
      "        [1.4250e-01, 8.1427e-02, 9.9075e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 48000, Loss: 13.191587448120117\n",
      "tensor([[1.0000e+00, 2.3730e-14, 1.2011e-10],\n",
      "        [1.0000e+00, 8.6433e-09, 2.3010e-05],\n",
      "        [1.0000e+00, 5.1350e-09, 1.4032e-05],\n",
      "        [1.3085e-01, 9.0648e-02, 9.9155e-01],\n",
      "        [1.3085e-01, 9.0648e-02, 9.9155e-01],\n",
      "        [1.3085e-01, 9.0648e-02, 9.9155e-01],\n",
      "        [1.3085e-01, 9.0648e-02, 9.9155e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 49000, Loss: 12.95713996887207\n",
      "tensor([[1.0000e+00, 2.0656e-14, 5.0181e-11],\n",
      "        [1.0000e+00, 9.8836e-09, 1.7190e-05],\n",
      "        [1.0000e+00, 4.9779e-09, 8.8108e-06],\n",
      "        [1.2003e-01, 1.0067e-01, 9.9228e-01],\n",
      "        [1.2003e-01, 1.0067e-01, 9.9228e-01],\n",
      "        [1.2003e-01, 1.0067e-01, 9.9228e-01],\n",
      "        [1.2003e-01, 1.0067e-01, 9.9228e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 50000, Loss: 12.71554946899414\n",
      "tensor([[1.0000e+00, 2.0570e-14, 2.1157e-11],\n",
      "        [1.0000e+00, 1.2162e-08, 1.3006e-05],\n",
      "        [1.0000e+00, 5.2127e-09, 5.5606e-06],\n",
      "        [1.1002e-01, 1.1155e-01, 9.9295e-01],\n",
      "        [1.1002e-01, 1.1155e-01, 9.9295e-01],\n",
      "        [1.1002e-01, 1.1155e-01, 9.9295e-01],\n",
      "        [1.1002e-01, 1.1155e-01, 9.9295e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 51000, Loss: 12.465555191040039\n",
      "tensor([[1.0000e+00, 2.5647e-14, 8.9023e-12],\n",
      "        [1.0000e+00, 1.7246e-08, 1.0127e-05],\n",
      "        [1.0000e+00, 6.2125e-09, 3.5051e-06],\n",
      "        [1.0074e-01, 1.2334e-01, 9.9355e-01],\n",
      "        [1.0074e-01, 1.2334e-01, 9.9355e-01],\n",
      "        [1.0074e-01, 1.2334e-01, 9.9355e-01],\n",
      "        [1.0074e-01, 1.2334e-01, 9.9355e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 52000, Loss: 12.206221580505371\n",
      "tensor([[1.0000e+00, 4.7772e-14, 3.6894e-12],\n",
      "        [1.0000e+00, 2.9555e-08, 7.7060e-06],\n",
      "        [1.0000e+00, 9.3312e-09, 2.1901e-06],\n",
      "        [9.2187e-02, 1.3606e-01, 9.9411e-01],\n",
      "        [9.2187e-02, 1.3606e-01, 9.9411e-01],\n",
      "        [9.2187e-02, 1.3606e-01, 9.9411e-01],\n",
      "        [9.2187e-02, 1.3606e-01, 9.9411e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 53000, Loss: 11.936610221862793\n",
      "tensor([[1.0000e+00, 1.4745e-13, 1.5275e-12],\n",
      "        [1.0000e+00, 6.5101e-08, 5.8362e-06],\n",
      "        [1.0000e+00, 1.8754e-08, 1.3674e-06],\n",
      "        [8.4293e-02, 1.4977e-01, 9.9461e-01],\n",
      "        [8.4293e-02, 1.4977e-01, 9.9461e-01],\n",
      "        [8.4293e-02, 1.4977e-01, 9.9461e-01],\n",
      "        [8.4293e-02, 1.4977e-01, 9.9461e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 54000, Loss: 11.655946731567383\n",
      "tensor([[1.0000e+00, 6.2998e-13, 6.5526e-13],\n",
      "        [1.0000e+00, 1.6446e-07, 4.4069e-06],\n",
      "        [1.0000e+00, 4.5459e-08, 8.7143e-07],\n",
      "        [7.7031e-02, 1.6449e-01, 9.9508e-01],\n",
      "        [7.7031e-02, 1.6449e-01, 9.9508e-01],\n",
      "        [7.7031e-02, 1.6449e-01, 9.9508e-01],\n",
      "        [7.7031e-02, 1.6449e-01, 9.9508e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 55000, Loss: 11.363378524780273\n",
      "tensor([[1.0000e+00, 3.4204e-12, 3.3534e-13],\n",
      "        [1.0000e+00, 4.3228e-07, 3.3152e-06],\n",
      "        [1.0000e+00, 1.2650e-07, 6.1490e-07],\n",
      "        [7.0348e-02, 1.8024e-01, 9.9550e-01],\n",
      "        [7.0348e-02, 1.8024e-01, 9.9550e-01],\n",
      "        [7.0348e-02, 1.8024e-01, 9.9550e-01],\n",
      "        [7.0348e-02, 1.8024e-01, 9.9550e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 56000, Loss: 11.058691024780273\n",
      "tensor([[1.0000e+00, 3.5405e-11, 4.0533e-13],\n",
      "        [1.0000e+00, 1.1730e-06, 2.4824e-06],\n",
      "        [1.0000e+00, 5.1081e-07, 7.1251e-07],\n",
      "        [6.4217e-02, 1.9703e-01, 9.9588e-01],\n",
      "        [6.4217e-02, 1.9703e-01, 9.9588e-01],\n",
      "        [6.4217e-02, 1.9703e-01, 9.9588e-01],\n",
      "        [6.4217e-02, 1.9703e-01, 9.9588e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 57000, Loss: 10.741267204284668\n",
      "tensor([[1.0000e+00, 7.4338e-10, 1.4812e-12],\n",
      "        [1.0000e+00, 3.4424e-06, 1.8597e-06],\n",
      "        [1.0000e+00, 3.1019e-06, 1.5638e-06],\n",
      "        [5.8589e-02, 2.1486e-01, 9.9623e-01],\n",
      "        [5.8589e-02, 2.1486e-01, 9.9623e-01],\n",
      "        [5.8589e-02, 2.1486e-01, 9.9623e-01],\n",
      "        [5.8589e-02, 2.1486e-01, 9.9623e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 58000, Loss: 10.411171913146973\n",
      "tensor([[1.0000e+00, 1.4949e-08, 6.5001e-12],\n",
      "        [1.0000e+00, 1.1014e-05, 1.4682e-06],\n",
      "        [1.0000e+00, 1.8367e-05, 3.8145e-06],\n",
      "        [5.3437e-02, 2.3372e-01, 9.9656e-01],\n",
      "        [5.3437e-02, 2.3372e-01, 9.9656e-01],\n",
      "        [5.3437e-02, 2.3372e-01, 9.9656e-01],\n",
      "        [5.3437e-02, 2.3372e-01, 9.9656e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 59000, Loss: 10.068208694458008\n",
      "tensor([[1.0000e+00, 3.6884e-07, 5.7111e-11],\n",
      "        [1.0000e+00, 4.6453e-05, 1.7540e-06],\n",
      "        [1.0000e+00, 1.2234e-04, 1.3890e-05],\n",
      "        [4.8719e-02, 2.5357e-01, 9.9685e-01],\n",
      "        [4.8719e-02, 2.5357e-01, 9.9685e-01],\n",
      "        [4.8719e-02, 2.5357e-01, 9.9685e-01],\n",
      "        [4.8719e-02, 2.5357e-01, 9.9685e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 60000, Loss: 9.709843635559082\n",
      "tensor([[1.0000e+00, 2.0006e-05, 3.7727e-09],\n",
      "        [1.0000e+00, 4.1582e-04, 9.0146e-06],\n",
      "        [9.9999e-01, 1.2825e-03, 1.6209e-04],\n",
      "        [4.4403e-02, 2.7439e-01, 9.9712e-01],\n",
      "        [4.4403e-02, 2.7439e-01, 9.9712e-01],\n",
      "        [4.4403e-02, 2.7439e-01, 9.9712e-01],\n",
      "        [4.4403e-02, 2.7439e-01, 9.9712e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 61000, Loss: 9.320982933044434\n",
      "tensor([[1.0000e+00, 5.5525e-04, 2.8900e-07],\n",
      "        [9.9999e-01, 4.0423e-03, 1.5681e-04],\n",
      "        [9.9991e-01, 9.0719e-03, 2.0562e-03],\n",
      "        [4.0462e-02, 2.9614e-01, 9.9736e-01],\n",
      "        [4.0462e-02, 2.9614e-01, 9.9736e-01],\n",
      "        [4.0462e-02, 2.9614e-01, 9.9736e-01],\n",
      "        [4.0462e-02, 2.9614e-01, 9.9736e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 62000, Loss: 8.897802352905273\n",
      "tensor([[1.0000e+00, 4.1617e-03, 5.1988e-06],\n",
      "        [9.9997e-01, 1.4468e-02, 6.5920e-04],\n",
      "        [9.9944e-01, 2.9783e-02, 1.1203e-02],\n",
      "        [3.6865e-02, 3.1873e-01, 9.9758e-01],\n",
      "        [3.6865e-02, 3.1873e-01, 9.9758e-01],\n",
      "        [3.6865e-02, 3.1873e-01, 9.9758e-01],\n",
      "        [3.6865e-02, 3.1873e-01, 9.9758e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 63000, Loss: 8.473445892333984\n",
      "tensor([[1.0000e+00, 1.0317e-02, 8.0660e-06],\n",
      "        [9.9996e-01, 2.7124e-02, 7.1599e-04],\n",
      "        [9.9918e-01, 5.1589e-02, 1.4878e-02],\n",
      "        [3.3582e-02, 3.4196e-01, 9.9776e-01],\n",
      "        [3.3582e-02, 3.4196e-01, 9.9776e-01],\n",
      "        [3.3582e-02, 3.4196e-01, 9.9776e-01],\n",
      "        [3.3582e-02, 3.4196e-01, 9.9776e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 64000, Loss: 8.04927921295166\n",
      "tensor([[1.0000e+00, 1.8334e-02, 5.5786e-06],\n",
      "        [9.9997e-01, 4.2839e-02, 5.8778e-04],\n",
      "        [9.9927e-01, 7.3712e-02, 1.2476e-02],\n",
      "        [3.0587e-02, 3.6564e-01, 9.9794e-01],\n",
      "        [3.0587e-02, 3.6564e-01, 9.9794e-01],\n",
      "        [3.0587e-02, 3.6564e-01, 9.9794e-01],\n",
      "        [3.0587e-02, 3.6564e-01, 9.9794e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 65000, Loss: 7.633168697357178\n",
      "tensor([[1.0000e+00, 2.9036e-02, 2.9472e-06],\n",
      "        [9.9998e-01, 6.0067e-02, 3.2856e-04],\n",
      "        [9.9944e-01, 9.8315e-02, 8.9789e-03],\n",
      "        [2.7856e-02, 3.8962e-01, 9.9810e-01],\n",
      "        [2.7856e-02, 3.8962e-01, 9.9810e-01],\n",
      "        [2.7856e-02, 3.8962e-01, 9.9810e-01],\n",
      "        [2.7856e-02, 3.8962e-01, 9.9810e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 66000, Loss: 7.235561370849609\n",
      "tensor([[1.0000e+00, 4.0781e-02, 1.0496e-06],\n",
      "        [9.9999e-01, 7.3729e-02, 9.3702e-05],\n",
      "        [9.9964e-01, 1.2234e-01, 5.1579e-03],\n",
      "        [2.5369e-02, 4.1378e-01, 9.9827e-01],\n",
      "        [2.5369e-02, 4.1378e-01, 9.9827e-01],\n",
      "        [2.5369e-02, 4.1378e-01, 9.9827e-01],\n",
      "        [2.5369e-02, 4.1378e-01, 9.9827e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 67000, Loss: 6.858607769012451\n",
      "tensor([[1.0000e+00, 4.6647e-02, 1.6242e-07],\n",
      "        [1.0000e+00, 7.3510e-02, 7.6725e-06],\n",
      "        [9.9985e-01, 1.3622e-01, 1.8335e-03],\n",
      "        [2.3104e-02, 4.3820e-01, 9.9842e-01],\n",
      "        [2.3104e-02, 4.3820e-01, 9.9842e-01],\n",
      "        [2.3104e-02, 4.3820e-01, 9.9842e-01],\n",
      "        [2.3104e-02, 4.3820e-01, 9.9842e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 68000, Loss: 6.483336448669434\n",
      "tensor([[1.0000e+00, 3.8505e-02, 8.4878e-09],\n",
      "        [1.0000e+00, 5.5774e-02, 2.0428e-07],\n",
      "        [9.9997e-01, 1.2786e-01, 3.4768e-04],\n",
      "        [2.1042e-02, 4.6325e-01, 9.9856e-01],\n",
      "        [2.1042e-02, 4.6325e-01, 9.9856e-01],\n",
      "        [2.1042e-02, 4.6325e-01, 9.9856e-01],\n",
      "        [2.1042e-02, 4.6325e-01, 9.9856e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 69000, Loss: 6.094485282897949\n",
      "tensor([[1.0000e+00, 2.7681e-02, 1.4952e-09],\n",
      "        [1.0000e+00, 4.0000e-02, 2.7770e-08],\n",
      "        [9.9999e-01, 1.1169e-01, 1.3265e-04],\n",
      "        [1.9163e-02, 4.8887e-01, 9.9869e-01],\n",
      "        [1.9163e-02, 4.8887e-01, 9.9869e-01],\n",
      "        [1.9163e-02, 4.8887e-01, 9.9869e-01],\n",
      "        [1.9163e-02, 4.8887e-01, 9.9869e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 70000, Loss: 5.696621894836426\n",
      "tensor([[1.0000e+00, 1.9824e-02, 1.3030e-09],\n",
      "        [1.0000e+00, 2.5307e-02, 7.2403e-09],\n",
      "        [9.9999e-01, 9.7312e-02, 1.2723e-04],\n",
      "        [1.7450e-02, 5.1459e-01, 9.9880e-01],\n",
      "        [1.7450e-02, 5.1459e-01, 9.9880e-01],\n",
      "        [1.7450e-02, 5.1459e-01, 9.9880e-01],\n",
      "        [1.7450e-02, 5.1459e-01, 9.9880e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 71000, Loss: 5.298020362854004\n",
      "tensor([[1.0000e+00, 1.4691e-02, 1.7870e-09],\n",
      "        [1.0000e+00, 1.5166e-02, 2.1816e-09],\n",
      "        [9.9998e-01, 8.6252e-02, 1.5846e-04],\n",
      "        [1.5894e-02, 5.4007e-01, 9.9890e-01],\n",
      "        [1.5894e-02, 5.4007e-01, 9.9890e-01],\n",
      "        [1.5894e-02, 5.4007e-01, 9.9890e-01],\n",
      "        [1.5894e-02, 5.4007e-01, 9.9890e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 72000, Loss: 4.904018878936768\n",
      "tensor([[1.0000e+00, 1.1202e-02, 2.3842e-09],\n",
      "        [1.0000e+00, 9.0164e-03, 6.9193e-10],\n",
      "        [9.9998e-01, 7.7556e-02, 1.9421e-04],\n",
      "        [1.4476e-02, 5.6512e-01, 9.9899e-01],\n",
      "        [1.4476e-02, 5.6512e-01, 9.9899e-01],\n",
      "        [1.4476e-02, 5.6512e-01, 9.9899e-01],\n",
      "        [1.4476e-02, 5.6512e-01, 9.9899e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 73000, Loss: 4.517578601837158\n",
      "tensor([[1.0000e+00, 8.6927e-03, 2.6699e-09],\n",
      "        [1.0000e+00, 5.3763e-03, 2.1135e-10],\n",
      "        [9.9998e-01, 7.0340e-02, 2.1511e-04],\n",
      "        [1.3187e-02, 5.8961e-01, 9.9908e-01],\n",
      "        [1.3187e-02, 5.8961e-01, 9.9908e-01],\n",
      "        [1.3187e-02, 5.8961e-01, 9.9908e-01],\n",
      "        [1.3187e-02, 5.8961e-01, 9.9908e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 74000, Loss: 4.140454292297363\n",
      "tensor([[1.0000e+00, 6.8165e-03, 2.4874e-09],\n",
      "        [1.0000e+00, 3.2491e-03, 6.2893e-11],\n",
      "        [9.9998e-01, 6.4113e-02, 2.1425e-04],\n",
      "        [1.2013e-02, 6.1348e-01, 9.9915e-01],\n",
      "        [1.2013e-02, 6.1348e-01, 9.9915e-01],\n",
      "        [1.2013e-02, 6.1348e-01, 9.9915e-01],\n",
      "        [1.2013e-02, 6.1348e-01, 9.9915e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 75000, Loss: 3.7740464210510254\n",
      "tensor([[1.0000e+00, 5.3731e-03, 2.0123e-09],\n",
      "        [1.0000e+00, 1.9925e-03, 1.8662e-11],\n",
      "        [9.9998e-01, 5.8566e-02, 1.9670e-04],\n",
      "        [1.0946e-02, 6.3666e-01, 9.9923e-01],\n",
      "        [1.0946e-02, 6.3666e-01, 9.9923e-01],\n",
      "        [1.0946e-02, 6.3666e-01, 9.9923e-01],\n",
      "        [1.0946e-02, 6.3666e-01, 9.9923e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 76000, Loss: 3.419365882873535\n",
      "tensor([[1.0000e+00, 4.2404e-03, 1.4801e-09],\n",
      "        [1.0000e+00, 1.2278e-03, 5.4681e-12],\n",
      "        [9.9999e-01, 5.3502e-02, 1.7093e-04],\n",
      "        [9.9743e-03, 6.5909e-01, 9.9929e-01],\n",
      "        [9.9743e-03, 6.5909e-01, 9.9929e-01],\n",
      "        [9.9743e-03, 6.5909e-01, 9.9929e-01],\n",
      "        [9.9743e-03, 6.5909e-01, 9.9929e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 77000, Loss: 3.0773563385009766\n",
      "tensor([[1.0000e+00, 3.3417e-03, 1.0198e-09],\n",
      "        [1.0000e+00, 7.6531e-04, 1.6706e-12],\n",
      "        [9.9999e-01, 4.8811e-02, 1.4303e-04],\n",
      "        [9.0899e-03, 6.8070e-01, 9.9935e-01],\n",
      "        [9.0899e-03, 6.8070e-01, 9.9935e-01],\n",
      "        [9.0899e-03, 6.8070e-01, 9.9935e-01],\n",
      "        [9.0899e-03, 6.8070e-01, 9.9935e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 78000, Loss: 2.748767375946045\n",
      "tensor([[1.0000e+00, 2.6246e-03, 6.6795e-10],\n",
      "        [1.0000e+00, 4.8162e-04, 5.3283e-13],\n",
      "        [9.9999e-01, 4.4424e-02, 1.1624e-04],\n",
      "        [8.2856e-03, 7.0145e-01, 9.9940e-01],\n",
      "        [8.2856e-03, 7.0145e-01, 9.9940e-01],\n",
      "        [8.2856e-03, 7.0145e-01, 9.9940e-01],\n",
      "        [8.2856e-03, 7.0145e-01, 9.9940e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 79000, Loss: 2.434072971343994\n",
      "tensor([[1.0000e+00, 2.0509e-03, 4.2252e-10],\n",
      "        [1.0000e+00, 3.0229e-04, 1.7118e-13],\n",
      "        [9.9999e-01, 4.0296e-02, 9.2589e-05],\n",
      "        [7.5519e-03, 7.2132e-01, 9.9945e-01],\n",
      "        [7.5519e-03, 7.2132e-01, 9.9945e-01],\n",
      "        [7.5519e-03, 7.2132e-01, 9.9945e-01],\n",
      "        [7.5519e-03, 7.2132e-01, 9.9945e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 80000, Loss: 2.1338486671447754\n",
      "tensor([[1.0000e+00, 1.5925e-03, 2.5739e-10],\n",
      "        [1.0000e+00, 1.9390e-04, 6.0756e-14],\n",
      "        [9.9999e-01, 3.6405e-02, 7.2162e-05],\n",
      "        [6.8859e-03, 7.4027e-01, 9.9950e-01],\n",
      "        [6.8859e-03, 7.4027e-01, 9.9950e-01],\n",
      "        [6.8859e-03, 7.4027e-01, 9.9950e-01],\n",
      "        [6.8859e-03, 7.4027e-01, 9.9950e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 81000, Loss: 1.8482096195220947\n",
      "tensor([[1.0000e+00, 1.2267e-03, 1.5361e-10],\n",
      "        [1.0000e+00, 1.2365e-04, 2.1774e-14],\n",
      "        [1.0000e+00, 3.2727e-02, 5.5578e-05],\n",
      "        [6.2783e-03, 7.5828e-01, 9.9954e-01],\n",
      "        [6.2783e-03, 7.5828e-01, 9.9954e-01],\n",
      "        [6.2783e-03, 7.5828e-01, 9.9954e-01],\n",
      "        [6.2783e-03, 7.5828e-01, 9.9954e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 82000, Loss: 1.5772643089294434\n",
      "tensor([[1.0000e+00, 9.3619e-04, 8.8065e-11],\n",
      "        [1.0000e+00, 7.8355e-05, 7.6932e-15],\n",
      "        [1.0000e+00, 2.9252e-02, 4.1824e-05],\n",
      "        [5.7247e-03, 7.7536e-01, 9.9958e-01],\n",
      "        [5.7247e-03, 7.7536e-01, 9.9958e-01],\n",
      "        [5.7247e-03, 7.7536e-01, 9.9958e-01],\n",
      "        [5.7247e-03, 7.7536e-01, 9.9958e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 83000, Loss: 1.321082353591919\n",
      "tensor([[1.0000e+00, 7.0641e-04, 4.9036e-11],\n",
      "        [1.0000e+00, 5.1566e-05, 3.2165e-15],\n",
      "        [1.0000e+00, 2.5967e-02, 3.0948e-05],\n",
      "        [5.2215e-03, 7.9150e-01, 9.9961e-01],\n",
      "        [5.2215e-03, 7.9150e-01, 9.9961e-01],\n",
      "        [5.2215e-03, 7.9150e-01, 9.9961e-01],\n",
      "        [5.2215e-03, 7.9150e-01, 9.9961e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 84000, Loss: 1.0793944597244263\n",
      "tensor([[1.0000e+00, 5.2565e-04, 2.7328e-11],\n",
      "        [1.0000e+00, 3.3907e-05, 1.4373e-15],\n",
      "        [1.0000e+00, 2.2859e-02, 2.2912e-05],\n",
      "        [4.7623e-03, 8.0672e-01, 9.9965e-01],\n",
      "        [4.7623e-03, 8.0672e-01, 9.9965e-01],\n",
      "        [4.7623e-03, 8.0672e-01, 9.9965e-01],\n",
      "        [4.7623e-03, 8.0672e-01, 9.9965e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 85000, Loss: 0.8519490957260132\n",
      "tensor([[1.0000e+00, 3.8470e-04, 1.4636e-11],\n",
      "        [1.0000e+00, 2.1958e-05, 6.2383e-16],\n",
      "        [1.0000e+00, 1.9925e-02, 1.6579e-05],\n",
      "        [4.3434e-03, 8.2104e-01, 9.9968e-01],\n",
      "        [4.3434e-03, 8.2104e-01, 9.9968e-01],\n",
      "        [4.3434e-03, 8.2104e-01, 9.9968e-01],\n",
      "        [4.3434e-03, 8.2104e-01, 9.9968e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 86000, Loss: 0.6385107040405273\n",
      "tensor([[1.0000e+00, 2.7587e-04, 7.4231e-12],\n",
      "        [1.0000e+00, 1.4081e-05, 2.6696e-16],\n",
      "        [1.0000e+00, 1.7160e-02, 1.1626e-05],\n",
      "        [3.9630e-03, 8.3447e-01, 9.9970e-01],\n",
      "        [3.9630e-03, 8.3447e-01, 9.9970e-01],\n",
      "        [3.9630e-03, 8.3447e-01, 9.9970e-01],\n",
      "        [3.9630e-03, 8.3447e-01, 9.9970e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 87000, Loss: 0.43855512142181396\n",
      "tensor([[1.0000e+00, 1.9276e-04, 3.5259e-12],\n",
      "        [1.0000e+00, 8.9588e-06, 1.1409e-16],\n",
      "        [1.0000e+00, 1.4555e-02, 7.8501e-06],\n",
      "        [3.6158e-03, 8.4704e-01, 9.9973e-01],\n",
      "        [3.6158e-03, 8.4704e-01, 9.9973e-01],\n",
      "        [3.6158e-03, 8.4704e-01, 9.9973e-01],\n",
      "        [3.6158e-03, 8.4704e-01, 9.9973e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 88000, Loss: 0.2516084313392639\n",
      "tensor([[1.0000e+00, 1.3025e-04, 1.5468e-12],\n",
      "        [1.0000e+00, 5.6615e-06, 4.9353e-17],\n",
      "        [1.0000e+00, 1.2105e-02, 5.0627e-06],\n",
      "        [3.2989e-03, 8.5879e-01, 9.9975e-01],\n",
      "        [3.2989e-03, 8.5879e-01, 9.9975e-01],\n",
      "        [3.2989e-03, 8.5879e-01, 9.9975e-01],\n",
      "        [3.2989e-03, 8.5879e-01, 9.9975e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 89000, Loss: 0.07716387510299683\n",
      "tensor([[1.0000e+00, 8.4082e-05, 6.2135e-13],\n",
      "        [1.0000e+00, 3.8047e-06, 2.7847e-17],\n",
      "        [1.0000e+00, 9.8036e-03, 3.1031e-06],\n",
      "        [3.0098e-03, 8.6974e-01, 9.9977e-01],\n",
      "        [3.0098e-03, 8.6974e-01, 9.9977e-01],\n",
      "        [3.0098e-03, 8.6974e-01, 9.9977e-01],\n",
      "        [3.0098e-03, 8.6974e-01, 9.9977e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 90000, Loss: -0.08529460430145264\n",
      "tensor([[1.0000e+00, 5.0837e-05, 2.3704e-13],\n",
      "        [1.0000e+00, 2.6397e-06, 2.0631e-17],\n",
      "        [1.0000e+00, 7.6431e-03, 1.8462e-06],\n",
      "        [2.7470e-03, 8.7995e-01, 9.9979e-01],\n",
      "        [2.7470e-03, 8.7995e-01, 9.9979e-01],\n",
      "        [2.7470e-03, 8.7995e-01, 9.9979e-01],\n",
      "        [2.7470e-03, 8.7995e-01, 9.9979e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 91000, Loss: -0.23635166883468628\n",
      "tensor([[1.0000e+00, 2.7874e-05, 7.5054e-14],\n",
      "        [1.0000e+00, 1.7876e-06, 1.5689e-17],\n",
      "        [1.0000e+00, 5.6303e-03, 9.8634e-07],\n",
      "        [2.5073e-03, 8.8943e-01, 9.9981e-01],\n",
      "        [2.5073e-03, 8.8943e-01, 9.9981e-01],\n",
      "        [2.5073e-03, 8.8943e-01, 9.9981e-01],\n",
      "        [2.5073e-03, 8.8943e-01, 9.9981e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 92000, Loss: -0.3766637444496155\n",
      "tensor([[1.0000e+00, 1.2869e-05, 1.6116e-14],\n",
      "        [1.0000e+00, 1.1870e-06, 1.2441e-17],\n",
      "        [1.0000e+00, 3.7548e-03, 4.2111e-07],\n",
      "        [2.2884e-03, 8.9823e-01, 9.9982e-01],\n",
      "        [2.2884e-03, 8.9823e-01, 9.9982e-01],\n",
      "        [2.2884e-03, 8.9823e-01, 9.9982e-01],\n",
      "        [2.2884e-03, 8.9823e-01, 9.9982e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 93000, Loss: -0.5068052411079407\n",
      "tensor([[1.0000e+00, 4.1222e-06, 1.3498e-15],\n",
      "        [1.0000e+00, 7.8597e-07, 1.0513e-17],\n",
      "        [1.0000e+00, 2.0272e-03, 1.0444e-07],\n",
      "        [2.0887e-03, 9.0639e-01, 9.9984e-01],\n",
      "        [2.0887e-03, 9.0639e-01, 9.9984e-01],\n",
      "        [2.0887e-03, 9.0639e-01, 9.9984e-01],\n",
      "        [2.0887e-03, 9.0639e-01, 9.9984e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 94000, Loss: -0.6273864507675171\n",
      "tensor([[1.0000e+00, 3.6421e-07, 3.4894e-18],\n",
      "        [1.0000e+00, 5.1493e-07, 9.3690e-18],\n",
      "        [1.0000e+00, 5.2016e-04, 3.4771e-09],\n",
      "        [1.9063e-03, 9.1395e-01, 9.9985e-01],\n",
      "        [1.9063e-03, 9.1395e-01, 9.9985e-01],\n",
      "        [1.9063e-03, 9.1395e-01, 9.9985e-01],\n",
      "        [1.9063e-03, 9.1395e-01, 9.9985e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 95000, Loss: -0.738858163356781\n",
      "tensor([[1.0000e+00, 1.3690e-08, 1.8117e-21],\n",
      "        [1.0000e+00, 2.8428e-07, 7.7473e-18],\n",
      "        [1.0000e+00, 8.1390e-05, 4.5825e-11],\n",
      "        [1.7398e-03, 9.2094e-01, 9.9987e-01],\n",
      "        [1.7398e-03, 9.2094e-01, 9.9987e-01],\n",
      "        [1.7398e-03, 9.2094e-01, 9.9987e-01],\n",
      "        [1.7398e-03, 9.2094e-01, 9.9987e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 96000, Loss: -0.841820240020752\n",
      "tensor([[1.0000e+00, 1.6045e-09, 2.9094e-23],\n",
      "        [1.0000e+00, 1.5700e-07, 6.0365e-18],\n",
      "        [1.0000e+00, 2.4545e-05, 4.3797e-12],\n",
      "        [1.5886e-03, 9.2739e-01, 9.9988e-01],\n",
      "        [1.5886e-03, 9.2739e-01, 9.9988e-01],\n",
      "        [1.5886e-03, 9.2739e-01, 9.9988e-01],\n",
      "        [1.5886e-03, 9.2739e-01, 9.9988e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 97000, Loss: -0.9368665218353271\n",
      "tensor([[1.0000e+00, 2.9569e-10, 1.4373e-24],\n",
      "        [1.0000e+00, 9.3904e-08, 4.6842e-18],\n",
      "        [1.0000e+00, 9.6115e-06, 8.0088e-13],\n",
      "        [1.4505e-03, 9.3335e-01, 9.9989e-01],\n",
      "        [1.4505e-03, 9.3335e-01, 9.9989e-01],\n",
      "        [1.4505e-03, 9.3335e-01, 9.9989e-01],\n",
      "        [1.4505e-03, 9.3335e-01, 9.9989e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 98000, Loss: -1.024487018585205\n",
      "tensor([[1.0000e+00, 6.8051e-11, 1.1951e-25],\n",
      "        [1.0000e+00, 5.9087e-08, 3.6340e-18],\n",
      "        [1.0000e+00, 4.2787e-06, 1.9780e-13],\n",
      "        [1.3244e-03, 9.3884e-01, 9.9990e-01],\n",
      "        [1.3244e-03, 9.3884e-01, 9.9990e-01],\n",
      "        [1.3244e-03, 9.3884e-01, 9.9990e-01],\n",
      "        [1.3244e-03, 9.3884e-01, 9.9990e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "Epoch: 99000, Loss: -1.1052556037902832\n",
      "tensor([[1.0000e+00, 1.7786e-11, 1.3416e-26],\n",
      "        [1.0000e+00, 3.8304e-08, 2.8118e-18],\n",
      "        [1.0000e+00, 2.0498e-06, 5.8097e-14],\n",
      "        [1.2093e-03, 9.4391e-01, 9.9991e-01],\n",
      "        [1.2093e-03, 9.4391e-01, 9.9991e-01],\n",
      "        [1.2093e-03, 9.4391e-01, 9.9991e-01],\n",
      "        [1.2093e-03, 9.4391e-01, 9.9991e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0., 10.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0., 10.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.]])\n",
      "GNN training (n=7) took 114.531\n",
      "GNN final continuous loss: -1.1795485019683838\n",
      "GNN best continuous loss: -1.1795485019683838\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[1, 0, 0],\n         [1, 0, 0],\n         [1, 0, 0],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1]]),\n tensor([[1, 0, 0],\n         [1, 0, 0],\n         [1, 0, 0],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1]]))"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = [(0,1, {\"weight\": 10, \"capacity\":10}),\n",
    "         (1,2, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (3,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (2,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (5,6, {\"weight\": 10, \"capacity\":10}),]\n",
    "graph = CreateDummyFunction(edges)\n",
    "graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "\n",
    "n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=7,patience=10000)\n",
    "# Establish pytorch GNN + optimizer\n",
    "opt_params = {'lr': learning_rate}\n",
    "gnn_hypers = {\n",
    "    'dim_embedding': dim_embedding,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'dropout': 0.0,\n",
    "    'number_classes': 3,\n",
    "    'prob_threshold': PROB_THRESHOLD,\n",
    "    'number_epochs': number_epochs,\n",
    "    'tolerance': tol,\n",
    "    'patience': patience\n",
    "}\n",
    "\n",
    "net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "# For tracking hyperparameters in results object\n",
    "gnn_hypers.update(opt_params)\n",
    "_, epoch, final_bitstring1, best_bitstring1 = run_gnn_training(\n",
    "    q_torch, graph_dgl, net, embed, optimizer, gnn_hypers['number_epochs'],\n",
    "    gnn_hypers['tolerance'], gnn_hypers['patience'], gnn_hypers['prob_threshold'], calculate_H_prime)\n",
    "final_bitstring1, best_bitstring1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1, 0, 0],\n         [1, 0, 0],\n         [1, 0, 0],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1]]),\n tensor([[1, 0, 0],\n         [1, 0, 0],\n         [1, 0, 0],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1],\n         [0, 1, 1]]))"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bitstring1, best_bitstring1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph 2 Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edges = [(0,1, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,2, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,3, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,5, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,6, {\"weight\": 10, \"capacity\":10}),\n",
    "         (1,2, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,4, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,4, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (3,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (3,5, {\"weight\": 10, \"capacity\":10}),\n",
    "         (3,6, {\"weight\": 10, \"capacity\":10}),\n",
    "         (4,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (4,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (5,6, {\"weight\": 10, \"capacity\":10}),]\n",
    "graph = CreateDummyFunction(edges)\n",
    "graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "DrawGraph(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n        [10.,  1.,  1.,  0., 10., 10., 10.],\n        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n        [10.,  1.,  1., 10.,  1.,  0., 10.],\n        [10.,  1.,  1., 10.,  1., 10.,  0.]])"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "q_torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training GCN on graph 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "edges = [(0,1, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,2, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,3, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,5, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,6, {\"weight\": 10, \"capacity\":10}),\n",
    "         (1,2, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,4, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,4, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (3,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (3,5, {\"weight\": 10, \"capacity\":10}),\n",
    "         (3,6, {\"weight\": 10, \"capacity\":10}),\n",
    "         (4,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (4,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (5,6, {\"weight\": 10, \"capacity\":10}),]\n",
    "graph = CreateDummyFunction(edges)\n",
    "graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "\n",
    "n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=7,patience=10000)\n",
    "# Establish pytorch GNN + optimizer\n",
    "opt_params = {'lr': learning_rate}\n",
    "gnn_hypers = {\n",
    "    'dim_embedding': dim_embedding,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'dropout': 0.0,\n",
    "    'number_classes': 3,\n",
    "    'prob_threshold': PROB_THRESHOLD,\n",
    "    'number_epochs': number_epochs,\n",
    "    'tolerance': tol,\n",
    "    'patience': patience\n",
    "}\n",
    "\n",
    "net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "# For tracking hyperparameters in results object\n",
    "gnn_hypers.update(opt_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GNN...\n",
      "Epoch: 0, Loss: 80.58333587646484\n",
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 1000, Loss: 71.67942810058594\n",
      "tensor([[0.5248, 0.5248, 0.5248],\n",
      "        [0.5248, 0.5248, 0.5248],\n",
      "        [0.5248, 0.5248, 0.5248],\n",
      "        [0.5248, 0.5248, 0.5248],\n",
      "        [0.5248, 0.5248, 0.5248],\n",
      "        [0.5248, 0.5248, 0.5248],\n",
      "        [0.5248, 0.5248, 0.5248]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 2000, Loss: 63.22507095336914\n",
      "tensor([[0.5491, 0.5491, 0.5491],\n",
      "        [0.5491, 0.5491, 0.5491],\n",
      "        [0.5491, 0.5491, 0.5491],\n",
      "        [0.5491, 0.5491, 0.5491],\n",
      "        [0.5491, 0.5491, 0.5491],\n",
      "        [0.5491, 0.5491, 0.5491],\n",
      "        [0.5491, 0.5491, 0.5491]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 3000, Loss: 55.21802520751953\n",
      "tensor([[0.5729, 0.5729, 0.5729],\n",
      "        [0.5729, 0.5729, 0.5729],\n",
      "        [0.5729, 0.5729, 0.5729],\n",
      "        [0.5729, 0.5729, 0.5729],\n",
      "        [0.5729, 0.5729, 0.5729],\n",
      "        [0.5729, 0.5729, 0.5729],\n",
      "        [0.5729, 0.5729, 0.5729]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 4000, Loss: 47.653968811035156\n",
      "tensor([[0.5962, 0.5962, 0.5962],\n",
      "        [0.5962, 0.5962, 0.5962],\n",
      "        [0.5962, 0.5962, 0.5962],\n",
      "        [0.5962, 0.5962, 0.5962],\n",
      "        [0.5962, 0.5962, 0.5962],\n",
      "        [0.5962, 0.5962, 0.5962],\n",
      "        [0.5962, 0.5962, 0.5962]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 5000, Loss: 40.52913284301758\n",
      "tensor([[0.6189, 0.6189, 0.6189],\n",
      "        [0.6189, 0.6189, 0.6189],\n",
      "        [0.6189, 0.6189, 0.6189],\n",
      "        [0.6189, 0.6189, 0.6189],\n",
      "        [0.6189, 0.6189, 0.6189],\n",
      "        [0.6189, 0.6189, 0.6189],\n",
      "        [0.6189, 0.6189, 0.6189]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 6000, Loss: 33.83966827392578\n",
      "tensor([[0.6410, 0.6410, 0.6410],\n",
      "        [0.6410, 0.6410, 0.6410],\n",
      "        [0.6410, 0.6410, 0.6410],\n",
      "        [0.6410, 0.6410, 0.6410],\n",
      "        [0.6410, 0.6410, 0.6410],\n",
      "        [0.6410, 0.6410, 0.6410],\n",
      "        [0.6410, 0.6410, 0.6410]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 7000, Loss: 27.580307006835938\n",
      "tensor([[0.6623, 0.6623, 0.6623],\n",
      "        [0.6623, 0.6623, 0.6623],\n",
      "        [0.6623, 0.6623, 0.6623],\n",
      "        [0.6623, 0.6623, 0.6623],\n",
      "        [0.6623, 0.6623, 0.6623],\n",
      "        [0.6623, 0.6623, 0.6623],\n",
      "        [0.6623, 0.6623, 0.6623]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 8000, Loss: 21.742874145507812\n",
      "tensor([[0.6829, 0.6829, 0.6829],\n",
      "        [0.6829, 0.6829, 0.6829],\n",
      "        [0.6829, 0.6829, 0.6829],\n",
      "        [0.6829, 0.6829, 0.6829],\n",
      "        [0.6829, 0.6829, 0.6829],\n",
      "        [0.6829, 0.6829, 0.6829],\n",
      "        [0.6829, 0.6829, 0.6829]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 9000, Loss: 16.316619873046875\n",
      "tensor([[0.7028, 0.7028, 0.7028],\n",
      "        [0.7028, 0.7028, 0.7028],\n",
      "        [0.7028, 0.7028, 0.7028],\n",
      "        [0.7028, 0.7028, 0.7028],\n",
      "        [0.7028, 0.7028, 0.7028],\n",
      "        [0.7028, 0.7028, 0.7028],\n",
      "        [0.7028, 0.7028, 0.7028]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 10000, Loss: 11.287429809570312\n",
      "tensor([[0.7218, 0.7218, 0.7218],\n",
      "        [0.7218, 0.7218, 0.7218],\n",
      "        [0.7218, 0.7218, 0.7218],\n",
      "        [0.7218, 0.7218, 0.7218],\n",
      "        [0.7218, 0.7218, 0.7218],\n",
      "        [0.7218, 0.7218, 0.7218],\n",
      "        [0.7218, 0.7218, 0.7218]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 11000, Loss: 6.6390228271484375\n",
      "tensor([[0.7400, 0.7400, 0.7400],\n",
      "        [0.7400, 0.7400, 0.7400],\n",
      "        [0.7400, 0.7400, 0.7400],\n",
      "        [0.7400, 0.7400, 0.7400],\n",
      "        [0.7400, 0.7400, 0.7400],\n",
      "        [0.7400, 0.7400, 0.7400],\n",
      "        [0.7400, 0.7400, 0.7400]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 12000, Loss: 2.3536529541015625\n",
      "tensor([[0.7573, 0.7573, 0.7573],\n",
      "        [0.7573, 0.7573, 0.7573],\n",
      "        [0.7573, 0.7573, 0.7573],\n",
      "        [0.7573, 0.7573, 0.7573],\n",
      "        [0.7573, 0.7573, 0.7573],\n",
      "        [0.7573, 0.7573, 0.7573],\n",
      "        [0.7573, 0.7573, 0.7573]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 13000, Loss: -1.588836669921875\n",
      "tensor([[0.7738, 0.7738, 0.7738],\n",
      "        [0.7738, 0.7738, 0.7738],\n",
      "        [0.7738, 0.7738, 0.7738],\n",
      "        [0.7738, 0.7738, 0.7738],\n",
      "        [0.7738, 0.7738, 0.7738],\n",
      "        [0.7738, 0.7738, 0.7738],\n",
      "        [0.7738, 0.7738, 0.7738]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 14000, Loss: -5.20843505859375\n",
      "tensor([[0.7894, 0.7894, 0.7894],\n",
      "        [0.7894, 0.7894, 0.7894],\n",
      "        [0.7894, 0.7894, 0.7894],\n",
      "        [0.7894, 0.7894, 0.7894],\n",
      "        [0.7894, 0.7894, 0.7894],\n",
      "        [0.7894, 0.7894, 0.7894],\n",
      "        [0.7894, 0.7894, 0.7894]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 15000, Loss: -8.52569580078125\n",
      "tensor([[0.8042, 0.8042, 0.8042],\n",
      "        [0.8042, 0.8042, 0.8042],\n",
      "        [0.8042, 0.8042, 0.8042],\n",
      "        [0.8042, 0.8042, 0.8042],\n",
      "        [0.8042, 0.8042, 0.8042],\n",
      "        [0.8042, 0.8042, 0.8042],\n",
      "        [0.8042, 0.8042, 0.8042]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 16000, Loss: -11.561492919921875\n",
      "tensor([[0.8182, 0.8182, 0.8182],\n",
      "        [0.8182, 0.8182, 0.8182],\n",
      "        [0.8182, 0.8182, 0.8182],\n",
      "        [0.8182, 0.8182, 0.8182],\n",
      "        [0.8182, 0.8182, 0.8182],\n",
      "        [0.8182, 0.8182, 0.8182],\n",
      "        [0.8182, 0.8182, 0.8182]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 17000, Loss: -14.336334228515625\n",
      "tensor([[0.8313, 0.8313, 0.8313],\n",
      "        [0.8313, 0.8313, 0.8313],\n",
      "        [0.8313, 0.8313, 0.8313],\n",
      "        [0.8313, 0.8313, 0.8313],\n",
      "        [0.8313, 0.8313, 0.8313],\n",
      "        [0.8313, 0.8313, 0.8313],\n",
      "        [0.8313, 0.8313, 0.8313]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 18000, Loss: -16.869476318359375\n",
      "tensor([[0.8437, 0.8437, 0.8437],\n",
      "        [0.8437, 0.8437, 0.8437],\n",
      "        [0.8437, 0.8437, 0.8437],\n",
      "        [0.8437, 0.8437, 0.8437],\n",
      "        [0.8437, 0.8437, 0.8437],\n",
      "        [0.8437, 0.8437, 0.8437],\n",
      "        [0.8437, 0.8437, 0.8437]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 19000, Loss: -19.179840087890625\n",
      "tensor([[0.8553, 0.8553, 0.8553],\n",
      "        [0.8553, 0.8553, 0.8553],\n",
      "        [0.8553, 0.8553, 0.8553],\n",
      "        [0.8553, 0.8553, 0.8553],\n",
      "        [0.8553, 0.8553, 0.8553],\n",
      "        [0.8553, 0.8553, 0.8553],\n",
      "        [0.8553, 0.8553, 0.8553]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 20000, Loss: -21.28564453125\n",
      "tensor([[0.8661, 0.8661, 0.8661],\n",
      "        [0.8661, 0.8661, 0.8661],\n",
      "        [0.8661, 0.8661, 0.8661],\n",
      "        [0.8661, 0.8661, 0.8661],\n",
      "        [0.8661, 0.8661, 0.8661],\n",
      "        [0.8661, 0.8661, 0.8661],\n",
      "        [0.8661, 0.8661, 0.8661]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 21000, Loss: -23.20391845703125\n",
      "tensor([[0.8763, 0.8763, 0.8763],\n",
      "        [0.8763, 0.8763, 0.8763],\n",
      "        [0.8763, 0.8763, 0.8763],\n",
      "        [0.8763, 0.8763, 0.8763],\n",
      "        [0.8763, 0.8763, 0.8763],\n",
      "        [0.8763, 0.8763, 0.8763],\n",
      "        [0.8763, 0.8763, 0.8763]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 22000, Loss: -24.949615478515625\n",
      "tensor([[0.8858, 0.8858, 0.8858],\n",
      "        [0.8858, 0.8858, 0.8858],\n",
      "        [0.8858, 0.8858, 0.8858],\n",
      "        [0.8858, 0.8858, 0.8858],\n",
      "        [0.8858, 0.8858, 0.8858],\n",
      "        [0.8858, 0.8858, 0.8858],\n",
      "        [0.8858, 0.8858, 0.8858]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 23000, Loss: -26.5396728515625\n",
      "tensor([[0.8946, 0.8946, 0.8946],\n",
      "        [0.8946, 0.8946, 0.8946],\n",
      "        [0.8946, 0.8946, 0.8946],\n",
      "        [0.8946, 0.8946, 0.8946],\n",
      "        [0.8946, 0.8946, 0.8946],\n",
      "        [0.8946, 0.8946, 0.8946],\n",
      "        [0.8946, 0.8946, 0.8946]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 24000, Loss: -27.985260009765625\n",
      "tensor([[0.9028, 0.9028, 0.9028],\n",
      "        [0.9028, 0.9028, 0.9028],\n",
      "        [0.9028, 0.9028, 0.9028],\n",
      "        [0.9028, 0.9028, 0.9028],\n",
      "        [0.9028, 0.9028, 0.9028],\n",
      "        [0.9028, 0.9028, 0.9028],\n",
      "        [0.9028, 0.9028, 0.9028]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 25000, Loss: -29.299041748046875\n",
      "tensor([[0.9104, 0.9104, 0.9104],\n",
      "        [0.9104, 0.9104, 0.9104],\n",
      "        [0.9104, 0.9104, 0.9104],\n",
      "        [0.9104, 0.9104, 0.9104],\n",
      "        [0.9104, 0.9104, 0.9104],\n",
      "        [0.9104, 0.9104, 0.9104],\n",
      "        [0.9104, 0.9104, 0.9104]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 26000, Loss: -30.494537353515625\n",
      "tensor([[0.9175, 0.9175, 0.9175],\n",
      "        [0.9175, 0.9175, 0.9175],\n",
      "        [0.9175, 0.9175, 0.9175],\n",
      "        [0.9175, 0.9175, 0.9175],\n",
      "        [0.9175, 0.9175, 0.9175],\n",
      "        [0.9175, 0.9175, 0.9175],\n",
      "        [0.9175, 0.9175, 0.9175]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 27000, Loss: -31.58209228515625\n",
      "tensor([[0.9241, 0.9241, 0.9241],\n",
      "        [0.9241, 0.9241, 0.9241],\n",
      "        [0.9241, 0.9241, 0.9241],\n",
      "        [0.9241, 0.9241, 0.9241],\n",
      "        [0.9241, 0.9241, 0.9241],\n",
      "        [0.9241, 0.9241, 0.9241],\n",
      "        [0.9241, 0.9241, 0.9241]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 28000, Loss: -32.571441650390625\n",
      "tensor([[0.9302, 0.9302, 0.9302],\n",
      "        [0.9302, 0.9302, 0.9302],\n",
      "        [0.9302, 0.9302, 0.9302],\n",
      "        [0.9302, 0.9302, 0.9302],\n",
      "        [0.9302, 0.9302, 0.9302],\n",
      "        [0.9302, 0.9302, 0.9302],\n",
      "        [0.9302, 0.9302, 0.9302]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 29000, Loss: -33.47125244140625\n",
      "tensor([[0.9358, 0.9358, 0.9358],\n",
      "        [0.9358, 0.9358, 0.9358],\n",
      "        [0.9358, 0.9358, 0.9358],\n",
      "        [0.9358, 0.9358, 0.9358],\n",
      "        [0.9358, 0.9358, 0.9358],\n",
      "        [0.9358, 0.9358, 0.9358],\n",
      "        [0.9358, 0.9358, 0.9358]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 30000, Loss: -34.28985595703125\n",
      "tensor([[0.9410, 0.9410, 0.9410],\n",
      "        [0.9410, 0.9410, 0.9410],\n",
      "        [0.9410, 0.9410, 0.9410],\n",
      "        [0.9410, 0.9410, 0.9410],\n",
      "        [0.9410, 0.9410, 0.9410],\n",
      "        [0.9410, 0.9410, 0.9410],\n",
      "        [0.9410, 0.9410, 0.9410]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 31000, Loss: -35.034332275390625\n",
      "tensor([[0.9458, 0.9458, 0.9458],\n",
      "        [0.9458, 0.9458, 0.9458],\n",
      "        [0.9458, 0.9458, 0.9458],\n",
      "        [0.9458, 0.9458, 0.9458],\n",
      "        [0.9458, 0.9458, 0.9458],\n",
      "        [0.9458, 0.9458, 0.9458],\n",
      "        [0.9458, 0.9458, 0.9458]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 32000, Loss: -35.711761474609375\n",
      "tensor([[0.9502, 0.9502, 0.9502],\n",
      "        [0.9502, 0.9502, 0.9502],\n",
      "        [0.9502, 0.9502, 0.9502],\n",
      "        [0.9502, 0.9502, 0.9502],\n",
      "        [0.9502, 0.9502, 0.9502],\n",
      "        [0.9502, 0.9502, 0.9502],\n",
      "        [0.9502, 0.9502, 0.9502]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 33000, Loss: -36.327972412109375\n",
      "tensor([[0.9543, 0.9543, 0.9543],\n",
      "        [0.9543, 0.9543, 0.9543],\n",
      "        [0.9543, 0.9543, 0.9543],\n",
      "        [0.9543, 0.9543, 0.9543],\n",
      "        [0.9543, 0.9543, 0.9543],\n",
      "        [0.9543, 0.9543, 0.9543],\n",
      "        [0.9543, 0.9543, 0.9543]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 34000, Loss: -36.888885498046875\n",
      "tensor([[0.9581, 0.9581, 0.9581],\n",
      "        [0.9581, 0.9581, 0.9581],\n",
      "        [0.9581, 0.9581, 0.9581],\n",
      "        [0.9581, 0.9581, 0.9581],\n",
      "        [0.9581, 0.9581, 0.9581],\n",
      "        [0.9581, 0.9581, 0.9581],\n",
      "        [0.9581, 0.9581, 0.9581]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 35000, Loss: -37.399261474609375\n",
      "tensor([[0.9616, 0.9616, 0.9616],\n",
      "        [0.9616, 0.9616, 0.9616],\n",
      "        [0.9616, 0.9616, 0.9616],\n",
      "        [0.9616, 0.9616, 0.9616],\n",
      "        [0.9616, 0.9616, 0.9616],\n",
      "        [0.9616, 0.9616, 0.9616],\n",
      "        [0.9616, 0.9616, 0.9616]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 36000, Loss: -37.8638916015625\n",
      "tensor([[0.9648, 0.9648, 0.9648],\n",
      "        [0.9648, 0.9648, 0.9648],\n",
      "        [0.9648, 0.9648, 0.9648],\n",
      "        [0.9648, 0.9648, 0.9648],\n",
      "        [0.9648, 0.9648, 0.9648],\n",
      "        [0.9648, 0.9648, 0.9648],\n",
      "        [0.9648, 0.9648, 0.9648]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 37000, Loss: -38.286865234375\n",
      "tensor([[0.9677, 0.9677, 0.9677],\n",
      "        [0.9677, 0.9677, 0.9677],\n",
      "        [0.9677, 0.9677, 0.9677],\n",
      "        [0.9677, 0.9677, 0.9677],\n",
      "        [0.9677, 0.9677, 0.9677],\n",
      "        [0.9677, 0.9677, 0.9677],\n",
      "        [0.9677, 0.9677, 0.9677]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 38000, Loss: -38.67205810546875\n",
      "tensor([[0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704],\n",
      "        [0.9704, 0.9704, 0.9704]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 39000, Loss: -39.023590087890625\n",
      "tensor([[0.9729, 0.9729, 0.9729],\n",
      "        [0.9729, 0.9729, 0.9729],\n",
      "        [0.9729, 0.9729, 0.9729],\n",
      "        [0.9729, 0.9729, 0.9729],\n",
      "        [0.9729, 0.9729, 0.9729],\n",
      "        [0.9729, 0.9729, 0.9729],\n",
      "        [0.9729, 0.9729, 0.9729]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 40000, Loss: -39.34375\n",
      "tensor([[0.9752, 0.9752, 0.9752],\n",
      "        [0.9752, 0.9752, 0.9752],\n",
      "        [0.9752, 0.9752, 0.9752],\n",
      "        [0.9752, 0.9752, 0.9752],\n",
      "        [0.9752, 0.9752, 0.9752],\n",
      "        [0.9752, 0.9752, 0.9752],\n",
      "        [0.9752, 0.9752, 0.9752]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 41000, Loss: -39.635711669921875\n",
      "tensor([[0.9773, 0.9773, 0.9773],\n",
      "        [0.9773, 0.9773, 0.9773],\n",
      "        [0.9773, 0.9773, 0.9773],\n",
      "        [0.9773, 0.9773, 0.9773],\n",
      "        [0.9773, 0.9773, 0.9773],\n",
      "        [0.9773, 0.9773, 0.9773],\n",
      "        [0.9773, 0.9773, 0.9773]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 42000, Loss: -39.90167236328125\n",
      "tensor([[0.9792, 0.9792, 0.9792],\n",
      "        [0.9792, 0.9792, 0.9792],\n",
      "        [0.9792, 0.9792, 0.9792],\n",
      "        [0.9792, 0.9792, 0.9792],\n",
      "        [0.9792, 0.9792, 0.9792],\n",
      "        [0.9792, 0.9792, 0.9792],\n",
      "        [0.9792, 0.9792, 0.9792]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 43000, Loss: -40.1439208984375\n",
      "tensor([[0.9810, 0.9810, 0.9810],\n",
      "        [0.9810, 0.9810, 0.9810],\n",
      "        [0.9810, 0.9810, 0.9810],\n",
      "        [0.9810, 0.9810, 0.9810],\n",
      "        [0.9810, 0.9810, 0.9810],\n",
      "        [0.9810, 0.9810, 0.9810],\n",
      "        [0.9810, 0.9810, 0.9810]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 44000, Loss: -40.364654541015625\n",
      "tensor([[0.9826, 0.9826, 0.9826],\n",
      "        [0.9826, 0.9826, 0.9826],\n",
      "        [0.9826, 0.9826, 0.9826],\n",
      "        [0.9826, 0.9826, 0.9826],\n",
      "        [0.9826, 0.9826, 0.9826],\n",
      "        [0.9826, 0.9826, 0.9826],\n",
      "        [0.9826, 0.9826, 0.9826]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 45000, Loss: -40.565399169921875\n",
      "tensor([[0.9841, 0.9841, 0.9841],\n",
      "        [0.9841, 0.9841, 0.9841],\n",
      "        [0.9841, 0.9841, 0.9841],\n",
      "        [0.9841, 0.9841, 0.9841],\n",
      "        [0.9841, 0.9841, 0.9841],\n",
      "        [0.9841, 0.9841, 0.9841],\n",
      "        [0.9841, 0.9841, 0.9841]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 46000, Loss: -40.7493896484375\n",
      "tensor([[0.9854, 0.9854, 0.9854],\n",
      "        [0.9854, 0.9854, 0.9854],\n",
      "        [0.9854, 0.9854, 0.9854],\n",
      "        [0.9854, 0.9854, 0.9854],\n",
      "        [0.9854, 0.9854, 0.9854],\n",
      "        [0.9854, 0.9854, 0.9854],\n",
      "        [0.9854, 0.9854, 0.9854]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 47000, Loss: -40.91717529296875\n",
      "tensor([[0.9867, 0.9867, 0.9867],\n",
      "        [0.9867, 0.9867, 0.9867],\n",
      "        [0.9867, 0.9867, 0.9867],\n",
      "        [0.9867, 0.9867, 0.9867],\n",
      "        [0.9867, 0.9867, 0.9867],\n",
      "        [0.9867, 0.9867, 0.9867],\n",
      "        [0.9867, 0.9867, 0.9867]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 48000, Loss: -41.07012939453125\n",
      "tensor([[0.9878, 0.9878, 0.9878],\n",
      "        [0.9878, 0.9878, 0.9878],\n",
      "        [0.9878, 0.9878, 0.9878],\n",
      "        [0.9878, 0.9878, 0.9878],\n",
      "        [0.9878, 0.9878, 0.9878],\n",
      "        [0.9878, 0.9878, 0.9878],\n",
      "        [0.9878, 0.9878, 0.9878]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 49000, Loss: -41.20965576171875\n",
      "tensor([[0.9888, 0.9888, 0.9888],\n",
      "        [0.9888, 0.9888, 0.9888],\n",
      "        [0.9888, 0.9888, 0.9888],\n",
      "        [0.9888, 0.9888, 0.9888],\n",
      "        [0.9888, 0.9888, 0.9888],\n",
      "        [0.9888, 0.9888, 0.9888],\n",
      "        [0.9888, 0.9888, 0.9888]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 50000, Loss: -41.336822509765625\n",
      "tensor([[0.9898, 0.9898, 0.9898],\n",
      "        [0.9898, 0.9898, 0.9898],\n",
      "        [0.9898, 0.9898, 0.9898],\n",
      "        [0.9898, 0.9898, 0.9898],\n",
      "        [0.9898, 0.9898, 0.9898],\n",
      "        [0.9898, 0.9898, 0.9898],\n",
      "        [0.9898, 0.9898, 0.9898]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 51000, Loss: -41.452880859375\n",
      "tensor([[0.9907, 0.9907, 0.9907],\n",
      "        [0.9907, 0.9907, 0.9907],\n",
      "        [0.9907, 0.9907, 0.9907],\n",
      "        [0.9907, 0.9907, 0.9907],\n",
      "        [0.9907, 0.9907, 0.9907],\n",
      "        [0.9907, 0.9907, 0.9907],\n",
      "        [0.9907, 0.9907, 0.9907]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 52000, Loss: -41.558746337890625\n",
      "tensor([[0.9915, 0.9915, 0.9915],\n",
      "        [0.9915, 0.9915, 0.9915],\n",
      "        [0.9915, 0.9915, 0.9915],\n",
      "        [0.9915, 0.9915, 0.9915],\n",
      "        [0.9915, 0.9915, 0.9915],\n",
      "        [0.9915, 0.9915, 0.9915],\n",
      "        [0.9915, 0.9915, 0.9915]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 53000, Loss: -41.6552734375\n",
      "tensor([[0.9922, 0.9922, 0.9922],\n",
      "        [0.9922, 0.9922, 0.9922],\n",
      "        [0.9922, 0.9922, 0.9922],\n",
      "        [0.9922, 0.9922, 0.9922],\n",
      "        [0.9922, 0.9922, 0.9922],\n",
      "        [0.9922, 0.9922, 0.9922],\n",
      "        [0.9922, 0.9922, 0.9922]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 54000, Loss: -41.743438720703125\n",
      "tensor([[0.9929, 0.9929, 0.9929],\n",
      "        [0.9929, 0.9929, 0.9929],\n",
      "        [0.9929, 0.9929, 0.9929],\n",
      "        [0.9929, 0.9929, 0.9929],\n",
      "        [0.9929, 0.9929, 0.9929],\n",
      "        [0.9929, 0.9929, 0.9929],\n",
      "        [0.9929, 0.9929, 0.9929]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 55000, Loss: -41.82379150390625\n",
      "tensor([[0.9935, 0.9935, 0.9935],\n",
      "        [0.9935, 0.9935, 0.9935],\n",
      "        [0.9935, 0.9935, 0.9935],\n",
      "        [0.9935, 0.9935, 0.9935],\n",
      "        [0.9935, 0.9935, 0.9935],\n",
      "        [0.9935, 0.9935, 0.9935],\n",
      "        [0.9935, 0.9935, 0.9935]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 56000, Loss: -41.897186279296875\n",
      "tensor([[0.9941, 0.9941, 0.9941],\n",
      "        [0.9941, 0.9941, 0.9941],\n",
      "        [0.9941, 0.9941, 0.9941],\n",
      "        [0.9941, 0.9941, 0.9941],\n",
      "        [0.9941, 0.9941, 0.9941],\n",
      "        [0.9941, 0.9941, 0.9941],\n",
      "        [0.9941, 0.9941, 0.9941]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 57000, Loss: -41.96405029296875\n",
      "tensor([[0.9946, 0.9946, 0.9946],\n",
      "        [0.9946, 0.9946, 0.9946],\n",
      "        [0.9946, 0.9946, 0.9946],\n",
      "        [0.9946, 0.9946, 0.9946],\n",
      "        [0.9946, 0.9946, 0.9946],\n",
      "        [0.9946, 0.9946, 0.9946],\n",
      "        [0.9946, 0.9946, 0.9946]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 58000, Loss: -42.025146484375\n",
      "tensor([[0.9950, 0.9950, 0.9950],\n",
      "        [0.9950, 0.9950, 0.9950],\n",
      "        [0.9950, 0.9950, 0.9950],\n",
      "        [0.9950, 0.9950, 0.9950],\n",
      "        [0.9950, 0.9950, 0.9950],\n",
      "        [0.9950, 0.9950, 0.9950],\n",
      "        [0.9950, 0.9950, 0.9950]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 59000, Loss: -42.080902099609375\n",
      "tensor([[0.9955, 0.9955, 0.9955],\n",
      "        [0.9955, 0.9955, 0.9955],\n",
      "        [0.9955, 0.9955, 0.9955],\n",
      "        [0.9955, 0.9955, 0.9955],\n",
      "        [0.9955, 0.9955, 0.9955],\n",
      "        [0.9955, 0.9955, 0.9955],\n",
      "        [0.9955, 0.9955, 0.9955]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 60000, Loss: -42.131866455078125\n",
      "tensor([[0.9959, 0.9959, 0.9959],\n",
      "        [0.9959, 0.9959, 0.9959],\n",
      "        [0.9959, 0.9959, 0.9959],\n",
      "        [0.9959, 0.9959, 0.9959],\n",
      "        [0.9959, 0.9959, 0.9959],\n",
      "        [0.9959, 0.9959, 0.9959],\n",
      "        [0.9959, 0.9959, 0.9959]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 61000, Loss: -42.178314208984375\n",
      "tensor([[0.9962, 0.9962, 0.9962],\n",
      "        [0.9962, 0.9962, 0.9962],\n",
      "        [0.9962, 0.9962, 0.9962],\n",
      "        [0.9962, 0.9962, 0.9962],\n",
      "        [0.9962, 0.9962, 0.9962],\n",
      "        [0.9962, 0.9962, 0.9962],\n",
      "        [0.9962, 0.9962, 0.9962]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 62000, Loss: -42.2208251953125\n",
      "tensor([[0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 63000, Loss: -42.259552001953125\n",
      "tensor([[0.9968, 0.9968, 0.9968],\n",
      "        [0.9968, 0.9968, 0.9968],\n",
      "        [0.9968, 0.9968, 0.9968],\n",
      "        [0.9968, 0.9968, 0.9968],\n",
      "        [0.9968, 0.9968, 0.9968],\n",
      "        [0.9968, 0.9968, 0.9968],\n",
      "        [0.9968, 0.9968, 0.9968]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 64000, Loss: -42.294830322265625\n",
      "tensor([[0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 0.9971, 0.9971]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 65000, Loss: -42.32708740234375\n",
      "tensor([[0.9974, 0.9974, 0.9974],\n",
      "        [0.9974, 0.9974, 0.9974],\n",
      "        [0.9974, 0.9974, 0.9974],\n",
      "        [0.9974, 0.9974, 0.9974],\n",
      "        [0.9974, 0.9974, 0.9974],\n",
      "        [0.9974, 0.9974, 0.9974],\n",
      "        [0.9974, 0.9974, 0.9974]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 66000, Loss: -42.3565673828125\n",
      "tensor([[0.9976, 0.9976, 0.9976],\n",
      "        [0.9976, 0.9976, 0.9976],\n",
      "        [0.9976, 0.9976, 0.9976],\n",
      "        [0.9976, 0.9976, 0.9976],\n",
      "        [0.9976, 0.9976, 0.9976],\n",
      "        [0.9976, 0.9976, 0.9976],\n",
      "        [0.9976, 0.9976, 0.9976]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 67000, Loss: -42.38348388671875\n",
      "tensor([[0.9978, 0.9978, 0.9978],\n",
      "        [0.9978, 0.9978, 0.9978],\n",
      "        [0.9978, 0.9978, 0.9978],\n",
      "        [0.9978, 0.9978, 0.9978],\n",
      "        [0.9978, 0.9978, 0.9978],\n",
      "        [0.9978, 0.9978, 0.9978],\n",
      "        [0.9978, 0.9978, 0.9978]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 68000, Loss: -42.408050537109375\n",
      "tensor([[0.9980, 0.9980, 0.9980],\n",
      "        [0.9980, 0.9980, 0.9980],\n",
      "        [0.9980, 0.9980, 0.9980],\n",
      "        [0.9980, 0.9980, 0.9980],\n",
      "        [0.9980, 0.9980, 0.9980],\n",
      "        [0.9980, 0.9980, 0.9980],\n",
      "        [0.9980, 0.9980, 0.9980]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 69000, Loss: -42.43048095703125\n",
      "tensor([[0.9982, 0.9982, 0.9982],\n",
      "        [0.9982, 0.9982, 0.9982],\n",
      "        [0.9982, 0.9982, 0.9982],\n",
      "        [0.9982, 0.9982, 0.9982],\n",
      "        [0.9982, 0.9982, 0.9982],\n",
      "        [0.9982, 0.9982, 0.9982],\n",
      "        [0.9982, 0.9982, 0.9982]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 70000, Loss: -42.450897216796875\n",
      "tensor([[0.9983, 0.9983, 0.9983],\n",
      "        [0.9983, 0.9983, 0.9983],\n",
      "        [0.9983, 0.9983, 0.9983],\n",
      "        [0.9983, 0.9983, 0.9983],\n",
      "        [0.9983, 0.9983, 0.9983],\n",
      "        [0.9983, 0.9983, 0.9983],\n",
      "        [0.9983, 0.9983, 0.9983]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 71000, Loss: -42.469696044921875\n",
      "tensor([[0.9985, 0.9985, 0.9985],\n",
      "        [0.9985, 0.9985, 0.9985],\n",
      "        [0.9985, 0.9985, 0.9985],\n",
      "        [0.9985, 0.9985, 0.9985],\n",
      "        [0.9985, 0.9985, 0.9985],\n",
      "        [0.9985, 0.9985, 0.9985],\n",
      "        [0.9985, 0.9985, 0.9985]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 72000, Loss: -42.48681640625\n",
      "tensor([[0.9986, 0.9986, 0.9986],\n",
      "        [0.9986, 0.9986, 0.9986],\n",
      "        [0.9986, 0.9986, 0.9986],\n",
      "        [0.9986, 0.9986, 0.9986],\n",
      "        [0.9986, 0.9986, 0.9986],\n",
      "        [0.9986, 0.9986, 0.9986],\n",
      "        [0.9986, 0.9986, 0.9986]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 73000, Loss: -42.50244140625\n",
      "tensor([[0.9987, 0.9987, 0.9987],\n",
      "        [0.9987, 0.9987, 0.9987],\n",
      "        [0.9987, 0.9987, 0.9987],\n",
      "        [0.9987, 0.9987, 0.9987],\n",
      "        [0.9987, 0.9987, 0.9987],\n",
      "        [0.9987, 0.9987, 0.9987],\n",
      "        [0.9987, 0.9987, 0.9987]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 74000, Loss: -42.5166015625\n",
      "tensor([[0.9988, 0.9988, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 75000, Loss: -42.52960205078125\n",
      "tensor([[0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 0.9989, 0.9989]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 76000, Loss: -42.54150390625\n",
      "tensor([[0.9990, 0.9990, 0.9990],\n",
      "        [0.9990, 0.9990, 0.9990],\n",
      "        [0.9990, 0.9990, 0.9990],\n",
      "        [0.9990, 0.9990, 0.9990],\n",
      "        [0.9990, 0.9990, 0.9990],\n",
      "        [0.9990, 0.9990, 0.9990],\n",
      "        [0.9990, 0.9990, 0.9990]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 77000, Loss: -42.552337646484375\n",
      "tensor([[0.9991, 0.9991, 0.9991],\n",
      "        [0.9991, 0.9991, 0.9991],\n",
      "        [0.9991, 0.9991, 0.9991],\n",
      "        [0.9991, 0.9991, 0.9991],\n",
      "        [0.9991, 0.9991, 0.9991],\n",
      "        [0.9991, 0.9991, 0.9991],\n",
      "        [0.9991, 0.9991, 0.9991]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 78000, Loss: -42.56219482421875\n",
      "tensor([[0.9992, 0.9992, 0.9992],\n",
      "        [0.9992, 0.9992, 0.9992],\n",
      "        [0.9992, 0.9992, 0.9992],\n",
      "        [0.9992, 0.9992, 0.9992],\n",
      "        [0.9992, 0.9992, 0.9992],\n",
      "        [0.9992, 0.9992, 0.9992],\n",
      "        [0.9992, 0.9992, 0.9992]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 79000, Loss: -42.571319580078125\n",
      "tensor([[0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 80000, Loss: -42.57958984375\n",
      "tensor([[0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9993]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 81000, Loss: -42.58709716796875\n",
      "tensor([[0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 82000, Loss: -42.593994140625\n",
      "tensor([[0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994],\n",
      "        [0.9994, 0.9994, 0.9994]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 83000, Loss: -42.600341796875\n",
      "tensor([[0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 84000, Loss: -42.6060791015625\n",
      "tensor([[0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995],\n",
      "        [0.9995, 0.9995, 0.9995]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 85000, Loss: -42.61138916015625\n",
      "tensor([[0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 86000, Loss: -42.616119384765625\n",
      "tensor([[0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 87000, Loss: -42.620513916015625\n",
      "tensor([[0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 0.9996, 0.9996]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 88000, Loss: -42.62445068359375\n",
      "tensor([[0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 89000, Loss: -42.628173828125\n",
      "tensor([[0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 90000, Loss: -42.631500244140625\n",
      "tensor([[0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 91000, Loss: -42.634521484375\n",
      "tensor([[0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997],\n",
      "        [0.9997, 0.9997, 0.9997]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 92000, Loss: -42.63739013671875\n",
      "tensor([[0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 93000, Loss: -42.63983154296875\n",
      "tensor([[0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 94000, Loss: -42.64215087890625\n",
      "tensor([[0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 95000, Loss: -42.64422607421875\n",
      "tensor([[0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 96000, Loss: -42.64617919921875\n",
      "tensor([[0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 0.9998, 0.9998]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 97000, Loss: -42.64794921875\n",
      "tensor([[0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 98000, Loss: -42.649627685546875\n",
      "tensor([[0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 99000, Loss: -42.651092529296875\n",
      "tensor([[0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 0.9999, 0.9999]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "GNN training (n=7) took 111.985\n",
      "GNN final continuous loss: -42.652435302734375\n",
      "GNN best continuous loss: -42.652435302734375\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[99], line 8\u001B[0m\n\u001B[1;32m      2\u001B[0m gnn_start \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m      4\u001B[0m _, epoch, final_bitstring, best_bitstring \u001B[38;5;241m=\u001B[39m run_gnn_training(\n\u001B[1;32m      5\u001B[0m     q_torch, graph_dgl, net, embed, optimizer, gnn_hypers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      6\u001B[0m     gnn_hypers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtolerance\u001B[39m\u001B[38;5;124m'\u001B[39m], gnn_hypers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpatience\u001B[39m\u001B[38;5;124m'\u001B[39m], gnn_hypers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprob_threshold\u001B[39m\u001B[38;5;124m'\u001B[39m], calculate_H)\n\u001B[0;32m----> 8\u001B[0m calc \u001B[38;5;241m=\u001B[39m \u001B[43mcalculateMinCut\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_torch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_bitstring\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m gnn_time \u001B[38;5;241m=\u001B[39m time() \u001B[38;5;241m-\u001B[39m gnn_start\n\u001B[1;32m     10\u001B[0m final_bitstring, best_bitstring\n",
      "Cell \u001B[0;32mIn[2], line 74\u001B[0m, in \u001B[0;36mcalculateMinCut\u001B[0;34m(adj_matrix, output, terminal1, terminal2)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(adj_matrix)):\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(adj_matrix)):\n\u001B[0;32m---> 74\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (output[i] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m output[j] \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.5\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (output[i] \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m output[j] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m) :\n\u001B[1;32m     75\u001B[0m             loss\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39madj_matrix[i][j]\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "print('Running GNN...')\n",
    "gnn_start = time()\n",
    "\n",
    "_, epoch, final_bitstring, best_bitstring = run_gnn_training(\n",
    "    q_torch, graph_dgl, net, embed, optimizer, gnn_hypers['number_epochs'],\n",
    "    gnn_hypers['tolerance'], gnn_hypers['patience'], gnn_hypers['prob_threshold'], calculate_H)\n",
    "\n",
    "calc = calculateMinCut(q_torch, best_bitstring)\n",
    "gnn_time = time() - gnn_start\n",
    "final_bitstring, best_bitstring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1]]),\n tensor([[1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1]]))"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bitstring, best_bitstring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 99.2914047241211\n",
      "tensor([[0.5235, 0.4812, 0.5002],\n",
      "        [0.5239, 0.4808, 0.5002],\n",
      "        [0.5267, 0.4786, 0.5003],\n",
      "        [0.5301, 0.4759, 0.5003],\n",
      "        [0.5268, 0.4785, 0.5003],\n",
      "        [0.5264, 0.4788, 0.5003],\n",
      "        [0.5233, 0.4814, 0.5002]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 1000, Loss: 87.56524658203125\n",
      "tensor([[0.6616, 0.4283, 0.5393],\n",
      "        [0.6592, 0.4299, 0.5390],\n",
      "        [0.6633, 0.4271, 0.5396],\n",
      "        [0.6707, 0.4220, 0.5405],\n",
      "        [0.6645, 0.4263, 0.5397],\n",
      "        [0.6634, 0.4271, 0.5396],\n",
      "        [0.6595, 0.4297, 0.5391]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 2000, Loss: 66.52015686035156\n",
      "tensor([[0.8024, 0.3018, 0.6042],\n",
      "        [0.7982, 0.3055, 0.6030],\n",
      "        [0.8026, 0.3016, 0.6043],\n",
      "        [0.8092, 0.2956, 0.6064],\n",
      "        [0.8043, 0.3001, 0.6048],\n",
      "        [0.8028, 0.3014, 0.6044],\n",
      "        [0.7992, 0.3047, 0.6033]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 3000, Loss: 37.91455841064453\n",
      "tensor([[0.8971, 0.1751, 0.6949],\n",
      "        [0.8933, 0.1795, 0.6924],\n",
      "        [0.8967, 0.1756, 0.6946],\n",
      "        [0.9013, 0.1702, 0.6977],\n",
      "        [0.8982, 0.1739, 0.6956],\n",
      "        [0.8969, 0.1753, 0.6947],\n",
      "        [0.8943, 0.1783, 0.6930]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 4000, Loss: 13.705463409423828\n",
      "tensor([[0.9465, 0.0974, 0.7835],\n",
      "        [0.9438, 0.1010, 0.7803],\n",
      "        [0.9460, 0.0980, 0.7830],\n",
      "        [0.9490, 0.0940, 0.7865],\n",
      "        [0.9471, 0.0966, 0.7842],\n",
      "        [0.9462, 0.0977, 0.7832],\n",
      "        [0.9445, 0.1000, 0.7812]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 5000, Loss: -3.5130255222320557\n",
      "tensor([[0.9715, 0.0537, 0.8544],\n",
      "        [0.9699, 0.0561, 0.8514],\n",
      "        [0.9713, 0.0541, 0.8540],\n",
      "        [0.9731, 0.0514, 0.8573],\n",
      "        [0.9719, 0.0531, 0.8551],\n",
      "        [0.9714, 0.0539, 0.8542],\n",
      "        [0.9703, 0.0554, 0.8523]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 6000, Loss: -15.00745964050293\n",
      "tensor([[0.9847, 0.0293, 0.9061],\n",
      "        [0.9838, 0.0307, 0.9038],\n",
      "        [0.9846, 0.0295, 0.9058],\n",
      "        [0.9857, 0.0278, 0.9086],\n",
      "        [0.9850, 0.0289, 0.9067],\n",
      "        [0.9847, 0.0294, 0.9060],\n",
      "        [0.9841, 0.0303, 0.9044]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 7000, Loss: -22.32656478881836\n",
      "tensor([[0.9919, 0.0159, 0.9413],\n",
      "        [0.9914, 0.0166, 0.9398],\n",
      "        [0.9918, 0.0159, 0.9413],\n",
      "        [0.9925, 0.0148, 0.9435],\n",
      "        [0.9920, 0.0156, 0.9419],\n",
      "        [0.9919, 0.0158, 0.9414],\n",
      "        [0.9915, 0.0164, 0.9402]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 8000, Loss: -26.753597259521484\n",
      "tensor([[0.9956, 0.0086, 0.9641],\n",
      "        [0.9954, 0.0090, 0.9631],\n",
      "        [0.9956, 0.0085, 0.9642],\n",
      "        [0.9960, 0.0079, 0.9657],\n",
      "        [0.9957, 0.0084, 0.9646],\n",
      "        [0.9957, 0.0085, 0.9642],\n",
      "        [0.9955, 0.0089, 0.9634]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 9000, Loss: -29.356910705566406\n",
      "tensor([[0.9976, 0.0047, 0.9782],\n",
      "        [0.9975, 0.0049, 0.9777],\n",
      "        [0.9976, 0.0046, 0.9783],\n",
      "        [0.9979, 0.0043, 0.9794],\n",
      "        [0.9977, 0.0046, 0.9786],\n",
      "        [0.9977, 0.0046, 0.9784],\n",
      "        [0.9975, 0.0048, 0.9778]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 10000, Loss: -30.873634338378906\n",
      "tensor([[0.9987, 0.0026, 0.9869],\n",
      "        [0.9986, 0.0027, 0.9865],\n",
      "        [0.9987, 0.0026, 0.9870],\n",
      "        [0.9988, 0.0023, 0.9877],\n",
      "        [0.9987, 0.0025, 0.9871],\n",
      "        [0.9987, 0.0025, 0.9870],\n",
      "        [0.9986, 0.0027, 0.9866]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 11000, Loss: -31.75645637512207\n",
      "tensor([[0.9993, 0.0014, 0.9921],\n",
      "        [0.9992, 0.0015, 0.9919],\n",
      "        [0.9993, 0.0014, 0.9922],\n",
      "        [0.9994, 0.0013, 0.9926],\n",
      "        [0.9993, 0.0014, 0.9923],\n",
      "        [0.9993, 0.0014, 0.9922],\n",
      "        [0.9993, 0.0015, 0.9919]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 12000, Loss: -32.27126693725586\n",
      "tensor([[9.9960e-01, 8.0556e-04, 9.9526e-01],\n",
      "        [9.9958e-01, 8.3555e-04, 9.9514e-01],\n",
      "        [9.9960e-01, 7.9097e-04, 9.9532e-01],\n",
      "        [9.9964e-01, 7.1867e-04, 9.9562e-01],\n",
      "        [9.9961e-01, 7.7769e-04, 9.9537e-01],\n",
      "        [9.9960e-01, 7.9032e-04, 9.9532e-01],\n",
      "        [9.9958e-01, 8.3053e-04, 9.9516e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 13000, Loss: -32.572208404541016\n",
      "tensor([[9.9977e-01, 4.5484e-04, 9.9716e-01],\n",
      "        [9.9976e-01, 4.7102e-04, 9.9709e-01],\n",
      "        [9.9978e-01, 4.4519e-04, 9.9720e-01],\n",
      "        [9.9980e-01, 4.0258e-04, 9.9739e-01],\n",
      "        [9.9978e-01, 4.3787e-04, 9.9723e-01],\n",
      "        [9.9978e-01, 4.4496e-04, 9.9720e-01],\n",
      "        [9.9976e-01, 4.6863e-04, 9.9710e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 14000, Loss: -32.74855041503906\n",
      "tensor([[9.9987e-01, 2.5823e-04, 9.9830e-01],\n",
      "        [9.9987e-01, 2.6700e-04, 9.9826e-01],\n",
      "        [9.9987e-01, 2.5197e-04, 9.9833e-01],\n",
      "        [9.9989e-01, 2.2683e-04, 9.9845e-01],\n",
      "        [9.9988e-01, 2.4792e-04, 9.9835e-01],\n",
      "        [9.9987e-01, 2.5193e-04, 9.9833e-01],\n",
      "        [9.9987e-01, 2.6589e-04, 9.9826e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 15000, Loss: -32.85203552246094\n",
      "tensor([[9.9993e-01, 1.4728e-04, 9.9898e-01],\n",
      "        [9.9992e-01, 1.5206e-04, 9.9896e-01],\n",
      "        [9.9993e-01, 1.4329e-04, 9.9900e-01],\n",
      "        [9.9994e-01, 1.2843e-04, 9.9908e-01],\n",
      "        [9.9993e-01, 1.4103e-04, 9.9901e-01],\n",
      "        [9.9993e-01, 1.4331e-04, 9.9900e-01],\n",
      "        [9.9992e-01, 1.5156e-04, 9.9896e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 16000, Loss: -32.91287612915039\n",
      "tensor([[9.9996e-01, 8.4336e-05, 9.9939e-01],\n",
      "        [9.9996e-01, 8.6946e-05, 9.9938e-01],\n",
      "        [9.9996e-01, 8.1819e-05, 9.9941e-01],\n",
      "        [9.9996e-01, 7.3023e-05, 9.9945e-01],\n",
      "        [9.9996e-01, 8.0554e-05, 9.9941e-01],\n",
      "        [9.9996e-01, 8.1852e-05, 9.9941e-01],\n",
      "        [9.9996e-01, 8.6732e-05, 9.9938e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 17000, Loss: -32.94863510131836\n",
      "tensor([[9.9998e-01, 4.8460e-05, 9.9964e-01],\n",
      "        [9.9998e-01, 4.9890e-05, 9.9963e-01],\n",
      "        [9.9998e-01, 4.6885e-05, 9.9965e-01],\n",
      "        [9.9998e-01, 4.1673e-05, 9.9968e-01],\n",
      "        [9.9998e-01, 4.6175e-05, 9.9965e-01],\n",
      "        [9.9998e-01, 4.6917e-05, 9.9965e-01],\n",
      "        [9.9998e-01, 4.9808e-05, 9.9963e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 18000, Loss: -32.969749450683594\n",
      "tensor([[9.9999e-01, 2.7932e-05, 9.9978e-01],\n",
      "        [9.9999e-01, 2.8717e-05, 9.9978e-01],\n",
      "        [9.9999e-01, 2.6952e-05, 9.9979e-01],\n",
      "        [9.9999e-01, 2.3861e-05, 9.9981e-01],\n",
      "        [9.9999e-01, 2.6552e-05, 9.9979e-01],\n",
      "        [9.9999e-01, 2.6977e-05, 9.9979e-01],\n",
      "        [9.9999e-01, 2.8693e-05, 9.9978e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 19000, Loss: -32.98212814331055\n",
      "tensor([[9.9999e-01, 1.6145e-05, 9.9987e-01],\n",
      "        [9.9999e-01, 1.6577e-05, 9.9987e-01],\n",
      "        [9.9999e-01, 1.5538e-05, 9.9987e-01],\n",
      "        [9.9999e-01, 1.3702e-05, 9.9989e-01],\n",
      "        [9.9999e-01, 1.5312e-05, 9.9988e-01],\n",
      "        [9.9999e-01, 1.5556e-05, 9.9987e-01],\n",
      "        [9.9999e-01, 1.6575e-05, 9.9987e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 20000, Loss: -32.989479064941406\n",
      "tensor([[1.0000e+00, 9.3558e-06, 9.9992e-01],\n",
      "        [1.0000e+00, 9.5934e-06, 9.9992e-01],\n",
      "        [1.0000e+00, 8.9810e-06, 9.9993e-01],\n",
      "        [1.0000e+00, 7.8901e-06, 9.9993e-01],\n",
      "        [1.0000e+00, 8.8530e-06, 9.9993e-01],\n",
      "        [1.0000e+00, 8.9941e-06, 9.9993e-01],\n",
      "        [1.0000e+00, 9.5999e-06, 9.9992e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 21000, Loss: -32.99378967285156\n",
      "tensor([[1.0000e+00, 5.4345e-06, 9.9995e-01],\n",
      "        [1.0000e+00, 5.5653e-06, 9.9995e-01],\n",
      "        [1.0000e+00, 5.2038e-06, 9.9996e-01],\n",
      "        [1.0000e+00, 4.5548e-06, 9.9996e-01],\n",
      "        [1.0000e+00, 5.1311e-06, 9.9996e-01],\n",
      "        [1.0000e+00, 5.2126e-06, 9.9996e-01],\n",
      "        [1.0000e+00, 5.5732e-06, 9.9995e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 22000, Loss: -32.996334075927734\n",
      "tensor([[1.0000e+00, 3.1635e-06, 9.9997e-01],\n",
      "        [1.0000e+00, 3.2356e-06, 9.9997e-01],\n",
      "        [1.0000e+00, 3.0219e-06, 9.9997e-01],\n",
      "        [1.0000e+00, 2.6354e-06, 9.9998e-01],\n",
      "        [1.0000e+00, 2.9805e-06, 9.9997e-01],\n",
      "        [1.0000e+00, 3.0277e-06, 9.9997e-01],\n",
      "        [1.0000e+00, 3.2426e-06, 9.9997e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 23000, Loss: -32.997825622558594\n",
      "tensor([[1.0000e+00, 1.8452e-06, 9.9998e-01],\n",
      "        [1.0000e+00, 1.8850e-06, 9.9998e-01],\n",
      "        [1.0000e+00, 1.7584e-06, 9.9998e-01],\n",
      "        [1.0000e+00, 1.5281e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 1.7348e-06, 9.9998e-01],\n",
      "        [1.0000e+00, 1.7622e-06, 9.9998e-01],\n",
      "        [1.0000e+00, 1.8904e-06, 9.9998e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 24000, Loss: -32.99871826171875\n",
      "tensor([[1.0000e+00, 1.0784e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 1.1003e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 1.0252e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 8.8782e-07, 9.9999e-01],\n",
      "        [1.0000e+00, 1.0117e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 1.0277e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 1.1042e-06, 9.9999e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 25000, Loss: -32.99925231933594\n",
      "tensor([[1.0000e+00, 6.3117e-07, 9.9999e-01],\n",
      "        [1.0000e+00, 6.4323e-07, 9.9999e-01],\n",
      "        [1.0000e+00, 5.9868e-07, 9.9999e-01],\n",
      "        [1.0000e+00, 5.1668e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 5.9097e-07, 9.9999e-01],\n",
      "        [1.0000e+00, 6.0026e-07, 9.9999e-01],\n",
      "        [1.0000e+00, 6.4596e-07, 9.9999e-01]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Epoch: 26000, Loss: -32.99953079223633\n",
      "tensor([[1.0000e+00, 3.6995e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 3.7657e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 3.5011e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 3.0113e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 3.4569e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 3.5111e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 3.7843e-07, 1.0000e+00]], grad_fn=<SigmoidBackward0>) tensor([[ 0., 10., 10., 10., 10., 10., 10.],\n",
      "        [10.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1.,  1.,  0., 10., 10., 10.],\n",
      "        [10.,  1.,  1., 10.,  0.,  1.,  1.],\n",
      "        [10.,  1.,  1., 10.,  1.,  0., 10.],\n",
      "        [10.,  1.,  1., 10.,  1., 10.,  0.]])\n",
      "Stopping early on epoch 26809 (patience: 10000)\n",
      "GNN training (n=7) took 30.695\n",
      "GNN final continuous loss: -32.99970626831055\n",
      "GNN best continuous loss: -32.99970626831055\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1]]),\n tensor([[1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1],\n         [1, 0, 1]]))"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = [(0,1, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,2, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,3, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,5, {\"weight\": 10, \"capacity\":10}),\n",
    "         (0,6, {\"weight\": 10, \"capacity\":10}),\n",
    "         (1,2, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,4, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (1,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,3, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,4, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (2,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (3,4, {\"weight\": 10, \"capacity\":10}),\n",
    "         (3,5, {\"weight\": 10, \"capacity\":10}),\n",
    "         (3,6, {\"weight\": 10, \"capacity\":10}),\n",
    "         (4,5, {\"weight\": 1, \"capacity\":1}),\n",
    "         (4,6, {\"weight\": 1, \"capacity\":1}),\n",
    "         (5,6, {\"weight\": 10, \"capacity\":10}),]\n",
    "graph = CreateDummyFunction(edges)\n",
    "graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "\n",
    "n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=7,patience=10000)\n",
    "# Establish pytorch GNN + optimizer\n",
    "opt_params = {'lr': learning_rate}\n",
    "gnn_hypers = {\n",
    "    'dim_embedding': dim_embedding,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'dropout': 0.0,\n",
    "    'number_classes': 3,\n",
    "    'prob_threshold': PROB_THRESHOLD,\n",
    "    'number_epochs': number_epochs,\n",
    "    'tolerance': tol,\n",
    "    'patience': patience\n",
    "}\n",
    "\n",
    "net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "# For tracking hyperparameters in results object\n",
    "gnn_hypers.update(opt_params)\n",
    "_, epoch, final_bitstring1, best_bitstring1 = run_gnn_training(\n",
    "    q_torch, graph_dgl, net, embed, optimizer, gnn_hypers['number_epochs'],\n",
    "    gnn_hypers['tolerance'], gnn_hypers['patience'], gnn_hypers['prob_threshold'], calculate_H_prime)\n",
    "final_bitstring1, best_bitstring1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}