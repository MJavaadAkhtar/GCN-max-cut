{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import random\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch as th\n",
    "# import matplotlib\n",
    "# from collections import OrderedDict, defaultdict\n",
    "# from networkx.algorithms.flow import shortest_augmenting_path\n",
    "# from dgl.nn.pytorch import GraphConv\n",
    "# from itertools import chain, islice, combinations\n",
    "# from networkx.algorithms.approximation.clique import maximum_independent_set as mis\n",
    "# from time import time\n",
    "# from networkx.algorithms.approximation.maxcut import one_exchange\n",
    "# from itertools import permutations\n",
    "# import matplotlib.pyplot as plt\n",
    "# import dgl\n",
    "# import pickle\n",
    "from commons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separate code base from heurestics\n",
    "# Utils code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "TORCH_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.float32\n",
    "\n",
    "\n",
    "def partition_weight(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph.\n",
    "    :param s: List indicating the partition of each edge (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    s = np.array(s)\n",
    "    partition_matrix = np.not_equal.outer(s, s).astype(int)\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "    return weight\n",
    "\n",
    "import torch\n",
    "\n",
    "def partition_weight2(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph as a PyTorch tensor.\n",
    "    :param s: Tensor indicating the partition of each node (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    # Ensure s is a tensor\n",
    "    # s = torch.tensor(s, dtype=torch.float32)\n",
    "\n",
    "    # Compute outer difference to create partition matrix\n",
    "    s = s.unsqueeze(0)  # Convert s to a row vector\n",
    "    t = s.t()           # Transpose s to a column vector\n",
    "    partition_matrix = (s != t).float()  # Compute outer product and convert boolean to float\n",
    "\n",
    "    # Calculate the weight of edges between different partitions\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut(q_torch, s):\n",
    "    '''\n",
    "\n",
    "    :param q_torch: The adjacent matrix of the graph\n",
    "    :param s: The binary output from the neural network. s will be in form of [[prob1, prob2, ..., prob n], ...]\n",
    "    :return: The calculated cut loss value\n",
    "    '''\n",
    "    if len(s) > 0:\n",
    "        totalCuts = len(s[0])\n",
    "        CutValue = 0\n",
    "        for i in range(totalCuts):\n",
    "            CutValue += partition_weight2(q_torch, s[:,i])\n",
    "        return CutValue/2\n",
    "    return 0\n",
    "\n",
    "def hyperParameters(n = 100, d = 3, p = None, graph_type = 'reg', number_epochs = int(1e5),\n",
    "                    learning_rate = 1e-4, PROB_THRESHOLD = 0.5, tol = 1e-4, patience = 100):\n",
    "    dim_embedding = int(np.sqrt(4096))    # e.g. 10\n",
    "    hidden_dim = int(dim_embedding/2)\n",
    "\n",
    "    return n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim\n",
    "def FIndAC(graph):\n",
    "    max_degree = max(dict(graph.degree()).values())\n",
    "    A_initial = max_degree + 1  # A is set to be one more than the maximum degree\n",
    "    C_initial = max_degree / 2  # C is set to half the maximum degree\n",
    "\n",
    "    return A_initial, C_initial\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_gnn_training2(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes=3, save_directory=None, torch_dtype = TORCH_DTYPE, torch_device = TORCH_DEVICE, labels=None):\n",
    "    \"\"\"\n",
    "    Train a GCN model with early stopping.\n",
    "    \"\"\"\n",
    "    # loss for a whole epoch\n",
    "    prev_loss = float('inf')  # Set initial loss to infinity for comparison\n",
    "    prev_cummulative_loss = float('inf')\n",
    "    cummulativeCount = 0\n",
    "    count = 0  # Patience counter\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    best_model_state = None  # Placeholder for the best model state\n",
    "    loss_list = []\n",
    "    epochList = []\n",
    "    cumulative_loss = 0\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # contains information regarding all terminal nodes for the dataset\n",
    "    terminal_configs = {}\n",
    "    epochCount = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    A = nn.Parameter(torch.tensor([65.0]))\n",
    "    C = nn.Parameter(torch.tensor([32.5]))\n",
    "\n",
    "    embed = nn.Embedding(80, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "    inputs = embed.weight\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss for each epoch\n",
    "\n",
    "        for key, (dgl_graph, adjacency_matrix,graph, terminals) in dataset.items():\n",
    "            epochCount +=1\n",
    "\n",
    "\n",
    "            # Ensure model is in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Pass the graph and the input features to the model\n",
    "            logits = net(dgl_graph, inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = loss_func(criterion, logits, labels, terminals[0], terminals[1])\n",
    "\n",
    "            loss = loss_func( logits, dgl_graph)\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update cumulative loss\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # # Check for early stopping\n",
    "            if epoch > 0 and (cumulative_loss > prev_loss or abs(prev_loss - cumulative_loss) <= tol):\n",
    "                count += 1\n",
    "                if count >= patience: # play around with patience value, try lower one\n",
    "                    print(f'Stopping early at epoch {epoch}')\n",
    "                    break\n",
    "            else:\n",
    "                count = 0  # Reset patience counter if loss decreases\n",
    "\n",
    "            # Update best model\n",
    "            if cumulative_loss < best_loss:\n",
    "                best_loss = cumulative_loss\n",
    "                best_model_state = net.state_dict()  # Save the best model state\n",
    "\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        # # Early stopping break from the outer loop\n",
    "        # if count >= patience:\n",
    "        #     count=0\n",
    "\n",
    "        prev_loss = cumulative_loss  # Update previous loss\n",
    "\n",
    "        if epoch % 100 == 0:  # Adjust printing frequency as needed\n",
    "            print(f'Epoch: {epoch}, Cumulative Loss: {cumulative_loss}')\n",
    "\n",
    "            if save_directory != None:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': net.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lossList':loss_list,\n",
    "                    'inputs':inputs}\n",
    "                torch.save(checkpoint, './epoch'+str(epoch)+'loss'+str(cumulative_loss)+ save_directory)\n",
    "\n",
    "            if (prev_cummulative_loss == cummulativeCount):\n",
    "                cummulativeCount+=1\n",
    "\n",
    "                if cummulativeCount > 4:\n",
    "                    break\n",
    "            else:\n",
    "                prev_cummulative_loss = cumulative_loss\n",
    "\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        net.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f'GNN training took {round(t_gnn, 3)} seconds.')\n",
    "    print(f'Best cumulative loss: {best_loss}')\n",
    "    loss = loss_func(logits, adjacency_matrix)\n",
    "    if save_directory != None:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lossList':loss_list,\n",
    "            'inputs':inputs}\n",
    "        torch.save(checkpoint, './final_'+save_directory)\n",
    "\n",
    "    return net, best_loss, epoch, inputs, loss_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HyperParameters initialization and related functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def printCombo(orig):\n",
    "    # Original dictionary\n",
    "    input_dict = orig\n",
    "\n",
    "    # Generate all permutations of the dictionary values\n",
    "    value_permutations = list(permutations(input_dict.values()))\n",
    "\n",
    "    # Create a list of dictionaries from the permutations\n",
    "    permuted_dicts = [{key: value for key, value in zip(input_dict.keys(), perm)} for perm in value_permutations]\n",
    "\n",
    "    return permuted_dicts\n",
    "\n",
    "def GetOptimalNetValue(net, dgl_graph, inp, q_torch, terminal_dict):\n",
    "    net.eval()\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if (dgl_graph.number_of_nodes() < 30):\n",
    "        inp = torch.ones((dgl_graph.number_of_nodes(), 30))\n",
    "\n",
    "    # find all potential combination of terminal nodes with respective indices\n",
    "\n",
    "    perm_items = printCombo(terminal_dict)\n",
    "    for i in perm_items:\n",
    "        probs = net(dgl_graph, inp, i)\n",
    "        binary_partitions = (probs >= 0.5).float()\n",
    "        cut_value_item = calculateAllCut(q_torch, binary_partitions)\n",
    "        if cut_value_item < best_loss:\n",
    "            best_loss = cut_value_item\n",
    "    return best_loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hamiltonian loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def terminal_independence_penalty(s, terminal_nodes):\n",
    "    \"\"\"\n",
    "    Calculate a penalty that enforces each terminal node to be in a distinct partition.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param terminal_nodes: A list of indices for terminal nodes.\n",
    "    :return: The penalty term.\n",
    "    \"\"\"\n",
    "    penalty = 0\n",
    "    num_terminals = len(terminal_nodes)\n",
    "    # Compare each pair of terminal nodes\n",
    "    for i in range(num_terminals):\n",
    "        for j in range(i + 1, num_terminals):\n",
    "            # Calculate the dot product of the probability vectors for the two terminals\n",
    "            dot_product = torch.dot(s[terminal_nodes[i]], s[terminal_nodes[j]])\n",
    "            # Penalize the similarity in their partition assignments (dot product should be close to 0)\n",
    "            penalty += dot_product\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def calculate_HA_vectorized(s):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HA.\n",
    "    :param s: A binary matrix of size |V| x |K| where s[i][j] is 1 if vertex i is in partition j.\n",
    "    :return: The HA value.\n",
    "    \"\"\"\n",
    "    # HA = ∑v∈V(∑k∈K(sv,k)−1)^2\n",
    "    HA = torch.sum((torch.sum(s, axis=1) - 1) ** 2)\n",
    "    return HA\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_intra_inter2(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC to minimize cut size.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    HC = 0\n",
    "    K = s.shape[1]\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)  # Probability node pair both in partition k\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)  # Probability node pair both in partition l\n",
    "            # Edges between partitions k and l\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "            HC += torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_min_cut_new(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Differentiable calculation of HC for minimizing edge weights between different partitions.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value, focusing on minimizing edge weights between partitions.\n",
    "    \"\"\"\n",
    "    K = s.shape[1]\n",
    "    V = s.shape[0]\n",
    "\n",
    "    # Create a full partition matrix indicating the likelihood of each node pair being in the same partition\n",
    "    partition_matrix = torch.matmul(s, s.T)\n",
    "\n",
    "    # Calculate the complement matrix, which indicates the likelihood of node pairs being in different partitions\n",
    "    complement_matrix = 1 - partition_matrix\n",
    "\n",
    "    # Apply adjacency matrix to only consider actual edges and their weights\n",
    "    inter_partition_edges = adjacency_matrix * complement_matrix\n",
    "\n",
    "    # Summing up all contributions for edges between different partitions\n",
    "    HC = torch.sum(inter_partition_edges)\n",
    "\n",
    "    return HC\n",
    "\n",
    "def calculate_HC_vectorized_old(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # HC = ∑(u,v)∈E(1−∑k∈K(su,k*sv,k))*adjacency_matrix[u,v]\n",
    "    K = s.shape[1]\n",
    "    # Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\n",
    "    prod = adjacency_matrix * (1 - s @ s.T)\n",
    "    HC = torch.sum(prod)\n",
    "    return HC\n",
    "import torch\n",
    "\n",
    "def min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The expected min-cut value, computed as a differentiable loss.\n",
    "    \"\"\"\n",
    "    V = s.size(0)  # Number of nodes\n",
    "    K = s.size(1)  # Number of partitions\n",
    "\n",
    "    # Ensure the partition matrix s sums to 1 over partitions\n",
    "    s = torch.softmax(s, dim=1)\n",
    "\n",
    "    # Compute the expected weight of edges within each partition\n",
    "    intra_partition_cut = torch.zeros((K, K), dtype=torch.float32)\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Probability that a node pair (i, j) is split between partitions k and l\n",
    "            partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "            partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "\n",
    "            # Compute the expected weight of the cut edges between partitions k and l\n",
    "            cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "            intra_partition_cut[k, l] = torch.sum(cut_weight)\n",
    "\n",
    "    # Sum up all contributions to get the total expected min-cut value\n",
    "    total_cut_weight = torch.sum(intra_partition_cut)\n",
    "\n",
    "    return total_cut_weight\n",
    "\n",
    "import torch\n",
    "\n",
    "# def min_cut_loss(s, adjacency_matrix):\n",
    "#     \"\"\"\n",
    "#     Compute a differentiable min-cut loss for a graph given node partition probabilities.\n",
    "#\n",
    "#     :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "#     :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "#     :return: The expected min-cut value, computed as a differentiable loss.\n",
    "#     \"\"\"\n",
    "#     V = s.size(0)  # Number of nodes\n",
    "#     K = s.size(1)  # Number of partitions\n",
    "#\n",
    "#     # Ensure the partition matrix s sums to 1 over partitions\n",
    "#     # s = torch.softmax(s, dim=1)\n",
    "#\n",
    "#     # Compute the expected weight of cut edges between each pair of partitions\n",
    "#     total_cut_weight = 0\n",
    "#     for k in range(K):\n",
    "#         for l in range(k + 1, K):\n",
    "#             # Probability that a node pair (i, j) is split between partitions k and l\n",
    "#             partition_k = s[:, k].unsqueeze(1)  # Shape: V x 1\n",
    "#             partition_l = s[:, l].unsqueeze(0)  # Shape: 1 x V\n",
    "#\n",
    "#             # Compute the expected weight of the cut edges between partitions k and l\n",
    "#             cut_weight = adjacency_matrix * (partition_k @ partition_l)\n",
    "#             total_cut_weight += torch.sum(cut_weight)\n",
    "#\n",
    "#     return total_cut_weight\n",
    "\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC for soft partitioning.\n",
    "    :param s: A probability matrix of size |V| x |K| where s[i][j] is the probability of vertex i being in partition j.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # Initialize HC to 0\n",
    "    HC = 0\n",
    "\n",
    "    # Iterate over each partition to calculate its contribution to HC\n",
    "    for k in range(s.shape[1]):\n",
    "        # Compute the probability matrix for partition k\n",
    "        partition_prob_matrix = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "\n",
    "        # Compute the contribution to HC for partition k\n",
    "        HC_k =adjacency_matrix * (1 - partition_prob_matrix)\n",
    "        # Sum up the contributions for partition k\n",
    "        HC += torch.sum(HC_k, dim=(0, 1))\n",
    "\n",
    "    # Since we've summed up the partition contributions twice (due to symmetry), divide by 2\n",
    "    HC = HC / 2\n",
    "\n",
    "    return HC\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor([0., 1., 1.])\n",
      "tensor(30.)\n",
      "tensor(10.)\n",
      "tensor(10.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "s = torch.Tensor([[0,1,0],[0,1,0],[0,0,1]])\n",
    "# print(calculate_HA_vectorized(s))\n",
    "# print(calculate_HA_vectorized(torch.Tensor([[0,0.9,0.9],[0.9,0.9,0],[0,0,0.9]])))\n",
    "terminal_loss = torch.abs(s[0] - s[1]-s[2])\n",
    "# print(terminal_loss)\n",
    "# print(10 * (1 - terminal_loss))\n",
    "# print(torch.sum(10 * (1 - terminal_loss)))\n",
    "print(torch.abs(s[0] - s[1]))\n",
    "print(torch.abs(s[0] - s[2]))\n",
    "print(torch.abs(s[2] - s[1]))\n",
    "\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[1]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[0] - s[2]))))\n",
    "print(torch.sum(10 * (1-torch.abs(s[2] - s[1]))))\n",
    "print(terminal_independence_penalty(s, [0,1,2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def train1(modelName):\n",
    "    n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(learning_rate=0.001, n=4096,patience=20)\n",
    "\n",
    "    # Establish pytorch GNN + optimizer\n",
    "    opt_params = {'lr': learning_rate}\n",
    "    gnn_hypers = {\n",
    "        'dim_embedding': dim_embedding,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': 0.0,\n",
    "        'number_classes': 3,\n",
    "        'prob_threshold': PROB_THRESHOLD,\n",
    "        'number_epochs': number_epochs,\n",
    "        'tolerance': tol,\n",
    "        'patience': patience,\n",
    "        'nodes':n\n",
    "    }\n",
    "    datasetItem = open_file('./testData/prepareDS.pkl')\n",
    "    # print(datasetItem)\n",
    "    # datasetItem_all = {}\n",
    "    # for key, (dgl_graph, adjacency_matrix,graph) in datasetItem.items():\n",
    "    #     A, C = FIndAC(graph)\n",
    "    #     datasetItem_all[key] = [dgl_graph, adjacency_matrix, graph, A, C]\n",
    "\n",
    "    # print(len(datasetItem), datasetItem[0][3])\n",
    "    # datasetItem_2 = {}\n",
    "    # datasetItem_2[0]=datasetItem[1]\n",
    "    # print(datasetItem_2)\n",
    "\n",
    "    net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "\n",
    "\n",
    "    # print(datasetItem[1][2].nodes)\n",
    "    # # Visualize graph\n",
    "    # pos = nx.kamada_kawai_layout(datasetItem[1][2])\n",
    "    # nx.draw(datasetItem[1][2], pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
    "    # cut_value, (part_1, part_2) = nx.minimum_cut(datasetItem_2[0][2], datasetItem_2[0][3][1], datasetItem_2[0][3][0], flow_func=shortest_augmenting_path)\n",
    "\n",
    "    # print(cut_value, len(part_1), len(part_2))\n",
    "\n",
    "    # resultList = []\n",
    "    # all_indexes = sorted(part_1.union(part_2))\n",
    "    # # Check membership for each index and append the appropriate pair to the result list\n",
    "    # for index in all_indexes:\n",
    "    #     if index in part_1:\n",
    "    #         resultList.append([1, 0])\n",
    "    #     elif index in part_2:\n",
    "    #         resultList.append([0, 1])\n",
    "\n",
    "    #\n",
    "    trained_net, bestLost, epoch, inp, lossList= run_gnn_training2(\n",
    "        datasetItem, net, optimizer, int(500),\n",
    "        gnn_hypers['tolerance'], gnn_hypers['patience'], loss_terminal,gnn_hypers['dim_embedding'], gnn_hypers['number_classes'], modelName,  TORCH_DTYPE,  TORCH_DEVICE)\n",
    "\n",
    "    return trained_net, bestLost, epoch, inp, lossList\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Training, Setting A to 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 1587806.7822265625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m penalty\u001B[38;5;241m*\u001B[39m terminal_independence_penalty(s, [\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n\u001B[0;32m---> 14\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_LossOrig.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[57], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[52], line 50\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m     46\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(logits, adjacency_matrix)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     52\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_compile.py:24\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dynamo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecursive\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:489\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    487\u001B[0m     dynamo_config_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 489\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    491\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/optim/optimizer.py:815\u001B[0m, in \u001B[0;36mOptimizer.zero_grad\u001B[0;34m(self, set_to_none)\u001B[0m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    813\u001B[0m     per_device_and_dtype_grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 815\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecord_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_zero_grad_profile_name\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    816\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    817\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/torch/autograd/profiler.py:601\u001B[0m, in \u001B[0;36mrecord_function.__init__\u001B[0;34m(self, name, args)\u001B[0m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks_on_exit: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;66;03m# TODO: TorchScript ignores standard type annotation here\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;66;03m# self.record: Optional[\"torch.classes.profiler._RecordFunction\"] = None\u001B[39;00m\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecord \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mannotate(\n\u001B[0;32m--> 601\u001B[0m     \u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch.classes.profiler._RecordFunction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    602\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/research/lib/python3.8/typing.py:257\u001B[0m, in \u001B[0;36m_tp_cache.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m--> 257\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m cached(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HA = calculate_HA_vectorized(s)\n",
    "    HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_min_cut_new(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_min_cut_intra_inter(s, adjacency_matrix)\n",
    "    return A * HA + C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=10000, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_LossOrig.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 14220161.884765625\n",
      "Epoch: 100, Cumulative Loss: 6690870.7734375\n",
      "Epoch: 200, Cumulative Loss: 6689155.9052734375\n",
      "Epoch: 300, Cumulative Loss: 6688431.19140625\n",
      "Epoch: 400, Cumulative Loss: 6688069.7470703125\n",
      "Epoch: 500, Cumulative Loss: 6687530.076171875\n",
      "Epoch: 600, Cumulative Loss: 6687087.41015625\n",
      "Epoch: 700, Cumulative Loss: 6687105.1103515625\n",
      "Epoch: 800, Cumulative Loss: 6687147.55859375\n",
      "Epoch: 900, Cumulative Loss: 6687304.62109375\n",
      "GNN training took 338.853 seconds.\n",
      "Best cumulative loss: 28800.201171875\n"
     ]
    }
   ],
   "source": [
    "# def corrected_mysteryLoss(s, adjacency_matrix):\n",
    "#     loss = 0\n",
    "#     K = s.shape[1]\n",
    "#     for k in range(K):\n",
    "#         for l in range(k + 1, K):\n",
    "#             partition_k = s[:, k].unsqueeze(1)\n",
    "#             partition_l = s[:, l].unsqueeze(0)\n",
    "#\n",
    "#             # partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "#             # partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "#\n",
    "#             inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "#             loss += torch.sum(inter_partition_edges)\n",
    "#\n",
    "#     return loss\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Use probabilities directly for nodes being in partitions k and l\n",
    "            # partition_k = s[:, k].unsqueeze(1)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "            # partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "            partition_l = s[:, l].unsqueeze(0)\n",
    "            # Edge weights between partitions\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k * partition_l)\n",
    "            min_cut_loss += torch.sum(inter_partition_edges)\n",
    "\n",
    "    # Regularization to encourage probabilities close to 0 or 1\n",
    "    regularization = torch.sum((s * (1 - s)))\n",
    "\n",
    "    return min_cut_loss + 0.01 * regularization  # Adjust regularization weight as necessary\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    HA = calculate_HA_vectorized(s)\n",
    "    # HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_min_cut_new(s, adjacency_matrix)\n",
    "    HC = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    # HC = min_cut_loss(s, adjacency_matrix)\n",
    "    return A * HA + C * HC\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=100, C=100, penalty=100000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    loss += penalty* terminal_independence_penalty(s, [0,1,2])\n",
    "    return loss\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 2634424.9970703125\n",
      "Epoch: 100, Cumulative Loss: 2528362.158203125\n",
      "Epoch: 200, Cumulative Loss: 2528362.158203125\n",
      "Epoch: 300, Cumulative Loss: 2528362.158203125\n",
      "Epoch: 400, Cumulative Loss: 2528362.158203125\n",
      "GNN training took 426.469 seconds.\n",
      "Best cumulative loss: 10473.2529296875\n"
     ]
    }
   ],
   "source": [
    "# def corrected_mysteryLoss(s, adjacency_matrix):\n",
    "#     loss = 0\n",
    "#     K = s.shape[1]\n",
    "#     for k in range(K):\n",
    "#         for l in range(k + 1, K):\n",
    "#             partition_k = s[:, k].unsqueeze(1)\n",
    "#             partition_l = s[:, l].unsqueeze(0)\n",
    "#\n",
    "#             # partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "#             # partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "#\n",
    "#             inter_partition_edges = adjacency_matrix * (partition_k + partition_l)\n",
    "#             loss += torch.sum(inter_partition_edges)\n",
    "#\n",
    "#     return loss\n",
    "\n",
    "def soft_min_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a soft min-cut loss that maintains differentiability by penalizing\n",
    "    the sum of squared differences from binary values (0 or 1).\n",
    "    \"\"\"\n",
    "    s = torch.softmax(s, dim=1)  # Ensure that s is a proper probability distribution\n",
    "    V, K = s.shape\n",
    "\n",
    "    min_cut_loss = 0\n",
    "    for k in range(K):\n",
    "        for l in range(k + 1, K):\n",
    "            # Use probabilities directly for nodes being in partitions k and l\n",
    "            # partition_k = s[:, k].unsqueeze(1)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "\n",
    "            partition_k = s[:, k].unsqueeze(1) * s[:, k].unsqueeze(0)\n",
    "            partition_l = s[:, l].unsqueeze(1) * s[:, l].unsqueeze(0)\n",
    "            # partition_l = s[:, l].unsqueeze(0)\n",
    "            # Edge weights between partitions\n",
    "            inter_partition_edges = adjacency_matrix * (partition_k @ partition_l)\n",
    "            min_cut_loss += torch.sum(inter_partition_edges)\n",
    "\n",
    "    # Regularization to encourage probabilities close to 0 or 1\n",
    "    regularization = torch.sum((s * (1 - s)))\n",
    "\n",
    "    return min_cut_loss + 0.01 * regularization  # Adjust regularization weight as necessary\n",
    "\n",
    "# def min_cut_loss(s, adjacency_matrix):\n",
    "#     \"\"\"\n",
    "#     Calculate the min-cut loss.\n",
    "#     :param s: Partition matrix.\n",
    "#     :param adjacency_matrix: Adjacency matrix of the graph.\n",
    "#     :return: Min-cut loss value.\n",
    "#     \"\"\"\n",
    "#     edge_loss = 0\n",
    "#     for i in range(adjacency_matrix.shape[0]):\n",
    "#         for j in range(adjacency_matrix.shape[1]):\n",
    "#             if adjacency_matrix[i, j] > 0:\n",
    "#                 edge_loss += adjacency_matrix[i, j] * (s[i] - s[j]).abs().sum()\n",
    "#     return edge_loss\n",
    "\n",
    "def calculate_multiclass_cut_value( s, A):\n",
    "\n",
    "    num_partitions = s.shape[1]\n",
    "    n = s.shape[0]\n",
    "\n",
    "    # Initialize the cut value\n",
    "    cut_value = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "    # Calculate contributions for all pairs of different partitions\n",
    "    for i in range(num_partitions):\n",
    "        for j in range(i + 1, num_partitions):\n",
    "            # Probabilities of nodes being in partitions i and j\n",
    "            p_i = s[:, i]\n",
    "            p_j = s[:, j]\n",
    "\n",
    "            # Outer products to calculate probabilities of being in different partitions\n",
    "            P_outer = torch.ger(p_i, p_j)\n",
    "            Q_outer = torch.ger(p_j, p_i)\n",
    "\n",
    "            # Expected cut-value matrix for partitions i and j\n",
    "            expected_cut_matrix = A * (P_outer + Q_outer)\n",
    "\n",
    "            # Add to total cut value\n",
    "            cut_value += torch.sum(torch.triu(expected_cut_matrix, 1))\n",
    "\n",
    "    return cut_value\n",
    "# def HC_(s, adjacency_matrix):\n",
    "#     # Ensure probabilities is a row vector\n",
    "#     probabilities = s.view(1, -1)\n",
    "#\n",
    "#     # Use probabilities directly for core and halo assignments\n",
    "#     core_a = probabilities  # Probabilities of being in partition A\n",
    "#     core_b = 1 - probabilities  # Probabilities of being in partition B\n",
    "#     # Calculate H_B using probabilistic assignments\n",
    "#     expected_connections = torch.matmul(core_a.T, core_b) + torch.matmul(core_b.T, core_a)\n",
    "#     discrepancy_matrix = adjacency_matrix - expected_connections\n",
    "#     H_B = torch.sum(discrepancy_matrix ** 2)\n",
    "#\n",
    "#     # Calculate H_C as the sum of squared probabilities in each partition's halo\n",
    "#     H_C = torch.sum(core_b) ** 2 + torch.sum(core_a) ** 2\n",
    "#\n",
    "#     return HC\n",
    "\n",
    "def partition_weight_2(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the sum of weights of edges that are in different partitions.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph as a PyTorch tensor.\n",
    "    :param s: Tensor indicating the partition of each node (0 or 1).\n",
    "    :return: Sum of weights of edges in different partitions.\n",
    "    \"\"\"\n",
    "    # Ensure s is a tensor\n",
    "    # s = torch.tensor(s, dtype=torch.float32)\n",
    "\n",
    "    # Compute outer difference to create partition matrix\n",
    "    s = s.unsqueeze(0) * s.unsqueeze(1)  # Convert s to a row vector\n",
    "    t = s.t()           # Transpose s to a column vector\n",
    "    partition_matrix = (s != t).float()  # Compute outer product and convert boolean to float\n",
    "\n",
    "    # Calculate the weight of edges between different partitions\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut_2(q_torch, s):\n",
    "    '''\n",
    "\n",
    "    :param q_torch: The adjacent matrix of the graph\n",
    "    :param s: The binary output from the neural network. s will be in form of [[prob1, prob2, ..., prob n], ...]\n",
    "    :return: The calculated cut loss value\n",
    "    '''\n",
    "    if len(s) > 0:\n",
    "        totalCuts = len(s[0])\n",
    "        CutValue = 0\n",
    "        for i in range(totalCuts):\n",
    "            CutValue += partition_weight_2(q_torch, s[:,i])\n",
    "        return CutValue/2\n",
    "    return 0\n",
    "\n",
    "def partition_weight_3(adj, s):\n",
    "    \"\"\"\n",
    "    Calculates the expected sum of weights of edges between nodes in different partitions,\n",
    "    using probabilities of node partition assignments.\n",
    "\n",
    "    :param adj: Adjacency matrix of the graph as a PyTorch tensor.\n",
    "    :param s: Tensor indicating the probability of each node being in a given partition.\n",
    "    :return: Expected sum of weights of edges between different partitions.\n",
    "    \"\"\"\n",
    "    # s should be a vector of probabilities that each node is in the partition\n",
    "    # Compute the probability matrix where each element (i, j) is the probability\n",
    "    # that node i and node j are in different partitions\n",
    "    s_outer = s.unsqueeze(0)\n",
    "    partition_matrix = torch.abs(s_outer)  # Absolute difference gives the probability of being in different partitions\n",
    "\n",
    "    # Calculate the expected weight of edges between different partitions\n",
    "    weight = (adj * partition_matrix).sum() / 2\n",
    "\n",
    "    return weight\n",
    "\n",
    "def calculateAllCut_3(q_torch, s):\n",
    "    \"\"\"\n",
    "    Calculates the total expected cut loss across all partitions.\n",
    "\n",
    "    :param q_torch: The adjacency matrix of the graph.\n",
    "    :param s: The output probabilities from the neural network for each partition.\n",
    "    :return: The total calculated cut loss value.\n",
    "    \"\"\"\n",
    "    if len(s) > 0:\n",
    "        totalCuts = s.shape[1]  # Assuming s is of shape [num_nodes, num_partitions]\n",
    "        CutValue = 0\n",
    "        # Iterate over all pairs of partitions\n",
    "        for i in range(totalCuts):\n",
    "            for j in range(i + 1, totalCuts):\n",
    "                # Calculate the expected cut weight between partition i and partition j\n",
    "                CutValue += partition_weight_3(q_torch, s[:, i] - s[:, j])\n",
    "        return CutValue\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_HC_vectorized(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of HC.\n",
    "    :param s: A binary matrix of size |V| x |K|.\n",
    "    :param adjacency_matrix: A matrix representing the graph where the value at [i][j] is the weight of the edge between i and j.\n",
    "    :return: The HC value.\n",
    "    \"\"\"\n",
    "    # HC = ∑(u,v)∈E(1−∑k∈K(su,k*sv,k))*adjacency_matrix[u,v]\n",
    "    K = s.shape[1]\n",
    "    # Outer product to find pairs of vertices in the same partition and then weight by the adjacency matrix\n",
    "    prod = adjacency_matrix * (1 - s @ s.T)\n",
    "    HC = torch.sum(prod)\n",
    "    return HC\n",
    "\n",
    "def Loss(s, adjacency_matrix,  A=1, C=1):\n",
    "    # HA = calculate_HA_vectorized(s)\n",
    "    # HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_min_cut_new(s, adjacency_matrix)\n",
    "    # HC = calculate_HC_vectorized(s, adjacency_matrix)\n",
    "    HC = soft_min_cut_loss(s, adjacency_matrix)\n",
    "    # HC = min_cut_loss(s, adjacency_matrix)\n",
    "    return  (C * HC)\n",
    "\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A=0, C=1, penalty=10000):\n",
    "    loss = Loss(s, adjacency_matrix, A, C)\n",
    "    # loss += (penalty* terminal_independence_penalty(s, [0,1,2]))\n",
    "    return loss\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Basic forward pass\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "        # h = F.sigmoid(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter_min_cut_loss_9.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 24380.278964996338\n",
      "Epoch: 100, Cumulative Loss: 26443.733406066895\n",
      "Epoch: 200, Cumulative Loss: 26389.7333984375\n",
      "Epoch: 300, Cumulative Loss: 26405.733375549316\n",
      "Epoch: 400, Cumulative Loss: 26403.733375549316\n",
      "GNN training took 416.921 seconds.\n",
      "Best cumulative loss: 40.904293060302734\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'edges'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 59\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[38;5;66;03m# Classifier applied directly to node features\u001B[39;00m\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(h)\n\u001B[0;32m---> 59\u001B[0m trained_net, bestLost, epoch, inp, lossList \u001B[38;5;241m=\u001B[39m \u001B[43mtrain1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_80wayCut_Lossinter_min_cut_loss_9.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[19], line 50\u001B[0m, in \u001B[0;36mtrain1\u001B[0;34m(modelName)\u001B[0m\n\u001B[1;32m     29\u001B[0m net, embed, optimizer \u001B[38;5;241m=\u001B[39m get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# print(datasetItem[1][2].nodes)\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# # Visualize graph\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# pos = nx.kamada_kawai_layout(datasetItem[1][2])\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m trained_net, bestLost, epoch, inp, lossList\u001B[38;5;241m=\u001B[39m \u001B[43mrun_gnn_training2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatasetItem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtolerance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpatience\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_terminal\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdim_embedding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgnn_hypers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnumber_classes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DTYPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mTORCH_DEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m trained_net, bestLost, epoch, inp, lossList\n",
      "Cell \u001B[0;32mIn[55], line 110\u001B[0m, in \u001B[0;36mrun_gnn_training2\u001B[0;34m(dataset, net, optimizer, number_epochs, tol, patience, loss_func, dim_embedding, total_classes, save_directory, torch_dtype, torch_device, labels)\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGNN training took \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(t_gnn,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBest cumulative loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 110\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madjacency_matrix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m save_directory \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    112\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    113\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m: epoch,\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: net\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: optimizer\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlossList\u001B[39m\u001B[38;5;124m'\u001B[39m:loss_list,\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m'\u001B[39m:inputs}\n",
      "Cell \u001B[0;32mIn[58], line 24\u001B[0m, in \u001B[0;36mloss_terminal\u001B[0;34m(s, g, A, C, penalty)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss_terminal\u001B[39m(s, g,  A\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, C\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m):\n\u001B[0;32m---> 24\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mpartition_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# loss += (penalty* terminal_independence_penalty(s, [0,1,2]))\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "Cell \u001B[0;32mIn[58], line 16\u001B[0m, in \u001B[0;36mpartition_loss\u001B[0;34m(g, node_logits, terminals)\u001B[0m\n\u001B[1;32m     14\u001B[0m cut_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     15\u001B[0m assignments \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(probabilities, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m u, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medges\u001B[49m()):\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m assignments[u] \u001B[38;5;241m!=\u001B[39m assignments[v]:\n\u001B[1;32m     18\u001B[0m         cut_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# Assuming uniform weight of 1 for each edge\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'edges'"
     ]
    }
   ],
   "source": [
    "def partition_loss(g, node_logits, terminals):\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(node_logits, dim=1)\n",
    "\n",
    "    # Terminal node embeddings\n",
    "    h_a, h_b, h_c = probabilities[terminals[0]], probabilities[terminals[1]], probabilities[terminals[2]]\n",
    "\n",
    "    # Separation loss\n",
    "    sep_loss = (torch.max(torch.tensor(0.0), 1 - torch.norm(h_a - h_b)) +\n",
    "                torch.max(torch.tensor(0.0), 1 - torch.norm(h_b - h_c)) +\n",
    "                torch.max(torch.tensor(0.0), 1 - torch.norm(h_c - h_a)))\n",
    "\n",
    "    # Cut size minimization loss\n",
    "    cut_loss = 0\n",
    "    assignments = torch.argmax(probabilities, dim=1)\n",
    "    for u, v in zip(*g.edges()):\n",
    "        if assignments[u] != assignments[v]:\n",
    "            cut_loss += 1  # Assuming uniform weight of 1 for each edge\n",
    "\n",
    "    return sep_loss + cut_loss\n",
    "\n",
    "\n",
    "def loss_terminal(s, g,  A=0, C=1, penalty=10000):\n",
    "    loss = partition_loss(g, s, [0,1,2])\n",
    "    # loss += (penalty* terminal_independence_penalty(s, [0,1,2]))\n",
    "    return loss\n",
    "\n",
    "class GCNSoftmax(nn.Module):\n",
    "    # def __init__(self, in_feats, hidden_size, num_classes, dropout, device):\n",
    "    #     super(GCNSoftmax, self).__init__()\n",
    "    #     self.dropout_frac = dropout\n",
    "    #     self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "    #     self.conv2 = GraphConv(hidden_size, num_classes).to(device)\n",
    "    #\n",
    "    # def forward(self, g, inputs):\n",
    "    #     # Basic forward pass\n",
    "    #     h = self.conv1(g, inputs)\n",
    "    #     h = F.relu(h)\n",
    "    #     h = F.dropout(h, p=self.dropout_frac, training=self.training)\n",
    "    #     h = self.conv2(g, h)\n",
    "    #     h = F.softmax(h, dim=1)  # Apply softmax over the classes dimension\n",
    "    #     # h = F.sigmoid(h)\n",
    "    #\n",
    "    #     return h\n",
    "\n",
    "    def __init__(self, in_feats, hidden_size, num_classes,dropout, device):\n",
    "        super(GCNSoftmax, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_size, hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        # Apply graph convolution and activation layer\n",
    "        h = torch.relu(self.conv1(g, features))\n",
    "        h = torch.relu(self.conv2(g, h))\n",
    "        # Classifier applied directly to node features\n",
    "        return self.classifier(h)\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter_min_cut_loss_9.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Cumulative Loss: 111788.00930786133\n",
      "Epoch: 100, Cumulative Loss: 75000.0\n",
      "Epoch: 200, Cumulative Loss: 75000.0\n",
      "Epoch: 300, Cumulative Loss: 75000.0\n",
      "Epoch: 400, Cumulative Loss: 75000.0\n",
      "GNN training took 155.964 seconds.\n",
      "Best cumulative loss: 300.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def probabilistic_cut_loss(s, adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the expected cut loss based on partition probabilities.\n",
    "\n",
    "    :param s: A matrix of size [num_nodes, num_partitions] where each element is the probability of a node being in a partition.\n",
    "    :param adjacency_matrix: The adjacency matrix of the graph.\n",
    "    :return: The expected cut loss.\n",
    "    \"\"\"\n",
    "    num_partitions = s.shape[1]\n",
    "    cut_loss = 0\n",
    "    for i in range(num_partitions):\n",
    "        for j in range(i + 1, num_partitions):\n",
    "            partition_i = s[:, i].unsqueeze(1)\n",
    "            partition_j = s[:, j].unsqueeze(0)\n",
    "            # Calculate the probability that nodes are in different partitions\n",
    "            inter_partition_prob = partition_i * partition_j\n",
    "            # Calculate the expected cut weight\n",
    "            cut_loss += torch.sum(adjacency_matrix * inter_partition_prob)\n",
    "    return cut_loss\n",
    "\n",
    "def terminal_independence_penalty(s, terminals):\n",
    "    \"\"\"\n",
    "    Enforce that terminal nodes are in separate partitions by penalizing overlaps.\n",
    "\n",
    "    :param s: Probability matrix of node partition assignments.\n",
    "    :param terminals: List of indices of terminal nodes.\n",
    "    :return: Penalty for terminal nodes sharing partitions.\n",
    "    \"\"\"\n",
    "    penalty = 0\n",
    "    num_terminals = len(terminals)\n",
    "    for i in range(num_terminals):\n",
    "        for j in range(i + 1, num_terminals):\n",
    "            # Calculate the dot product to determine the overlap in partition probabilities\n",
    "            overlap = torch.dot(s[terminals[i]], s[terminals[j]])\n",
    "            penalty += overlap  # Penalize any overlap\n",
    "    return penalty\n",
    "\n",
    "def loss_terminal(s, adjacency_matrix,  A= 0, C=1, T=100):\n",
    "    \"\"\"\n",
    "    Compute the overall loss including cut loss and terminal independence.\n",
    "\n",
    "    :param s: Node partition probabilities.\n",
    "    :param adjacency_matrix: Graph adjacency matrix.\n",
    "    :param terminals: List of terminal node indices.\n",
    "    :param C: Weight for the cut loss.\n",
    "    :param T: Weight for the terminal independence penalty.\n",
    "    :return: Total loss.\n",
    "    \"\"\"\n",
    "    cut_loss = probabilistic_cut_loss(s, adjacency_matrix)\n",
    "    terminal_loss = terminal_independence_penalty(s, [0,1,2])\n",
    "    total_loss = C * cut_loss + T * terminal_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "trained_net, bestLost, epoch, inp, lossList = train1('_80wayCut_Lossinter_min_cut_loss_9.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_net, bestLost, epoch, inp, lossList = train1()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_net_2, bestLost_2, epoch_2, inp_2, lossList_2 = train1()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing on exp1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=30,patience=1000)\n",
    "#\n",
    "# # Establish pytorch GNN + optimizer\n",
    "# opt_params = {'lr': learning_rate}\n",
    "# gnn_hypers = {\n",
    "#     'dim_embedding': dim_embedding,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'dropout': 0.0,\n",
    "#     'number_classes': 3,\n",
    "#     'prob_threshold': PROB_THRESHOLD,\n",
    "#     'number_epochs': number_epochs,\n",
    "#     'tolerance': tol,\n",
    "#     'patience': patience,\n",
    "#     'nodes':n\n",
    "# }\n",
    "#\n",
    "# net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "# # load nerual network model for evaluation\n",
    "# model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, './exp1.pth')\n",
    "# model.eval()\n",
    "#\n",
    "# #create dummy graph\n",
    "# graph = CreateGraph(20)\n",
    "# dgl_graph = dgl.from_networkx(nx_graph=graph)\n",
    "# dgl_graph = dgl_graph.to(TORCH_DEVICE)\n",
    "# q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "# # find min cut\n",
    "# print(\"Heurestic 3-way min-cut value: \" + str(find3WayCut(graph, [0,16,19])))\n",
    "# print(\"Neural Network 3-way min-cut value: \" + str(GetOptimalNetValue(model,dgl_graph, inputs, q_torch, {0:0, 16:1, 19:2})))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_item = {}\n",
    "# for i in range(10):\n",
    "#\n",
    "#\n",
    "#     graph = CreateGraph(30)\n",
    "#     graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "#     graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "#     q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "#     test_item[i] = [graph_dgl, q_torch, graph]\n",
    "#\n",
    "# for key, (dgl_graph, adjacency_matrix,graph) in test_item.items():\n",
    "#     print(\"Heurestic 3-way min-cut value: \" + str(find3WayCut(graph, [0,20,28])))\n",
    "#     print(\"Neural Network 3-way min-cut value: \" + str(GetOptimalNetValue(model,dgl_graph, inputs, adjacency_matrix, {0:0, 20:1, 28:2})))\n",
    "#     print(f'-------')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment 2\n",
    "\n",
    "Creating training set with the following constraints:\n",
    "- Each graph have precisey 100 nodes\n",
    "- Each graph has random edges (50% probability of edge creation)\n",
    "- Each edge has random value of 1-500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# datasetItem = {}\n",
    "# for i in range(1000):\n",
    "#\n",
    "#\n",
    "#     graph = CreateGraph(100)\n",
    "#     graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "#     graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "#     q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "#     datasetItem[i] = [graph_dgl, q_torch, graph]\n",
    "#     print(\"Graph Number Created: \"+str(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(datasetItem[1][2].nodes)\n",
    "# # Visualize graph\n",
    "# pos = nx.kamada_kawai_layout(datasetItem[1][2])\n",
    "# nx.draw(datasetItem[1][2], pos, with_labels=True, node_color=[[.7, .7, .7]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=100,patience=1000)\n",
    "#\n",
    "# # Establish pytorch GNN + optimizer\n",
    "# opt_params = {'lr': learning_rate}\n",
    "# gnn_hypers = {\n",
    "#     'dim_embedding': dim_embedding,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'dropout': 0.0,\n",
    "#     'number_classes': 3,\n",
    "#     'prob_threshold': PROB_THRESHOLD,\n",
    "#     'number_epochs': number_epochs,\n",
    "#     'tolerance': tol,\n",
    "#     'patience': patience,\n",
    "#     'nodes':n\n",
    "# }\n",
    "#\n",
    "# net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "#\n",
    "# terminal_nodes = 3\n",
    "#\n",
    "# trained_net, bestLost, epoch, inp= run_gnn_training(\n",
    "#     datasetItem, net, embed, optimizer, int(1e3),\n",
    "#     gnn_hypers['tolerance'], gnn_hypers['patience'], Loss, terminal_nodes, 100, gnn_hypers['number_classes'], './exp2.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Exp2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=100,patience=1000)\n",
    "#\n",
    "# # Establish pytorch GNN + optimizer\n",
    "# opt_params = {'lr': learning_rate}\n",
    "# gnn_hypers = {\n",
    "#     'dim_embedding': dim_embedding,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'dropout': 0.0,\n",
    "#     'number_classes': 3,\n",
    "#     'prob_threshold': PROB_THRESHOLD,\n",
    "#     'number_epochs': number_epochs,\n",
    "#     'tolerance': tol,\n",
    "#     'patience': patience,\n",
    "#     'nodes':n\n",
    "# }\n",
    "#\n",
    "# net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "# # load nerual network model for evaluation\n",
    "# model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, './exp2.pth')\n",
    "# model.eval()\n",
    "#\n",
    "# #create dummy graph\n",
    "# graph = CreateGraph(100)\n",
    "# dgl_graph = dgl.from_networkx(nx_graph=graph)\n",
    "# dgl_graph = graph_dgl.to(TORCH_DEVICE)\n",
    "# q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "# # find min cut\n",
    "# print(\"Heurestic 3-way min-cut value: \" + str(find3WayCut(graph, [0,20,28])))\n",
    "# print(\"Neural Network 3-way min-cut value: \" + str(GetOptimalNetValue(model,dgl_graph, inputs, q_torch, {0:0, 20:1, 28:2})))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_item = {}\n",
    "# for i in range(100):\n",
    "#\n",
    "#\n",
    "#     graph = CreateGraph(100)\n",
    "#     graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "#     graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "#     q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "#     test_item[i] = [graph_dgl, q_torch, graph]\n",
    "#\n",
    "# indices = []\n",
    "# i = 0\n",
    "# heurestic_cut = []\n",
    "# neural_cut = []\n",
    "# for key, (dgl_graph, adjacency_matrix,graph) in test_item.items():\n",
    "#     heurestic_cut.append(find3WayCut(graph, [0,20,65]))\n",
    "#     neural_cut.append(GetOptimalNetValue(model,dgl_graph, inputs, adjacency_matrix, {0:0, 20:1, 65:2}))\n",
    "#     i+=1\n",
    "#     indices.append(i)\n",
    "#     print(\"Heurestic 3-way min-cut value: \" + str(heurestic_cut[-1]))\n",
    "#     print(\"Neural Network 3-way min-cut value: \" + str(neural_cut[-1]))\n",
    "#     print(f'-------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Function to plot True vs Prediction graph\n",
    "# def barPlot(indices, heurestic_cut, neural_cut):\n",
    "#     # Example data\n",
    "#     n_groups = len(heurestic_cut)\n",
    "#     index = np.arange(n_groups)\n",
    "#     bar_width = 0.35\n",
    "#\n",
    "#     # Create bars\n",
    "#     plt.figure(figsize=(30, 6))\n",
    "#     bar1 = plt.bar(index, heurestic_cut, bar_width, label='Heurestic')\n",
    "#     bar2 = plt.bar(index + bar_width, neural_cut, bar_width, label='Neural Network')\n",
    "#\n",
    "#     # Add details\n",
    "#     plt.xlabel('Graph Number')\n",
    "#     plt.ylabel('Minimum Cut Value')\n",
    "#     plt.title('Comparison of Minimum Cut Values by Algorithm')\n",
    "#     plt.xticks(index + bar_width / 2, range(1, n_groups + 1))\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# barPlot(indices, heurestic_cut, neural_cut)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# percentage = 0\n",
    "# min_cut_value_less_than_heuristic = 0\n",
    "# for i in indices:\n",
    "#     if neural_cut[i-1] <= heurestic_cut[i-1]:\n",
    "#         percentage+=1\n",
    "#     if neural_cut[i-1] < heurestic_cut[i-1]:\n",
    "#         min_cut_value_less_than_heuristic+=1\n",
    "# print(\"Percentage better or equal min-cut value:\" +  str(percentage))\n",
    "# print(\"Percentage better min-cut value:\" +  str(min_cut_value_less_than_heuristic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_item = {}\n",
    "# for i in range(100):\n",
    "#\n",
    "#\n",
    "#     graph = CreateGraph_random(100, 100)\n",
    "#     graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "#     graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "#     q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "#     test_item[i] = [graph_dgl, q_torch, graph]\n",
    "#\n",
    "# indices = []\n",
    "# i = 0\n",
    "# heurestic_cut = []\n",
    "# neural_cut = []\n",
    "# for key, (dgl_graph, adjacency_matrix,graph) in test_item.items():\n",
    "#     heurestic_cut.append(find3WayCut(graph, [0,20,65]))\n",
    "#     neural_cut.append(GetOptimalNetValue(model,dgl_graph, inputs, adjacency_matrix, {0:0, 20:1, 65:2}))\n",
    "#     i+=1\n",
    "#     indices.append(i)\n",
    "#     print(\"Heurestic 3-way min-cut value: \" + str(heurestic_cut[-1]))\n",
    "#     print(\"Neural Network 3-way min-cut value: \" + str(neural_cut[-1]))\n",
    "#     print(f'-------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# percentage = 0\n",
    "# min_cut_value_less_than_heuristic = 0\n",
    "# for i in indices:\n",
    "#     if neural_cut[i-1] <= heurestic_cut[i-1]:\n",
    "#         percentage+=1\n",
    "#     if neural_cut[i-1] < heurestic_cut[i-1]:\n",
    "#         min_cut_value_less_than_heuristic+=1\n",
    "# print(\"Percentage better or equal min-cut value:\" +  str(percentage))\n",
    "# print(\"Percentage better min-cut value:\" +  str(min_cut_value_less_than_heuristic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment 3\n",
    "\n",
    "Creating training set with the following constraints:\n",
    "- Each graph have precisely 300 nodes\n",
    "- Each graph has random edges (50% probability of edge creation)\n",
    "- Each edge has random value of 1-500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# datasetItem = {}\n",
    "# for i in range(1000):\n",
    "#\n",
    "#\n",
    "#     graph = CreateGraph_random(300, 500)\n",
    "#     graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "#     graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "#     q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "#\n",
    "#     datasetItem[i] = [graph_dgl, q_torch, graph]\n",
    "#     print(\"Graph Number Created: \"+str(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=300,patience=1000)\n",
    "#\n",
    "# # Establish pytorch GNN + optimizer\n",
    "# opt_params = {'lr': learning_rate}\n",
    "# gnn_hypers = {\n",
    "#     'dim_embedding': dim_embedding,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'dropout': 0.0,\n",
    "#     'number_classes': 3,\n",
    "#     'prob_threshold': PROB_THRESHOLD,\n",
    "#     'number_epochs': number_epochs,\n",
    "#     'tolerance': tol,\n",
    "#     'patience': patience,\n",
    "#     'nodes':n\n",
    "# }\n",
    "#\n",
    "# net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "#\n",
    "# terminal_nodes = 3\n",
    "#\n",
    "# trained_net, bestLost, epoch, inp= run_gnn_training(\n",
    "#     datasetItem, net, embed, optimizer, int(1e3),\n",
    "#     gnn_hypers['tolerance'], gnn_hypers['patience'], Loss, terminal_nodes, 300, gnn_hypers['number_classes'], './exp3.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n, d, p, graph_type, number_epochs, learning_rate, PROB_THRESHOLD, tol, patience, dim_embedding, hidden_dim = hyperParameters(n=300,patience=1000)\n",
    "#\n",
    "# # Establish pytorch GNN + optimizer\n",
    "# opt_params = {'lr': learning_rate}\n",
    "# gnn_hypers = {\n",
    "#     'dim_embedding': dim_embedding,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'dropout': 0.0,\n",
    "#     'number_classes': 3,\n",
    "#     'prob_threshold': PROB_THRESHOLD,\n",
    "#     'number_epochs': number_epochs,\n",
    "#     'tolerance': tol,\n",
    "#     'patience': patience,\n",
    "#     'nodes':n\n",
    "# }\n",
    "#\n",
    "# net, embed, optimizer = get_gnn(n, gnn_hypers, opt_params, TORCH_DEVICE, TORCH_DTYPE)\n",
    "# # load nerual network model for evaluation\n",
    "# model, inputs =LoadNeuralModel(net, gnn_hypers, TORCH_DEVICE, './exp3.pth')\n",
    "# model.eval()\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "test_item = {}\n",
    "for i in range(100):\n",
    "\n",
    "\n",
    "    graph = CreateGraph_random(300, 500)\n",
    "    graph_dgl = dgl.from_networkx(nx_graph=graph)\n",
    "    graph_dgl = graph_dgl.to(TORCH_DEVICE)\n",
    "    q_torch = qubo_dict_to_torch(graph, gen_adj_matrix(graph), torch_dtype=TORCH_DTYPE, torch_device=TORCH_DEVICE)\n",
    "\n",
    "    test_item[i] = [graph_dgl, q_torch, graph]\n",
    "\n",
    "indices = []\n",
    "i = 0\n",
    "heurestic_cut = []\n",
    "neural_cut = []\n",
    "for key, (dgl_graph, adjacency_matrix,graph) in test_item.items():\n",
    "    heurestic_cut.append(find3WayCut(graph, [0,150,250]))\n",
    "    neural_cut.append(GetOptimalNetValue(model,dgl_graph, inputs, adjacency_matrix, {0:0, 150:1, 250:2}))\n",
    "    i+=1\n",
    "    indices.append(i)\n",
    "    print(\"Heurestic 3-way min-cut value: \" + str(heurestic_cut[-1]))\n",
    "    print(\"Neural Network 3-way min-cut value: \" + str(neural_cut[-1]))\n",
    "    print(f'-------')\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# percentage = 0\n",
    "# min_cut_value_less_than_heuristic = 0\n",
    "# for i in indices:\n",
    "#     if neural_cut[i-1] <= heurestic_cut[i-1]:\n",
    "#         percentage+=1\n",
    "#     if neural_cut[i-1] < heurestic_cut[i-1]:\n",
    "#         min_cut_value_less_than_heuristic+=1\n",
    "# print(\"Percentage better or equal min-cut value:\" +  str(percentage))\n",
    "# print(\"Percentage better min-cut value:\" +  str(min_cut_value_less_than_heuristic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from matplotlib.ticker import ScalarFormatter\n",
    "# def barPlot_2(heurestic_cut, neural_cut):\n",
    "#     # Example data\n",
    "#     n_groups = len(heurestic_cut)\n",
    "#     index = np.arange(n_groups)\n",
    "#     bar_width = 0.35\n",
    "#\n",
    "#     # Create bars\n",
    "#     plt.figure(figsize=(30, 6))\n",
    "#     bar1 = plt.bar(index, heurestic_cut, bar_width, label='Heurestic', log=True)\n",
    "#     bar2 = plt.bar(index + bar_width, neural_cut, bar_width, label='Neural Network', log=True)\n",
    "#\n",
    "#     # Add details\n",
    "#     plt.xlabel('Graph Number')\n",
    "#     plt.ylabel('Minimum Cut Value')\n",
    "#     plt.title('Comparison of Minimum Cut Values by Algorithm')\n",
    "#     plt.xticks(index + bar_width / 2, range(1, n_groups + 1))\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.gca().yaxis.set(major_formatter=ScalarFormatter(), minor_formatter=ScalarFormatter());\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# barPlot_2( heurestic_cut, neural_cut)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}