{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Performance Visualization\n",
    "\n",
    "This notebook creates publication-quality visualizations for neural network testing results.\n",
    "\n",
    "## Features:\n",
    "- Load testing results from the neural testing module\n",
    "- Create comprehensive comparison charts (CPLEX vs Randomized vs GCN)\n",
    "- Generate performance analysis plots\n",
    "- Export publication-ready figures\n",
    "- Modern styling with error bars and statistical significance\n",
    "\n",
    "**Inspired by NeuralTesting.py but with clean, modular design using TestingNeuralNetwork.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T02:40:18.544544Z",
     "start_time": "2025-08-20T02:40:18.263528Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add the current directory to Python path for imports\n",
    "current_dir = Path.cwd()\n",
    "if str(current_dir) not in sys.path:\n",
    "    sys.path.append(str(current_dir))\n",
    "if str(current_dir.parent) not in sys.path:\n",
    "    sys.path.append(str(current_dir.parent))\n",
    "\n",
    "# Import our modules\n",
    "from commons import open_file\n",
    "from Testing.TestingNeuralNetwork import test_multiple_graphs, analyze_results\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, Dict, Tuple, Optional\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization configuration\n",
    "VIZ_CONFIG = {\n",
    "    'results_directory': './neural_testing_results',\n",
    "    'output_directory': './publication_figures',\n",
    "    'figure_format': 'png',\n",
    "    'dpi': 300,\n",
    "    'figure_size': (12, 8),\n",
    "    'font_size': 12,\n",
    "    'title_size': 14,\n",
    "    'legend_size': 10\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(VIZ_CONFIG['output_directory'], exist_ok=True)\n",
    "\n",
    "# Set matplotlib parameters\n",
    "plt.rcParams.update({\n",
    "    'font.size': VIZ_CONFIG['font_size'],\n",
    "    'axes.titlesize': VIZ_CONFIG['title_size'],\n",
    "    'axes.labelsize': VIZ_CONFIG['font_size'],\n",
    "    'xtick.labelsize': VIZ_CONFIG['font_size'],\n",
    "    'ytick.labelsize': VIZ_CONFIG['font_size'],\n",
    "    'legend.fontsize': VIZ_CONFIG['legend_size'],\n",
    "    'figure.titlesize': VIZ_CONFIG['title_size']\n",
    "})\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in VIZ_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neural network test results\n",
    "results_file = os.path.join(VIZ_CONFIG['results_directory'], 'neural_network_test_results.pkl')\n",
    "\n",
    "try:\n",
    "    detailed_results = open_file(results_file)\n",
    "    \n",
    "    # Extract components\n",
    "    test_results = detailed_results['individual_results']\n",
    "    results_by_size = detailed_results['results_by_size']\n",
    "    analysis = detailed_results['analysis']\n",
    "    testing_config = detailed_results['testing_config']\n",
    "    \n",
    "    print(f\"✓ Loaded test results successfully\")\n",
    "    print(f\"  Total tests: {len(test_results)}\")\n",
    "    print(f\"  Graph sizes: {sorted(results_by_size.keys())}\")\n",
    "    print(f\"  Analysis keys: {list(analysis.keys())}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ Results file not found: {results_file}\")\n",
    "    print(\"Please run the neural_network_testing.ipynb notebook first to generate results.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading results: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data for Algorithm Comparison\n",
    "\n",
    "For demonstration purposes, we'll create sample CPLEX and randomized algorithm results.\n",
    "In practice, you would load these from your actual experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for comparison (replace with actual CPLEX/randomized results)\n",
    "def generate_sample_baseline_data(results_by_size: Dict, noise_factor: float = 0.1) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate sample baseline data for CPLEX and randomized algorithms.\n",
    "    In practice, replace this with actual experimental results.\n",
    "    \"\"\"\n",
    "    baseline_data = {}\n",
    "    \n",
    "    for size, data in results_by_size.items():\n",
    "        if not data['simple']['cut_values']:\n",
    "            continue\n",
    "            \n",
    "        gcn_cuts = data['post_processed']['cut_values']\n",
    "        \n",
    "        # Generate realistic CPLEX results (typically better than GCN)\n",
    "        cplex_cuts = []\n",
    "        for cut in gcn_cuts:\n",
    "            # CPLEX typically achieves 10-20% better results than GCN\n",
    "            cplex_cut = cut * (1.1 + np.random.normal(0, noise_factor))\n",
    "            cplex_cuts.append(max(cut, cplex_cut))  # Ensure CPLEX is at least as good\n",
    "        \n",
    "        # Generate realistic randomized results (typically worse than GCN)\n",
    "        randomized_cuts = []\n",
    "        for cut in gcn_cuts:\n",
    "            # Randomized typically achieves 5-15% worse results than GCN\n",
    "            randomized_cut = cut * (0.9 + np.random.normal(0, noise_factor))\n",
    "            randomized_cuts.append(randomized_cut)\n",
    "        \n",
    "        # Generate runtime data\n",
    "        gcn_times = data['post_processed']['times']\n",
    "        cplex_times = [t * (10 + np.random.exponential(5)) for t in gcn_times]  # CPLEX much slower\n",
    "        randomized_times = [t * (0.1 + np.random.normal(0, 0.05)) for t in gcn_times]  # Randomized much faster\n",
    "        \n",
    "        baseline_data[size] = {\n",
    "            'cplex': {\n",
    "                'cut_values': cplex_cuts,\n",
    "                'times': cplex_times,\n",
    "                'std_percent': [np.random.uniform(2, 5) for _ in cplex_cuts]  # Small variance for CPLEX\n",
    "            },\n",
    "            'randomized': {\n",
    "                'cut_values': randomized_cuts,\n",
    "                'times': randomized_times,\n",
    "                'std_percent': [np.random.uniform(8, 15) for _ in randomized_cuts]  # Higher variance\n",
    "            },\n",
    "            'gcn_simple': {\n",
    "                'cut_values': data['simple']['cut_values'],\n",
    "                'times': data['simple']['times'],\n",
    "                'std_percent': [np.random.uniform(3, 8) for _ in data['simple']['cut_values']]\n",
    "            },\n",
    "            'gcn_post': {\n",
    "                'cut_values': data['post_processed']['cut_values'],\n",
    "                'times': data['post_processed']['times'],\n",
    "                'std_percent': [np.random.uniform(3, 8) for _ in data['post_processed']['cut_values']]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return baseline_data\n",
    "\n",
    "# Generate sample data\n",
    "baseline_data = generate_sample_baseline_data(results_by_size)\n",
    "\n",
    "print(\"Sample baseline data generated for comparison:\")\n",
    "for size in sorted(baseline_data.keys()):\n",
    "    data = baseline_data[size]\n",
    "    print(f\"  Size {size}: CPLEX={np.mean(data['cplex']['cut_values']):.1f}, \"\n",
    "          f\"Randomized={np.mean(data['randomized']['cut_values']):.1f}, \"\n",
    "          f\"GCN={np.mean(data['gcn_post']['cut_values']):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_algorithm_comparison_chart(baseline_data: Dict, title: str = 'Algorithm Performance Comparison',\n",
    "                                     y_label: str = 'Cut Value', show_percentages: bool = True,\n",
    "                                     save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Create a comprehensive algorithm comparison chart similar to NeuralTesting.py barPlot_3_dot.\n",
    "    \"\"\"\n",
    "    sizes = sorted(baseline_data.keys())\n",
    "    n_groups = len(sizes)\n",
    "    x = np.arange(n_groups)\n",
    "    bar_width = 0.2\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=VIZ_CONFIG['figure_size'])\n",
    "    \n",
    "    # Colors for each algorithm\n",
    "    colors = {\n",
    "        'cplex': '#87CEEB',      # Sky blue\n",
    "        'randomized': '#FFA500',  # Orange\n",
    "        'gcn_simple': '#FF6347',  # Red\n",
    "        'gcn_post': '#90EE90'     # Light green\n",
    "    }\n",
    "    \n",
    "    labels = {\n",
    "        'cplex': 'CPLEX',\n",
    "        'randomized': 'Randomized Algorithm',\n",
    "        'gcn_simple': 'GCN',\n",
    "        'gcn_post': 'GCN with Post-processing'\n",
    "    }\n",
    "    \n",
    "    algorithms = ['cplex', 'randomized', 'gcn_simple', 'gcn_post']\n",
    "    \n",
    "    # Plot bars for each algorithm\n",
    "    bars = {}\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        means = [np.mean(baseline_data[size][alg]['cut_values']) for size in sizes]\n",
    "        stds = [np.std(baseline_data[size][alg]['cut_values']) for size in sizes]\n",
    "        \n",
    "        bars[alg] = ax.bar(x + i * bar_width, means, bar_width, \n",
    "                          label=labels[alg], color=colors[alg], alpha=0.8,\n",
    "                          yerr=stds, capsize=5, error_kw={'color': 'black', 'alpha': 0.7})\n",
    "    \n",
    "    # Add percentage annotations if requested\n",
    "    if show_percentages:\n",
    "        for i, size in enumerate(sizes):\n",
    "            cplex_mean = np.mean(baseline_data[size]['cplex']['cut_values'])\n",
    "            \n",
    "            for j, alg in enumerate(algorithms[1:], 1):  # Skip CPLEX (baseline)\n",
    "                alg_mean = np.mean(baseline_data[size][alg]['cut_values'])\n",
    "                percentage = (alg_mean / cplex_mean * 100) if cplex_mean > 0 else 0\n",
    "                \n",
    "                x_pos = x[i] + j * bar_width\n",
    "                y_pos = alg_mean * 0.5\n",
    "                \n",
    "                ax.text(x_pos, y_pos, f'{percentage:.0f}%',\n",
    "                       ha='center', va='center', fontweight='bold',\n",
    "                       color='white' if percentage < 90 else 'black',\n",
    "                       fontsize=VIZ_CONFIG['font_size'] - 2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Graph Size (nodes)')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + bar_width * 1.5)\n",
    "    ax.set_xticklabels(sizes)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=VIZ_CONFIG['dpi'], bbox_inches='tight')\n",
    "        print(f\"Chart saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_runtime_comparison_chart(baseline_data: Dict, title: str = 'Runtime Comparison',\n",
    "                                  save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Create a runtime comparison chart with time annotations.\n",
    "    \"\"\"\n",
    "    sizes = sorted(baseline_data.keys())\n",
    "    n_groups = len(sizes)\n",
    "    x = np.arange(n_groups)\n",
    "    bar_width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=VIZ_CONFIG['figure_size'])\n",
    "    \n",
    "    # Colors\n",
    "    colors = ['#FFA500', '#FF6347', '#90EE90']  # Orange, Red, Green\n",
    "    algorithms = ['randomized', 'gcn_simple', 'gcn_post']\n",
    "    labels = ['Randomized Algorithm', 'GCN', 'GCN with Post-processing']\n",
    "    \n",
    "    # Plot bars\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        means = [np.mean(baseline_data[size][alg]['times']) for size in sizes]\n",
    "        stds = [np.std(baseline_data[size][alg]['times']) for size in sizes]\n",
    "        \n",
    "        bars = ax.bar(x + i * bar_width, means, bar_width, \n",
    "                     label=labels[i], color=colors[i], alpha=0.8,\n",
    "                     yerr=stds, capsize=5)\n",
    "        \n",
    "        # Add time annotations on top of bars\n",
    "        for j, (mean, std) in enumerate(zip(means, stds)):\n",
    "            x_pos = x[j] + i * bar_width\n",
    "            y_pos = mean + std + mean * 0.05  # Slightly above error bar\n",
    "            \n",
    "            ax.text(x_pos, y_pos, f'{mean:.2f}s',\n",
    "                   ha='center', va='bottom', fontweight='bold',\n",
    "                   fontsize=VIZ_CONFIG['font_size'] - 2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Graph Size (nodes)')\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + bar_width)\n",
    "    ax.set_xticklabels(sizes)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=VIZ_CONFIG['dpi'], bbox_inches='tight')\n",
    "        print(f\"Runtime chart saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_scalability_line_plot(baseline_data: Dict, title: str = 'Algorithm Scalability Analysis',\n",
    "                                save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Create a line plot showing how algorithms scale with graph size.\n",
    "    \"\"\"\n",
    "    sizes = sorted(baseline_data.keys())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    algorithms = ['cplex', 'randomized', 'gcn_simple', 'gcn_post']\n",
    "    colors = ['#87CEEB', '#FFA500', '#FF6347', '#90EE90']\n",
    "    labels = ['CPLEX', 'Randomized', 'GCN', 'GCN + Post-proc']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    \n",
    "    # Performance scalability (left plot)\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        means = [np.mean(baseline_data[size][alg]['cut_values']) for size in sizes]\n",
    "        stds = [np.std(baseline_data[size][alg]['cut_values']) for size in sizes]\n",
    "        \n",
    "        ax1.errorbar(sizes, means, yerr=stds, label=labels[i], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, \n",
    "                    markersize=8, capsize=5)\n",
    "    \n",
    "    ax1.set_xlabel('Graph Size (nodes)')\n",
    "    ax1.set_ylabel('Average Cut Value')\n",
    "    ax1.set_title('Performance Scalability')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Runtime scalability (right plot, excluding CPLEX for better scale)\n",
    "    for i, alg in enumerate(algorithms[1:], 1):  # Skip CPLEX\n",
    "        means = [np.mean(baseline_data[size][alg]['times']) for size in sizes]\n",
    "        stds = [np.std(baseline_data[size][alg]['times']) for size in sizes]\n",
    "        \n",
    "        ax2.errorbar(sizes, means, yerr=stds, label=labels[i], \n",
    "                    color=colors[i], marker=markers[i], linewidth=2, \n",
    "                    markersize=8, capsize=5)\n",
    "    \n",
    "    ax2.set_xlabel('Graph Size (nodes)')\n",
    "    ax2.set_ylabel('Average Runtime (seconds)')\n",
    "    ax2.set_title('Runtime Scalability')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=VIZ_CONFIG['title_size'] + 2)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=VIZ_CONFIG['dpi'], bbox_inches='tight')\n",
    "        print(f\"Scalability plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_improvement_analysis(baseline_data: Dict, title: str = 'GCN Post-processing Effectiveness',\n",
    "                              save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Create a detailed analysis of GCN post-processing improvements.\n",
    "    \"\"\"\n",
    "    sizes = sorted(baseline_data.keys())\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Absolute improvement\n",
    "    improvements = []\n",
    "    for size in sizes:\n",
    "        simple_cuts = baseline_data[size]['gcn_simple']['cut_values']\n",
    "        post_cuts = baseline_data[size]['gcn_post']['cut_values']\n",
    "        size_improvements = [post - simple for simple, post in zip(simple_cuts, post_cuts)]\n",
    "        improvements.extend(size_improvements)\n",
    "    \n",
    "    ax1.hist(improvements, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax1.axvline(x=0, color='red', linestyle='--', alpha=0.8, label='No improvement')\n",
    "    ax1.axvline(x=np.mean(improvements), color='blue', linestyle='-', alpha=0.8, \n",
    "                label=f'Mean: {np.mean(improvements):.1f}')\n",
    "    ax1.set_xlabel('Cut Value Improvement')\n",
    "    ax1.set_ylabel('Number of Graphs')\n",
    "    ax1.set_title('Distribution of Post-processing Improvements')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Improvement by graph size\n",
    "    size_improvements = []\n",
    "    size_labels = []\n",
    "    for size in sizes:\n",
    "        simple_cuts = baseline_data[size]['gcn_simple']['cut_values']\n",
    "        post_cuts = baseline_data[size]['gcn_post']['cut_values']\n",
    "        improvements = [(post - simple) / simple * 100 if simple > 0 else 0 \n",
    "                       for simple, post in zip(simple_cuts, post_cuts)]\n",
    "        size_improvements.append(improvements)\n",
    "        size_labels.append(f'{size} nodes')\n",
    "    \n",
    "    ax2.boxplot(size_improvements, labels=size_labels)\n",
    "    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    ax2.set_ylabel('Improvement (%)')\n",
    "    ax2.set_title('Improvement Distribution by Graph Size')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Success rate by size\n",
    "    success_rates = []\n",
    "    for size in sizes:\n",
    "        simple_cuts = baseline_data[size]['gcn_simple']['cut_values']\n",
    "        post_cuts = baseline_data[size]['gcn_post']['cut_values']\n",
    "        successes = sum(1 for simple, post in zip(simple_cuts, post_cuts) if post > simple)\n",
    "        rate = successes / len(simple_cuts) * 100 if simple_cuts else 0\n",
    "        success_rates.append(rate)\n",
    "    \n",
    "    ax3.bar(range(len(sizes)), success_rates, color='lightblue', alpha=0.8)\n",
    "    ax3.set_xlabel('Graph Size')\n",
    "    ax3.set_ylabel('Success Rate (%)')\n",
    "    ax3.set_title('Post-processing Success Rate by Graph Size')\n",
    "    ax3.set_xticks(range(len(sizes)))\n",
    "    ax3.set_xticklabels([f'{size}' for size in sizes])\n",
    "    ax3.set_ylim(0, 100)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Runtime overhead\n",
    "    runtime_ratios = []\n",
    "    for size in sizes:\n",
    "        simple_times = baseline_data[size]['gcn_simple']['times']\n",
    "        post_times = baseline_data[size]['gcn_post']['times']\n",
    "        ratios = [post / simple if simple > 0 else 1 for simple, post in zip(simple_times, post_times)]\n",
    "        runtime_ratios.append(np.mean(ratios))\n",
    "    \n",
    "    ax4.plot(sizes, runtime_ratios, 'ro-', linewidth=2, markersize=8)\n",
    "    ax4.axhline(y=1, color='black', linestyle='--', alpha=0.8, label='No overhead')\n",
    "    ax4.set_xlabel('Graph Size (nodes)')\n",
    "    ax4.set_ylabel('Runtime Ratio (Post/Simple)')\n",
    "    ax4.set_title('Post-processing Runtime Overhead')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=VIZ_CONFIG['title_size'] + 2)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=VIZ_CONFIG['dpi'], bbox_inches='tight')\n",
    "        print(f\"Improvement analysis saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"Advanced visualization functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Performance Comparison\n",
    "\n",
    "Create comprehensive comparison charts between different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create algorithm comparison chart for cut values\n",
    "save_path = os.path.join(VIZ_CONFIG['output_directory'], f'algorithm_comparison_cut_values.{VIZ_CONFIG[\"figure_format\"]}')\n",
    "create_algorithm_comparison_chart(\n",
    "    baseline_data=baseline_data,\n",
    "    title='3-Way Max-Cut: Algorithm Performance Comparison',\n",
    "    y_label='Maximum Cut Value',\n",
    "    show_percentages=True,\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Analysis\n",
    "\n",
    "Compare algorithm runtime performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create runtime comparison chart\n",
    "save_path = os.path.join(VIZ_CONFIG['output_directory'], f'runtime_comparison.{VIZ_CONFIG[\"figure_format\"]}')\n",
    "create_runtime_comparison_chart(\n",
    "    baseline_data=baseline_data,\n",
    "    title='Algorithm Runtime Comparison',\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability Analysis\n",
    "\n",
    "Examine how algorithms scale with increasing graph size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scalability analysis\n",
    "save_path = os.path.join(VIZ_CONFIG['output_directory'], f'scalability_analysis.{VIZ_CONFIG[\"figure_format\"]}')\n",
    "create_scalability_line_plot(\n",
    "    baseline_data=baseline_data,\n",
    "    title='Algorithm Scalability: Performance vs Graph Size',\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing Effectiveness Analysis\n",
    "\n",
    "Detailed analysis of GCN post-processing improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create post-processing analysis\n",
    "save_path = os.path.join(VIZ_CONFIG['output_directory'], f'post_processing_analysis.{VIZ_CONFIG[\"figure_format\"]}')\n",
    "create_improvement_analysis(\n",
    "    baseline_data=baseline_data,\n",
    "    title='GCN Post-processing: Comprehensive Effectiveness Analysis',\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Analysis: Two-Algorithm Comparison\n",
    "\n",
    "Simplified comparison between GCN variants (similar to NeuralTesting.py barPlot_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gcn_comparison_chart(baseline_data: Dict, title: str = 'GCN Variants Comparison',\n",
    "                               save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Create a focused comparison between GCN variants.\n",
    "    \"\"\"\n",
    "    sizes = sorted(baseline_data.keys())\n",
    "    n_groups = len(sizes)\n",
    "    x = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=VIZ_CONFIG['figure_size'])\n",
    "    \n",
    "    # Data for GCN variants\n",
    "    simple_means = [np.mean(baseline_data[size]['gcn_simple']['cut_values']) for size in sizes]\n",
    "    simple_stds = [np.std(baseline_data[size]['gcn_simple']['cut_values']) for size in sizes]\n",
    "    \n",
    "    post_means = [np.mean(baseline_data[size]['gcn_post']['cut_values']) for size in sizes]\n",
    "    post_stds = [np.std(baseline_data[size]['gcn_post']['cut_values']) for size in sizes]\n",
    "    \n",
    "    # Create bars\n",
    "    bars1 = ax.bar(x - bar_width/2, simple_means, bar_width, \n",
    "                   label='GCN', color='#FF6347', alpha=0.8,\n",
    "                   yerr=simple_stds, capsize=5)\n",
    "    \n",
    "    bars2 = ax.bar(x + bar_width/2, post_means, bar_width, \n",
    "                   label='GCN with Post-processing', color='#90EE90', alpha=0.8,\n",
    "                   yerr=post_stds, capsize=5)\n",
    "    \n",
    "    # Add value annotations on top of bars\n",
    "    for i, (simple_mean, post_mean, simple_std, post_std) in enumerate(zip(simple_means, post_means, simple_stds, post_stds)):\n",
    "        # Simple GCN annotation\n",
    "        ax.text(x[i] - bar_width/2, simple_mean + simple_std + simple_mean * 0.02,\n",
    "                f'{simple_mean:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Post-processed GCN annotation\n",
    "        ax.text(x[i] + bar_width/2, post_mean + post_std + post_mean * 0.02,\n",
    "                f'{post_mean:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Improvement percentage in the middle\n",
    "        improvement_pct = (post_mean - simple_mean) / simple_mean * 100 if simple_mean > 0 else 0\n",
    "        max_height = max(simple_mean + simple_std, post_mean + post_std)\n",
    "        ax.text(x[i], max_height + max_height * 0.05,\n",
    "                f'{improvement_pct:+.1f}%', ha='center', va='bottom', \n",
    "                fontweight='bold', color='blue', fontsize=VIZ_CONFIG['font_size'] - 1)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Graph Size (nodes)')\n",
    "    ax.set_ylabel('Maximum Cut Value')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(sizes)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=VIZ_CONFIG['dpi'], bbox_inches='tight')\n",
    "        print(f\"GCN comparison chart saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create GCN variants comparison\n",
    "save_path = os.path.join(VIZ_CONFIG['output_directory'], f'gcn_variants_comparison.{VIZ_CONFIG[\"figure_format\"]}')\n",
    "create_gcn_comparison_chart(\n",
    "    baseline_data=baseline_data,\n",
    "    title='GCN Performance: With vs Without Post-processing',\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def perform_statistical_analysis(baseline_data: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Perform statistical significance testing on the results.\n",
    "    \"\"\"\n",
    "    print(\"Statistical Significance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for size in sorted(baseline_data.keys()):\n",
    "        data = baseline_data[size]\n",
    "        \n",
    "        simple_cuts = data['gcn_simple']['cut_values']\n",
    "        post_cuts = data['gcn_post']['cut_values']\n",
    "        \n",
    "        # Paired t-test (since we're comparing the same graphs)\n",
    "        t_stat, p_value = stats.ttest_rel(post_cuts, simple_cuts)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        differences = [post - simple for post, simple in zip(post_cuts, simple_cuts)]\n",
    "        cohens_d = np.mean(differences) / np.std(differences) if np.std(differences) > 0 else 0\n",
    "        \n",
    "        # Determine significance\n",
    "        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "        \n",
    "        print(f\"\\nGraph Size {size} nodes:\")\n",
    "        print(f\"  Mean improvement: {np.mean(differences):.2f} ± {np.std(differences):.2f}\")\n",
    "        print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "        print(f\"  p-value: {p_value:.6f} {significance}\")\n",
    "        print(f\"  Cohen's d: {cohens_d:.3f}\")\n",
    "        print(f\"  Effect size: {'Large' if abs(cohens_d) > 0.8 else 'Medium' if abs(cohens_d) > 0.5 else 'Small'}\")\n",
    "    \n",
    "    print(f\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "\n",
    "# Perform statistical analysis\n",
    "perform_statistical_analysis(baseline_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualization_summary(baseline_data: Dict, analysis: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of all visualizations and findings.\n",
    "    \"\"\"\n",
    "    sizes = sorted(baseline_data.keys())\n",
    "    total_graphs = sum(len(baseline_data[size]['gcn_simple']['cut_values']) for size in sizes)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    all_improvements = []\n",
    "    all_success_rates = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        simple_cuts = baseline_data[size]['gcn_simple']['cut_values']\n",
    "        post_cuts = baseline_data[size]['gcn_post']['cut_values']\n",
    "        \n",
    "        improvements = [post - simple for simple, post in zip(simple_cuts, post_cuts)]\n",
    "        all_improvements.extend(improvements)\n",
    "        \n",
    "        successes = sum(1 for imp in improvements if imp > 0)\n",
    "        success_rate = successes / len(improvements) * 100 if improvements else 0\n",
    "        all_success_rates.append(success_rate)\n",
    "    \n",
    "    avg_improvement = np.mean(all_improvements)\n",
    "    overall_success_rate = np.mean(all_success_rates)\n",
    "    \n",
    "    summary = f\"\"\"Neural Network Visualization Summary Report\n",
    "{'='*60}\n",
    "\n",
    "Dataset Overview:\n",
    "  Total test graphs: {total_graphs}\n",
    "  Graph sizes tested: {sizes}\n",
    "  Graphs per size: {[len(baseline_data[size]['gcn_simple']['cut_values']) for size in sizes]}\n",
    "\n",
    "Post-processing Performance:\n",
    "  Average improvement: {avg_improvement:+.2f} cut value\n",
    "  Overall success rate: {overall_success_rate:.1f}%\n",
    "  Standard deviation: {np.std(all_improvements):.2f}\n",
    "\n",
    "Algorithm Rankings (by average performance):\"\"\"\n",
    "    \n",
    "    # Calculate average performance for each algorithm\n",
    "    alg_performance = {}\n",
    "    for alg in ['cplex', 'gcn_post', 'gcn_simple', 'randomized']:\n",
    "        all_cuts = []\n",
    "        for size in sizes:\n",
    "            all_cuts.extend(baseline_data[size][alg]['cut_values'])\n",
    "        alg_performance[alg] = np.mean(all_cuts)\n",
    "    \n",
    "    # Sort by performance\n",
    "    sorted_algs = sorted(alg_performance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    alg_names = {\n",
    "        'cplex': 'CPLEX (Integer Solver)',\n",
    "        'gcn_post': 'GCN with Post-processing',\n",
    "        'gcn_simple': 'GCN (Simple)',\n",
    "        'randomized': 'Randomized Algorithm'\n",
    "    }\n",
    "    \n",
    "    for i, (alg, avg_cut) in enumerate(sorted_algs, 1):\n",
    "        summary += f\"\\n  {i}. {alg_names[alg]}: {avg_cut:.1f}\"\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "\n",
    "Generated Visualizations:\n",
    "  1. Algorithm Performance Comparison (Cut Values)\n",
    "  2. Runtime Comparison Analysis\n",
    "  3. Scalability Analysis (Performance vs Graph Size)\n",
    "  4. Post-processing Effectiveness Analysis\n",
    "  5. GCN Variants Comparison\n",
    "  6. Statistical Significance Testing\n",
    "\n",
    "Key Findings:\n",
    "  - Post-processing {'improves' if avg_improvement > 0 else 'decreases'} GCN performance on average\n",
    "  - Success rate varies from {min(all_success_rates):.1f}% to {max(all_success_rates):.1f}% across graph sizes\n",
    "  - {'CPLEX' if sorted_algs[0][0] == 'cplex' else 'GCN'} achieves the best overall performance\n",
    "  - Randomized algorithm provides fastest runtime but lowest quality\n",
    "\n",
    "Figures saved to: {VIZ_CONFIG['output_directory']}\n",
    "Format: {VIZ_CONFIG['figure_format'].upper()}, DPI: {VIZ_CONFIG['dpi']}\n",
    "\n",
    "{'='*60}\"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display summary\n",
    "summary_report = generate_visualization_summary(baseline_data, analysis)\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = os.path.join(VIZ_CONFIG['output_directory'], 'visualization_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nVisualization summary saved to: {summary_file}\")\n",
    "print(f\"All figures saved to: {VIZ_CONFIG['output_directory']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
